[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "보건학데이터과학 실습",
    "section": "",
    "text": "소개"
  },
  {
    "objectID": "index.html#보건학",
    "href": "index.html#보건학",
    "title": "보건학데이터과학 실습",
    "section": "보건학",
    "text": "보건학\n보건학은 인간과 그들의 사회의 건강을 향상시키기 위한 과학입니다. 보건학은 의학에서 사회과학에 이르기까지 여러 학문과 밀접하게 연관되어 있습니다. 따라서, 다학제적 접근법은 보건학 전문가에게 가장 중요한 기술 중 하나입니다."
  },
  {
    "objectID": "index.html#보건학-데이터과학",
    "href": "index.html#보건학-데이터과학",
    "title": "보건학데이터과학 실습",
    "section": "보건학 데이터과학",
    "text": "보건학 데이터과학\n데이터 과학은 크게 데이터를 수집, 처리, 분석하고, 그 결과를 해석하여 의사결정이나 예측, 분류 등의 작업을 수행하는 분야입니다. 이를 위해 프로그래밍, 통계학, 머신러닝 및 도메인 지식을 종합적으로 활용합니다. 이러한 기술을 보건학에 접목시키는 것이 보건학 데이터과학입니다.\n보건학 데이터과학의 주요 특징은 다음과 같습니다:\n\n다양한 데이터의 수집 및 관리:\n\n보건학에서는 환자 기록, 임상 데이터, 역학 조사 데이터, 환경 데이터 등 다양한 유형의 데이터를 다룹니다. 이러한 데이터의 효율적인 수집과 관리는 보건학의 데이터과학에서 중요한 부분을 차지합니다.\n\n통계 및 머신러닝의 적용:\n\n공공보건의 문제를 해결하기 위해, 통계적 방법과 머신러닝 기술을 사용하여 데이터에서 통찰력을 얻거나 예측 모델을 만드는 작업을 수행합니다.\n\n데이터 시각화:\n\n복잡한 보건 데이터를 쉽게 이해하고 해석하기 위해 시각화 기술을 사용합니다. 이를 통해 정책 결정자나 일반 대중에게 데이터를 효과적으로 전달할 수 있습니다.\n\n재현성 및 공유 가능한 연구:\n\n보건학 데이터과학에서는 연구 결과의 재현성을 보장하고, 연구 결과를 다른 연구자와 공유할 수 있도록 투명한 방법을 추구합니다.\n\n도메인 지식:\n\n보건학 데이터과학자는 단순히 데이터과학의 기술만을 가지고 있는 것이 아니라, 보건학에 대한 깊은 이해도 필요합니다. 이를 통해 데이터를 올바르게 해석하고, 실제 보건 문제에 적절한 솔루션을 제시할 수 있습니다."
  },
  {
    "objectID": "index.html#보건학-데이터-과학-과학이-되려면",
    "href": "index.html#보건학-데이터-과학-과학이-되려면",
    "title": "보건학데이터과학 실습",
    "section": "보건학 데이터 과학, 과학이 되려면",
    "text": "보건학 데이터 과학, 과학이 되려면\n과학에서의 제현성 과학에서의 재현성은 연구 결과를 다른 연구자들이 동일한 조건 하에서 반복해서 얻을 수 있는지의 여부를 나타내는 중요한 원칙입니다. 재현성은 과학적 연구의 진정성과 신뢰성을 평가하는 기준 중 하나로 간주됩니다.\n보건학 데이터과학이 과학의 기준에 부합하려면 다음과 같은 재현성 요소들을 갖추어야 합니다\n\n데이터의 접근성:\n\n연구에 사용된 원본 데이터는 공개되어야 하며, 연구를 재현하려는 다른 연구자들이 접근할 수 있어야 합니다. 당연히 개인정보 보호와 관련된 법률 및 규정을 준수하면서, 필요한 경우 익명화 또는 변조된 형태로 데이터를 공개해야 합니다.\n\n분석 코드 및 소프트웨어의 공개:\n\n연구에 사용된 데이터 처리, 분석, 시각화 등의 코드와 사용된 소프트웨어의 버전 정보도 공개되어야 합니다.\n\n분석 방법론의 명확성:\n\n사용된 통계적 방법, 머신러닝 알고리즘, 모델링 접근법 등이 명확하게 기술되어야 합니다.\n\n외부 변수 및 제어:\n\n연구 과정에서 영향을 미칠 수 있는 외부 변수들에 대한 정보와 그것들을 어떻게 제어했는지에 대한 정보가 필요합니다.\n\n결과의 재현 가능성 평가:\n\n가능한 경우, 연구 결과의 재현 가능성을 평가하기 위해 독립된 데이터셋이나 방법론을 사용하여 검증을 시도해야 합니다.\n\n연구 환경의 명세:\n\n연구가 수행된 하드웨어 및 소프트웨어 환경, 그리고 이를 설정하기 위한 파라미터 등도 기록되어야 합니다. 이는 특히 계산적으로 복잡한 모델이나 시뮬레이션을 다룰 때 중요합니다.\n\n피어 리뷰:\n\n보건학 데이터과학 연구의 결과는 동료 평가 과정을 거쳐 검증되어야 합니다. 피어 리뷰는 연구의 품질과 재현성을 높이는 데 중요한 역할을 합니다.\n\n\n정리하면 아래와 같은 흐름데로 연구하게 됩니다.\n 이 튜토리얼에서는 R, Rstudio, markdown, Shiny server, PostgreSQL 및 github를 사용할 것입니다. R은 무료 오픈 소스 통계 언어로, 데이터 과학 분야에서 널리 사용됩니다. 가장 중요한 참고 자료는 Rafael A. Irizarry의 책과 Hadley Wickham의 책입니다. 아래와 같습니다."
  },
  {
    "objectID": "index.html#참고-books",
    "href": "index.html#참고-books",
    "title": "보건학데이터과학 실습",
    "section": "참고 books",
    "text": "참고 books\n\n\n\n\n\n\n\n\ntitle\nauthos\nurl\n\n\n\n\nIntroduction to Data Science with R\nRafael A. Irizarry\nhttps://rafalab.github.io/dsbook/\n\n\nR for Data Science\nGarrett Grolemund, Hadley Wickham\nhttps://r4ds.had.co.nz/index.html\n\n\n\n\nI hope you will get valuable experience with me.\n\n그럼 시작하겠습니다.!!! jinha"
  },
  {
    "objectID": "210_randrstudio.html#rstudio-cloud",
    "href": "210_randrstudio.html#rstudio-cloud",
    "title": "1  R & R studio 사용",
    "section": "1.1 Rstudio Cloud",
    "text": "1.1 Rstudio Cloud\nR studio cloud는 R과 R studio를 가장 빠르고 쉽게 사용할 수 있는 방법입니다. 다양한 옵션이 있으며, 무료 옵션도 포함되어 있습니다.\n\nRstudio cloud 로그인 방법\n\n RStudio Cloud (Posit cloud)  에 접속합니다.\n로그인을 합니다.\nNew Project 를 실행합니다.\n시작합니다."
  },
  {
    "objectID": "210_randrstudio.html#r-and-r-studio-on-window-system",
    "href": "210_randrstudio.html#r-and-r-studio-on-window-system",
    "title": "1  R & R studio 사용",
    "section": "1.2 R and R studio on Window system",
    "text": "1.2 R and R studio on Window system\n여기에서는 Windows 시스템에서 R과 R 스튜디오를 설치하는 방법을 설명합니다.\n\n구글에서 R cran을 검색하세요.\n\n\n아래와 같은 사이트가 나타날 것입니다.\n\n\nR을 다운로드 및 설치하기를 클릭하세요.\nWindows용 R 다운로드하기\nBase → Install R for the first Time\nDownload R * for Windows\n\n\n\n\nR설치\n\n\n\n1.2.1 R studio install\nR 스튜디오 설치는 정말 쉽습니다. Rstudio 로 검색한 다음 아래의 사이트에 들어갑니다. https://posit.co/download/rstudio-desktop/\n\n\n\nR 설치 사이트\n\n\n여기서 R studio 를 클릭해서 다운로드 후 설치하면 됩니다.\n\n\n1.2.2 R studio project\nR 스튜디오는 프로젝트 옵션을 제공합니다. 프로젝트는 데이터와 스크립트를 가져오고 내보내는 기본 경로를 만듭니다. 프로젝트를 공유할 때 협업이 쉬워집니다.\n우수 project 폴더를 만듭니다. 그리고 이곳에 각각의 프로젝트를 설치할 것입니다.\n\n\n\nR Project 세팅 1\n\n\n\n프로젝트 생성하기\n\n\nStep 1: RStudio 상단 메뉴에서 File을 클릭한 후, New Project…를 선택합니다.\nStep 2: 새로운 창이 열리면, 아래의 옵션 중 하나를 선택할 수 있습니다:\nNew Directory: 새로운 디렉토리(폴더)와 함께 새로운 프로젝트를 만듭니다.\nExisting Directory: 이미 존재하는 디렉토리(폴더)를 기반으로 프로젝트를 만듭니다.\nVersion Control (Git 또는 SVN): Git이나 SVN과 연동된 버전 관리 프로젝트를 만듭니다. 이 옵션은 프로젝트를 Git 리포지토리와 연동하고 싶을 때 유용합니다.\n\n\n\n\nR Project 세팅 2\n\n\n\n새로운 프로젝트 만들기 (New Directory 선택 시)\n\n\nStep 3: New Directory를 선택한 경우, New Project나 특정 템플릿(예: R 패키지, Shiny 앱 등)을 선택할 수 있습니다. 일반적인 분석 프로젝트라면 New Project를 선택하면 됩니다.\nStep 4: 새로운 프로젝트에 사용할 디렉토리의 이름을 입력하고, - 디렉토리를 저장할 경로를 지정합니다.\nStep 5: 필요한 경우 Create a git repository를 선택하여 Git 버전 관리 기능을 프로젝트에 추가할 수 있습니다.\nStep 6: Create Project 버튼을 클릭하면, 새로운 프로젝트가 생성되고 해당 디렉토리 안에 작업 환경이 설정됩니다. \n\n\n\n\nR Project 세팅 4\n\n\n\n기본 폴더와 파일 생성하기\n\n\n.Rproj : R프로젝트 파일로, 일관된 작업을 보장하고, 사용자별 설정을 저장할 수 있습니다.\ndata: 모든 원 또는 가공된 데이터 파일을 여기에 보관해야 합니다.\nrscript: 모든 r 스크립트가 저장되어 있어야 합니다.\ndatastep.R: 데이터 정리, 변환 및 전처리를 합니다.\nanalysis.R: 핵심 분석 방법(통계 테스트, 데이터 모델링 등)을 포함합니다.\nsources: 추가 스크립트 하위 폴더입니다.\nfunction.R: 서로 다른 스크립트에서 사용할 사용자 정의 함수를 정의해놓을 수 있습니다.\nresults: plot, 테이블, 및 처리된 파일과 같은 모든 결과물을 여기에 저장합니다.\nmanuscript: 프로젝트에 대해 작성 중인 보고서나 논문 초안, 메모 및 최종 버전을 저장합니다.\n\n\n\n\nR Project 세팅 5\n\n\n\n\n저는 또한 흔히 사용되는 기본 디렉토리를 권장합니다. 이것은 동료와 아이디어를 공유하는 또 다른 규칙입니다.\n\n\n우리가 Windows 시스템을 사용하더라도 복사 및 이동과 같은 기본 명령은 코딩 프로세스를 용이하게 만드는 데 필요합니다. 아래 동영상을 보세요\n\n\n네, 이제 R 코딩을 시작할 준비가 되었습니다."
  },
  {
    "objectID": "210_randrstudio.html#r-and-r-studio-on-ubuntus",
    "href": "210_randrstudio.html#r-and-r-studio-on-ubuntus",
    "title": "1  R & R studio 사용",
    "section": "1.3 R and R studio on Ubuntus",
    "text": "1.3 R and R studio on Ubuntus\n우분투스 서버에 R을 설치하는 과정입니다. 아마존, 구글클라우드등 서버에 설치할때 아래의 동영상을 따라해 주세요. 필요한 분만 하시면 됩니다. 여러 복잡한 과정이 있으니, 아래의 구글 docs를 보고 따라해 주세요.\n참고 동영상 \nGoogle Doc   Google Doc Download"
  },
  {
    "objectID": "221_Rbasic1.html#vector-matirx-list",
    "href": "221_Rbasic1.html#vector-matirx-list",
    "title": "2  R 속성",
    "section": "2.1 vector matirx list",
    "text": "2.1 vector matirx list\n\n2.1.1 objects\n값(value)을 변수(variable)에 대입시키는 방법이다. 이를 이용해서 변수를 통한 연산이 가능하다.\n\na <-1\nb <-2\nc <- -1\n\na*b+c\n\n[1] 1\n\n\n값에는 숫자 외에도 여러 종류가 가능하다. 대표적인 것이 숫자(numeric), 문자(character), 논리값(logical) 값을 대입시킬 수 있다.\n\nstudent_a_age  <- 43\nstudent_a_name <- '윤진하'\nstudent_a_pass <- FALSE\n\n즉, 한변수에 하나의 값을 제공하는 0차원적 개념이다.\n\n\n2.1.2 백터 (Vectors), 팩터 (factors)\n백터는 한 변수에 여러개의 값이 존재하는 경우다. 이때 c() 를 사용하고 c는 concatenate의 C이다. 결측값 (missing value) 는 NA나 NAN으로 표시된다. 백터에는 순서가 존재하므로, my_vector[i] 를 이용하여 i번째 값을 추출할 수있다.\n\nstudent_all_age <- c(24, 31, 40, 16)\nstudent_all_age\n\n[1] 24 31 40 16\n\n\n각각 아래의 명령에 따라 어떤 값이 나타날지 예상해보자.\n\nstudent_all_age[1]\nstudent_all_age[1:2]\nstudent_all_age[c(1, 3)]\nstudent_all_age[-4]\n\n숫자로된 백터는 기본 함수를 사용하여 요약값을 나타낼 수 있다. max(), min(), range(), length(), sum(), mean(), prod(), sd(), var(), sort() 등이 있다.\n\nmax(student_all_age)\n\n[1] 40\n\nlength(student_all_age)\n\n[1] 4\n\nstudent_all_name <- c('영희', '철수', '은미', '재석')\nsummary(student_all_name)\n\n   Length     Class      Mode \n        4 character character \n\nstudent_all_class <- c('group1','group2', 'group1', 'group2' )\nsummary(student_all_class)\n\n   Length     Class      Mode \n        4 character character \n\n\n여기서 student_all_class 는 group이 1과2로 나뉘어 있는 것을 볼 수 있다. 이러한 개념은 팩터로 살펴볼 수 있다. 컴퓨터에게 sutdent_all_class가 팩터라는 것을 알려주자.\n\nstudent_all_group <-factor(c('group1','group2', 'group1', 'group2' ))\nsummary(student_all_group)\n\ngroup1 group2 \n     2      2 \n\n\ngroup이 있으니 몇가지 쉬운 이용방법이 생겼다.\n\ntapply(student_all_age, student_all_group, mean)\n\ngroup1 group2 \n  32.0   23.5 \n\n\n미리 이야기 하지만 *apply에 익수해져야한다. 이제, 1차원의 데이터 형식을 보았다. 메트릭스로 넘어 2차원의 자료를 살펴 보자\n\n\n2.1.3 매트릭스 (Matrix), 데이터프래임 (data frame), 리스트 (list)\n우리가 흔히 보았던 엑셀이나 표 형식의 데이터이다. 메트릭스는 행(row)와 열(column)로 구성되어 있고 백터 변수를 행을 기준으로 나열 할지, 열을 기준으로 할지 여부로 구성할 수 있다. 어떤 결과가 나오는지 확인해 보자.\n\nA1 = matrix( \n     c(1, 2, 3, 4, 5, 6, 7, 8),  # 값 \n     nrow=2,                     # 행 갯수\n     ncol=4,                     # 열 갯수\n     byrow = TRUE)               # 행을 기준으로 순서대로\nA1 # 출력\n\n그렇다면 똑 같이 1, 2, 3, 4, 5, 6, 7, 8를 이용해서 아래의 행렬을 만들어보자\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n[4,]    7    8\n\n\n행렬이 행(column)에 같은 성질의 값 들로 이루어져 있다면 데이터프래임은 좀더 자유롭다고 볼 수 있다.\n\ndspub_class <- data.frame(\n  'name'  = student_all_name, \n  'age'   = student_all_age,\n  'group' = student_all_group\n)\ndspub_class\n\n  name age  group\n1 영희  24 group1\n2 철수  31 group2\n3 은미  40 group1\n4 재석  16 group2\n\n\n데이터프래임 부터는 tidyverse 패키지를 사용해서 몇가지를 보겠다. %>%는 pipe로 앞에 있는 것을 가지도 뒤에 명령을 하자는 조사 같은 것으로 이해하자. mutate는 가로 안에 있는 명령을 수행해서 새로운 변수를 만들라는 것이다.\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\ndspub_class %>%\n  group_by(group) %>%\n  summarize(avg = mean(age))\n\n# A tibble: 2 × 2\n  group    avg\n  <fct>  <dbl>\n1 group1  32  \n2 group2  23.5\n\n\n과제가 있다. 과제는 DSpub내 group1 과 gropu2가 몇개인지와 최고 나이를 맞추는 것이다. 영희/철수에게 첫번째 숙제를, 은미/재석에게 두번째 숙제를 냈다. 과제를 받았다.\n\nhomework1 <- dspub_class %>%\n  group_by(group) %>%\n  mutate(avg_age = mean(age))\nhomework2 <- dspub_class %>%\n  group_by(group) %>%\n  mutate(max_age = max(age))\n\n\nhomework1\n\n# A tibble: 4 × 4\n# Groups:   group [2]\n  name    age group  avg_age\n  <chr> <dbl> <fct>    <dbl>\n1 영희     24 group1    32  \n2 철수     31 group2    23.5\n3 은미     40 group1    32  \n4 재석     16 group2    23.5\n\nhomework2\n\n# A tibble: 4 × 4\n# Groups:   group [2]\n  name    age group  max_age\n  <chr> <dbl> <fct>    <dbl>\n1 영희     24 group1      40\n2 철수     31 group2      31\n3 은미     40 group1      40\n4 재석     16 group2      31\n\n\n이것을 어딘가에 저장하고 싶다, 이때 list를 사용할 수 있다.\n\nsecond_week_dspub <-\n  list(\n       student_all_age,\n       student_all_class,\n       student_all_group,\n       student_all_name,\n       dspub_class, \n       homework1,\n       homework2\n       )\nsecond_week_dspub\n\n[[1]]\n[1] 24 31 40 16\n\n[[2]]\n[1] \"group1\" \"group2\" \"group1\" \"group2\"\n\n[[3]]\n[1] group1 group2 group1 group2\nLevels: group1 group2\n\n[[4]]\n[1] \"영희\" \"철수\" \"은미\" \"재석\"\n\n[[5]]\n  name age  group\n1 영희  24 group1\n2 철수  31 group2\n3 은미  40 group1\n4 재석  16 group2\n\n[[6]]\n# A tibble: 4 × 4\n# Groups:   group [2]\n  name    age group  avg_age\n  <chr> <dbl> <fct>    <dbl>\n1 영희     24 group1    32  \n2 철수     31 group2    23.5\n3 은미     40 group1    32  \n4 재석     16 group2    23.5\n\n[[7]]\n# A tibble: 4 × 4\n# Groups:   group [2]\n  name    age group  max_age\n  <chr> <dbl> <fct>    <dbl>\n1 영희     24 group1      40\n2 철수     31 group2      31\n3 은미     40 group1      40\n4 재석     16 group2      31\n\n\n숙제 과제만 뽑아 내서 보고 싶다면, 해당 list만 출력하면 된다. 즉 list에는 거의 모든 자료가 이질성을 갖고 있더라도 저장된다.\n\nsecond_week_dspub[[7]]\n\n# A tibble: 4 × 4\n# Groups:   group [2]\n  name    age group  max_age\n  <chr> <dbl> <fct>    <dbl>\n1 영희     24 group1      40\n2 철수     31 group2      31\n3 은미     40 group1      40\n4 재석     16 group2      31"
  },
  {
    "objectID": "221_Rbasic1.html#기초-연산",
    "href": "221_Rbasic1.html#기초-연산",
    "title": "2  R 속성",
    "section": "2.2 기초 연산",
    "text": "2.2 기초 연산\n단순 계산기로 사용할 수 있다. 예를 들어 1+2 의 값이라던가, log2(10) 등을 계산할 수 있다. Rsutdio 의 스크립트 창이나 콘솔 창에 아래의 항목을 작성해 볼 수 있다.\n\n3+4;4-3;4/3;3*4\nlog2(10)\nabs(-4)\nsqrt(4)\n\n기초 함수는 아래와 같다.\n\n\n\nOperator\nDescription\n\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^ or **\nexponentiation\n\n\nx %% y\nmodulus (x mod y) 5%%2 is 1\n\n\nx %/% y\ninteger division 5%/%2 is 2\n\n\n\n\n\n\nOperator\nDescription\n\n\n\n\n<\nless than\n\n\n<=\nless than or equal to\n\n\n>\ngreater than\n\n\n>=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to\n\n\n!x\nNot x\n\n\nx\ny\n\n\nx & y\nx AND y\n\n\nisTRUE(x)\ntest if X is TRUE\n\n\n\n\n\n\n\n\n\n\nOperator\nDescription\n\n\n\n\nLogarithms and exponentials\nlog2(x), log10(x), exp(x)\n\n\nTrigonometric functions\ncos(x), sin(x), tan(x), acos(x), asin(x), atan(x)\n\n\nOthers\nabs(x): absolute value; sqrt(x): square root.\n\n\n\n기초 함수 중에 몇몇은 그림을 그려보아야 이해가 쉽다.\n\npm10 = rnorm(n=100, mean = 10, sd = 5) # 평균이 10이고 표준편차가 5인 100개의 랜덤 변수를 pm10 이라고 가정하자\ndate= rep(1:100)  # 1일부터 100까지의 시간이 있다고 해보자\nplot(x=date, y=pm10, type = \"l\") # \"l\" line 형식으로 그려보았다. \n\n\n\noz = sin(date) # 오존은 시간에 따라 햇빛이 있을 때 높게 올라간다. sine 함수를 따른다고 가정해 보자\nplot(x=date, y = oz, type = \"l\") # \n\n\n\noz2 = oz**2 # -값을 갖는 것은 좀 이상하다. 제곱을 통해 변경해 보자\nplot(x=date, y = oz2, type = \"l\") # \n\n\n\nozabs = abs(oz) # 접곱보다 절대 값이 어떨까?\nplot(x=date, y = ozabs, type = \"l\") # \n\n\n\npmtrend = pm10 + date  # 시간에 따라 pm10 농도가 올라간다고 가정해 보자\nplot(x=date, y = pmtrend, type=\"l\")\n\n\n\npmtrend.log = log(pmtrend) # 로그 값을 넣어보자. 특별한 의미는 없이 함수에 대한 실습이다. \nplot(x=date, y = pmtrend.log, type=\"l\") #"
  },
  {
    "objectID": "221_Rbasic1.html#조건부-연산",
    "href": "221_Rbasic1.html#조건부-연산",
    "title": "2  R 속성",
    "section": "2.3 조건부 연산",
    "text": "2.3 조건부 연산\nif-else 라는 조건에 따라 연산을 수행시킨다. 예를 들어 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 의 백터에서 5보다 작으면 A 크면 B를 적용시켜주다.\n\nnums<- 1\nif (nums <5) {\n        chars = 'A'\n} else{\n        chars = 'B'\n}\nchars\n\n[1] \"A\"\n\n\n\nnums <- 6\nif (nums <5) {\n        chars = 'A'\n} else{\n        chars = 'B'\n}\nchars\n\n[1] \"B\"\n\n\n몇가지 예제를 더 살펴보자.\n\na<-round(rnorm(10)*10)\na\n\n [1]   1   9  -2  -6  -6  10 -15  -1  14  10\n\ntab <- ifelse(a>0, '양수', '음수')\ntab\n\n [1] \"양수\" \"양수\" \"음수\" \"음수\" \"음수\" \"양수\" \"음수\" \"음수\" \"양수\" \"양수\"\n\ndata.frame(a, tab)\n\n     a  tab\n1    1 양수\n2    9 양수\n3   -2 음수\n4   -6 음수\n5   -6 음수\n6   10 양수\n7  -15 음수\n8   -1 음수\n9   14 양수\n10  10 양수\n\n\n데이터 클리닝에서 자주 사용하는 두개의 조건문 any()와 all()이 있다. any()는 하나라도 TRUE값이 있으면 TRUE를 변환해주고, all()은 모두 TRUE여야 TRUE를 돌려준다.\n\nnew.var <- c(1, 2, NA)\nis.na(new.var)\n\n[1] FALSE FALSE  TRUE\n\nany(is.na(new.var))\n\n[1] TRUE\n\nall(is.na(new.var))\n\n[1] FALSE\n\n\nindex 를 이용하면 조건 문에서 IF (또는 Where)의 개념을 사용할 수 있다. iris 데이터에서 Sepal.Length 가 가장 큰 값은 찾고, Sepal.Length 최고 값을 갖은 Species의 종류를 찾고자한다. 어떻게 하면될까?\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntable(iris$Species)\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\nmax(iris$Sepal.Length)\n\n[1] 7.9\n\nmax.length <- which.max(iris$Sepal.Length)\niris$Species[max.length]\n\n[1] virginica\nLevels: setosa versicolor virginica\n\n\n같은 방법으로 iris 데이터에서 Sepal.Length 가 가장 작은 찾고, Sepal.Length 최소 값을 갖은 Species의 종류를 찾고자한다. 최소 값과 종류를 쏘보자\n\nmin(iris$Sepal.Length)\n\n[1] 4.3\n\nmin.length <- which.min(iris$Sepal.Length)\niris$Species[min.length]\n\n[1] setosa\nLevels: setosa versicolor virginica"
  },
  {
    "objectID": "221_Rbasic1.html#함수-만들기",
    "href": "221_Rbasic1.html#함수-만들기",
    "title": "2  R 속성",
    "section": "2.4 함수 만들기",
    "text": "2.4 함수 만들기\nR을 이용하는 이유중 하나가 함수를 손쉽게 만들고 그 결과를 활용하기가 쉽다는 것이다. 자동문, 반복문, 데이터 클리능, 데이터 시각화 등에서 자주 사용하는 기본 원리이다. 숫자 2개를 넣으면 덧 샘을 해주는 함수를 만들어 보자\n\naddtive.function = function(x, y ){\n  x + y\n}\n\n\naddtive.function(100, 2)\n\n[1] 102\n\n\n\n퀴즈\n\n숫자 2개를 넣으면 두 수의 차이를 보여주는 함수를 만들어 보자 abs 사용. #-#에 계산식을 넣어 함수를 완성해 보세요.\n\nabs.function= function(x, y ){\n  #--#\n}\n\n평균을 구해주는 함수 avg를 만들어 보자. length는 길이를 말해주니, 몇개의 변수값이 있는 지 알 수 있다.\n\nmy_vector<- 1:50\navg <- function(x){\n        sum(x)/length(x)\n}\navg(my_vector)\n\n[1] 25.5\n\n\n좀더 확장해서 변수 갯수, 평균, 최고, 최저 값을 나타내는 함수를 만들어 보자.\n\ntabs <- function(x){\n        data.frame( '평균'      = mean(x), \n                    '변수갯수'  = length(x), \n                    '최고값'    = max(x), \n                    '최저값'    = min(x)\n        )\n}\n\ntabs(my_vector)\n\n  평균 변수갯수 최고값 최저값\n1 25.5       50     50      1\n\n\n그럼 이를 이용해서, 다음을 해석해 보자\n\navg <- function(x, arithmetic = TRUE){\n  n <- length(x)\n  ifelse(arithmetic, sum(x)/n, prod(x)^(1/n))\n}"
  },
  {
    "objectID": "221_Rbasic1.html#반복문-vectorization-functionals",
    "href": "221_Rbasic1.html#반복문-vectorization-functionals",
    "title": "2  R 속성",
    "section": "2.5 반복문, vectorization, functionals",
    "text": "2.5 반복문, vectorization, functionals\n\n2.5.1 for loop\n1a, 2a, 3a, 4a, 5a, 6a, 7a, 8a, 9a, 10a 을 만들어보자, 어떻게 하면될까?\n\nc('1a', '2a', '10a') # 이렇게 해보는 것도 좋지만\n\n[1] \"1a\"  \"2a\"  \"10a\"\n\n\n반목문을 사용하면, 아래와 같다.\n\nfor (i in 1:10){\n  print(paste0(i, 'a'))\n}\n\n[1] \"1a\"\n[1] \"2a\"\n[1] \"3a\"\n[1] \"4a\"\n[1] \"5a\"\n[1] \"6a\"\n[1] \"7a\"\n[1] \"8a\"\n[1] \"9a\"\n[1] \"10a\"\n\n\n물론 대부분 이렇게 사용하지만, 아래와 같이 사용한다.\n\npaste0(1:10, \"a\")\n\n [1] \"1a\"  \"2a\"  \"3a\"  \"4a\"  \"5a\"  \"6a\"  \"7a\"  \"8a\"  \"9a\"  \"10a\"\n\n\n1:n까지의 숫자 합을 만들어보자, 그리고 이를 1부터 100일때 까지 만들고 그림을 그려보자\n\ncompute <- function(n){ sum(1:n)}\ncompute(10)\n\n[1] 55\n\ntest <-c()\nfor (n in 1:100){\n  test[n] <- compute(n)\n}\n\nplot(1:100, test)\n\n\n\n\n\n\n2.5.2 vectorization 과 apply 구문\n사실 ifelse 를 잘 사용하지 않는다. 이는 속도의 문제와도 관련된다. 실제 ifesel 로 10분이 걸리는 연산이 1분으로 줄수도 있다. 이때 사용하게 되는 것이 백터화와 apply 구문이다. 이미 past0(1:10,“a”) 같은 구문이 편할 수 있다는 것을 느꼈을 것이다. 이번에는 구구단 2단과 3단을 서로 곱해보자. 어떻게 하면 좋을까 for와 if를 생각하기 보다 아래를 고려해 보자. 아래가 백터화이다.\n\nn2 <- c(1:9*2)\nn3 <- c(1:9*3)\nn2*n3\n\n[1]   6  24  54  96 150 216 294 384 486\n\n\n2단에 3단이 아닌 다른 단을 곱하는 함수를 사용해 보자\n\nnew.function<-function(n2){\n  c(1:9*2) * c(1:9*n2)\n}\n\nnew.function( 4)\n\n[1]   8  32  72 128 200 288 392 512 648\n\n\n그럼 3단 대신에 1, 2, 3, 4, 5, 6, 7, 8, 9 단을 모두 해보자\n\nsapply(1:9, new.function)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n [1,]    2    4    6    8   10   12   14   16   18\n [2,]    8   16   24   32   40   48   56   64   72\n [3,]   18   36   54   72   90  108  126  144  162\n [4,]   32   64   96  128  160  192  224  256  288\n [5,]   50  100  150  200  250  300  350  400  450\n [6,]   72  144  216  288  360  432  504  576  648\n [7,]   98  196  294  392  490  588  686  784  882\n [8,]  128  256  384  512  640  768  896 1024 1152\n [9,]  162  324  486  648  810  972 1134 1296 1458\n\n\n상기 행렬을 만들기위해 ifelse를 사용하거나 for 문을 사용하면 좀더 머리가 복잡해 질 수 있다. apply 구문은 확실히 머리가 가벼워진다. tidyverse 부분을 할 때 apply, lapply, sapply등을 다루게 될 것이다."
  },
  {
    "objectID": "221_Rbasic1.html#iris-data-와-apply-구문",
    "href": "221_Rbasic1.html#iris-data-와-apply-구문",
    "title": "2  R 속성",
    "section": "2.6 iris data 와 apply 구문",
    "text": "2.6 iris data 와 apply 구문\nR을 여러 데이터를 이미 내장하고 있습니다. 이를 통해 여러 통계와 기계학습 등을 연습할 수 있도록 돕고 있습니다. 그 중 가장 유명한 자료인 iris 데이터를 불러오겠습니다.\n\ndata(\"iris\")\n\niris 데이터셋은 아이리스 꽃에 대한 측정값을 포함하고 있습니다. 이 데이터셋에는 3개의 다른 아이리스 꽃 종류에 대한 총 150개의 샘플이 있습니다. 데이터의 구조는 다음과 같습니다:\n\nSepal.Length (꽃받침 길이)\nSepal.Width (꽃받침 너비)\nPetal.Length (꽃잎 길이)\nPetal.Width (꽃잎 너비)\nSpecies (종): 아이리스 꽃의 종류를 나타냅니다.\n\n이 데이터셋에는 ‘setosa’(세토사), ‘versicolor’(버시컬러), ‘virginica’(버지니카)라는 3개의 종류가 있습니다.\n\n\n\nlibrary(tidyverse)\niris %>% head()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nSpecies 는 종에 대한 이야기이고, 이것은 문자로 구성되어 있습니다. 나머지는 모두 숫자로 되어 있습니다. 숫자로 되어 있는 변수들에서 평균, 중간, 표준편차를 구해보겠습니다.\n이 예제를 통해 apply 구문이 어떻게 사용되는지 알아보겠습니다. 우선 apply구문을 사용하지 않고 구해보겠습니다.\n\niris_num=iris[, c(1:4)] # Species 를 제외한 나머지 변수만 선정\niris_num=iris %>% select(1:4) # tidyverse를 통해 같은 결과 사용\n# 아래와 같인 평균, sd 를 구했습니다.\niris_num %>% pull(Sepal.Length) %>% mean()\n\n[1] 5.843333\n\niris_num %>% pull(Sepal.Width)  %>% mean()\n\n[1] 3.057333\n\niris_num %>% pull(Petal.Length) %>% mean()\n\n[1] 3.758\n\niris_num %>% pull(Petal.Width)  %>% mean()\n\n[1] 1.199333\n\niris_num %>% pull(Sepal.Length) %>% sd()\n\n[1] 0.8280661\n\niris_num %>% pull(Sepal.Width)  %>% sd()\n\n[1] 0.4358663\n\niris_num %>% pull(Petal.Length) %>% sd()\n\n[1] 1.765298\n\niris_num %>% pull(Petal.Width)  %>% sd()\n\n[1] 0.7622377\n\n\n같은 방식으로 median 도 구할 수 있겠습니다. 그런데, 무언가 반복되는 느낌이 듭니다. 코드를 만들때 반복된다면, 동일한 논리가 반복된다면 함수를 사용할 수 있습니다. 이때 apply 구문을 이용해 보겠습니다. apply는 결과를 백터로, lapply는 결과를 list로 반환해 줍니다. sapply는 lapply 값을 보기 좋게 만들어 줍니다.\n\napply(iris_num, 2, mean)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\nlapply(iris_num, mean)\n\n$Sepal.Length\n[1] 5.843333\n\n$Sepal.Width\n[1] 3.057333\n\n$Petal.Length\n[1] 3.758\n\n$Petal.Width\n[1] 1.199333\n\nsapply(iris_num, mean)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n\n그렇다면 이제는 평균, 중간값, 표준편차 모두를 구해보겠습니다. 반복되는 것이니 함수를 만들겠습니다. stat_smry는\n\nstat_smry = function(x){\n  list(\n    mean  =mean(x), \n    median=median(x), \n    std   = sd(x)\n  )\n}\n\n이제 sapply를 이용하여 작업해 보겠습니다.\n\nsapply(iris_num, stat_smry)\n\n       Sepal.Length Sepal.Width Petal.Length Petal.Width\nmean   5.843333     3.057333    3.758        1.199333   \nmedian 5.8          3           4.35         1.3        \nstd    0.8280661    0.4358663   1.765298     0.7622377  \n\n\n개인적으로 저는 lapply를 사용합니다. 그 이유는 단계별 확인 후 합치는 것이 데이터를 분석할 때 유리한 점이 있기 때문입니다. 단계별 확인이란 반복하는 데이터 별로 어디에 문제가 있는지 확인하는 과정이 데이터가 커질 수록 꼭 필요하기 때문입니다. 아래는 lapply로 tt라는 list를 만들고 확인한 후 do.call(rbidn, .) 모두 합치는 방식입니다 .rbind 는 row bind 로 list 안에 있는 백터, 데이터프레임 등을 누적해서 합쳐서 하나의 파일로 만드는 것입니다. 복잡하지요, 이해보다는 익숙해 지는 순간이 온 것입니다.\n\ntt = lapply(iris_num, stat_smry)\n\n\nnames(tt) # 각 반복된 요인 이름\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n\ntt[[1]]   # 첫번째 여기서는 \"Sepal.Length\"에대한 값\n\n$mean\n[1] 5.843333\n\n$median\n[1] 5.8\n\n$std\n[1] 0.8280661\n\ndo.call(rbind, tt) # 정리된 모양\n\n             mean     median std      \nSepal.Length 5.843333 5.8    0.8280661\nSepal.Width  3.057333 3      0.4358663\nPetal.Length 3.758    4.35   1.765298 \nPetal.Width  1.199333 1.3    0.7622377"
  },
  {
    "objectID": "231_dataimportsave.html#데이터-업로드",
    "href": "231_dataimportsave.html#데이터-업로드",
    "title": "3  데이터 불러오기",
    "section": "3.1 데이터 업로드",
    "text": "3.1 데이터 업로드\n데이터를 불러오는 방법과 저장하는 방법에 대해서 이야기 하겠습니다.\n\n로칼 데이터 불러오기\n\n우선 rstudio 에서 upload 버튼을 사용하는 것입니다. 우측 아래에 있습니다. 여기서 화살표 위로되어 있는 버튼이 업로드 버튼입니다.\n\n\n\n업로드버튼\n\n\n업로드 버튼을 눌러서, 파일을 업로드 하는 방식입니다. 가장 많이 사용하는 방식입니다.\n\n웹이 있는 데이터 불러오기\n\n웹에 있는 데이터가 링크가 있다면 다운로드 받는 방법입니다. download.file 이란 함수를 쓰고, 주소와 저장 장소 및 저장이름을 적어 주는 거입니다.\n\ndownload.file(\"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/kwcsData6th.rds\", \"data/kwcsData6th.rds\")\n\n이 자료를 불러오겠습니다.\n\nkwcs = readRDS(\"data/kwcsData6th.rds\")\n#head(kwcs)\n\n사용설명서도 다운로드 받겠습니다.\n\ndownload.file(\"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/pdf3.pdf\", \"data/kwcs6thkorea.pdf\")"
  },
  {
    "objectID": "231_dataimportsave.html#데이터-불러오기",
    "href": "231_dataimportsave.html#데이터-불러오기",
    "title": "3  데이터 불러오기",
    "section": "3.2 데이터 불러오기",
    "text": "3.2 데이터 불러오기\n\n3.2.1 CSV 파일\nread.csv, write.csv 를 이용합니다. iris 데이터를 이용해서 실습하겠습니다.\n\ndata(iris)\nwrite.csv(iris, \"data/iris.csv\")\niris_import = read.csv(\"data/iris.csv\")\n\ndata(iris) 이 부분은 R의 내장 데이터셋인 iris를 로드합니다. iris 데이터셋은 150개의 관측값과 5개의 변수 (꽃받침의 길이, 꽃받침의 너비, 꽃잎의 길이, 꽃잎의 너비, 종류)로 구성되어 있습니다. write.csv(iris, \"data/iris.csv\") 이 부분은 iris 데이터셋을 CSV 파일 형태로 저장합니다. 여기서 “data/iris.csv”는 파일의 경로와 이름을 나타냅니다. 해당 코드는 현재 작업 중인 디렉토리의 data 폴더 내에 iris.csv라는 이름으로 파일을 저장하려고 시도합니다. 만약 data 폴더가 존재하지 않으면 오류가 발생할 수 있습니다. write.csv 함수는 기본적으로 행 이름도 CSV 파일에 저장합니다. 따라서 CSV 파일에는 추가적으로 행 번호가 포함됩니다. iris_import = read.csv(\"data/iris.csv\") 이 부분은 방금 저장한 iris.csv 파일을 다시 R로 읽어들여 iris_import 변수에 저장합니다. read.csv 함수는 기본적으로 첫 번째 행을 변수명으로 간주합니다. 이 때, 위에서 언급했던 추가적으로 저장된 행 번호는 첫 번째 열로 읽히게 됩니다. 이 열의 이름은 일반적으로 X로 지정됩니다. 요약하면, 이 코드는 R의 iris 데이터셋을 CSV 파일로 저장한 다음, 그 파일을 다시 R로 읽어들이는 과정을 보여줍니다.\n\n\n3.2.2 excel 파일\n\niris 파일 excel로 저장\n\n먼저, Excel 파일로 저장하기 위해 writexl 패키지가 필요합니다. 그리고 xlsx 파일을 불러오기 위해서는 readxl 패키지가 필요합니다. install.packages를 이용하여 패키지를 설치합니다.\n\ninstall.packages(\"writexl\")\ninstall.packages(\"readxl\")\n\n\nlibrary(writexl)\nlibrary(readxl)\n\niris 데이터셋을 Excel 파일로 저장합니다. 이때 write_xlsx를 이용하고, 저장하고자 하는 데이터, 그리고 저장될 이름을 경로와 함께 저장합니다. write_xlsx 함수는 writexl 패키지에 포함된 함수로, 첫 번째 인수로 받은 데이터 프레임(iris)을 두 번째 인수로 받은 경로(“data/iris_saved.xlsx”)에 Excel 파일로 저장합니다.\n\n# iris 데이터셋을 Excel 파일로 저장\nwrite_xlsx(iris, \"data/iris_saved.xlsx\")\n\n\niris_xlsx <- readxl::read_xlsx(\"data/iris_saved.xlsx\")\n\nreadxl::read_xlsx(\"iris_saved.xlsx\"), read_xlsx 함수는 readxl 패키지에 있는 함수로, 인수로 받은 경로의 Excel 파일을 R의 데이터 프레임으로 불러옵니다. rreadxl::: readxl 패키지의 read_xlsx 함수를 호출하기 위해 패키지 이름과 함께 사용되는 방식입니다. 이렇게 하면 해당 패키지를 라이브러리로 로드하지 않고도 패키지의 함수를 사용할 수 있습니다.\n“data/iris_saved.xlsx”: 불러올 Excel 파일의 경로입니다. iris_xlsx: 위 코드에서 불러온 데이터를 저장하는 변수입니다. 불러온 데이터는 이 변수에 저장되므로 후에 iris_xlsx를 사용하여 데이터를 조회하거나 처리할 수 있습니다.\n\n\n3.2.3 stata, sas, spss 파일\nhaven 패키지는 주로 SPSS, Stata, SAS와 같은 다른 통계 소프트웨어에서 사용되는 파일 형식을 R에서 읽고 쓰기 위해 사용됩니다. 이번에는 iris 데이터셋과 haven 패키지를 함께 사용하여, 데이터셋을 이러한 형식으로 저장하고 다시 불러오는 실습을 하겠습니다.\n\ninstall.packages(\"haven\")\n\n\nlibrary(haven)\n\n\nSTATA.dta\n\n\nhaven::write_dta(iris, \"data/iris_stata.dta\")\n\n아마 error 메세지가 위에 처럼 발생할 것입니다. 이는 stata에서 사용하는 데이터 형식에에는 변수명에 .이 있는 경우 error를 발생시킬 수도 있기 때문입니다. 나중에 실습하겠지만, 변수 이름을 바꾸는 과정을 통해 . 을 _로 바꾸어 보겠습니다. 즉 현재는 변수 이름이 Sepal.Length 처럼 되어 있는 것을 Sepal_Length로 변경하는 것입니다. gsub을 이용할 것입니다.\n\nnames(iris) <- gsub(\"\\\\.\", \"_\", names(iris))\nwrite_dta(iris, \"data/iris_saved.dta\")\n\n이후 data 폴더로 가서 외부 반출을 한 다음 stata에서 열어 보면 되겠습니다.\n\n \n\n\nSPASS.sav\n\n\n# iris 데이터셋을 SPSS 파일로 저장\nwrite_sav(iris, \"data/iris_spss.sav\")\n\n\n# 저장한 SPSS 파일을 데이터 프레임으로 불러오기\niris_from_spss <- read_sav(\"data/iris_spss.sav\")\n\nwrite_sav: haven 패키지의 함수로, R의 데이터 프레임을 SPSS .sav 형식으로 저장합니다. read_sav: SPSS .sav 형식의 파일을 R의 데이터 프레임으로 불러옵니다.\n\nSAS, sas7bdat\n\n\n# iris 데이터셋을 SAS 파일로 저장\nwrite_sas(iris, \"data/iris_sas.sas7bdat\")\n\nWarning: `write_sas()` was deprecated in haven 2.5.2.\nℹ Please use `write_xpt()` instead.\n\n\n\n# 저장한 SAS 파일을 데이터 프레임으로 불러오기\niris_from_sas <- read_sas(\"data/iris_sas.sas7bdat\")"
  },
  {
    "objectID": "241_datamanipulation.html#dplyr-tidyverse",
    "href": "241_datamanipulation.html#dplyr-tidyverse",
    "title": "4  Data Manipulation",
    "section": "4.1 dplyr (tidyverse)",
    "text": "4.1 dplyr (tidyverse)\n데이터를 모으는 입장과 데이터를 분석하는 입장은 매우 다릅니다. 데이터를 모을 때는 모으는 연구자의 입장에서 하게되고, 이를 분석하는 연구자에 맞도록 변형하는 과정이 필요합니다. 또한 표를 만들거나 도표를 만드는 과정에서 데이터의 모양을 우리가 원하는 데로 변형할 필요가 있습니다. R에서 이과정을 쉽게 하기 위해 가장 많이 사용되는 것이 dplyr (tidyverse) 입니다. 이를 이용해서 실습해 보겠습니다\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"htmlTable\")) install.packages(\"htmlTable\")\nif(!require(\"haven\")) install.packages(\"haven\")\nif(!require(\"DT\")) install.packages(\"DT\")"
  },
  {
    "objectID": "241_datamanipulation.html#실습-데이터-준비",
    "href": "241_datamanipulation.html#실습-데이터-준비",
    "title": "4  Data Manipulation",
    "section": "4.2 실습 데이터 준비",
    "text": "4.2 실습 데이터 준비\n데이터 표를 만드는 실습은 6차 근로환경조사 자료를 통해 실습할 것입니다.. 자료는 안전보건공단, 근로환경조사 원시자료 사이트 (http://kosha.or.kr/kosha/data/primitiveData.do) 에서 신청할 수 있습니다.. 데이터를 불러오겠습니다. 안전보건공단 홈페이에서 자료를 다운 받는게 원칙입니다. 다만 실습을 빠르게 진행하기 위해서, dspubs.org 페이지에 있는 파일을 이용하겠습니다.   kwcsData6th.rds   자신의 folder에 data 라는 folder가 있는지 확인하십시오. data라는 폴더에 다운로드하고, 불러오도록 하겠습니다.\n\nurl <- \"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/kwcsData6th.rds\"\ndownload.file(url, \"data/tutorKWCS.rds\")\nkwcs = readRDS(\"data/tutorKWCS.rds\")\n\n데이터 10개만 살펴보겠습니다. 이때 head()는 처음 10개, tail() 은 뒤에 10개, slice(5:15) 은 5번째부터 15번째 까지 입니다.\n\nkwcs %>% head() %>% DT::datatable()\n\n\n\n\n\n\n > 설문지와 변수설명 파일은 아래와 같습니다. \n\ndownload.file(\"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/pdf3.pdf\", \"data/kwcs6thkorea.pdf\")"
  },
  {
    "objectID": "241_datamanipulation.html#select",
    "href": "241_datamanipulation.html#select",
    "title": "4  Data Manipulation",
    "section": "4.3 select",
    "text": "4.3 select\n필요한 데이터를 종으로, columns 로 분할하는 것입니다. 즉 변수명으로 데이터를 분할하는 것입니다. 선택될 변수는 변수명과 변수번호(왼쪽 부터 1번)를 이용해서 할 수 있습니다. \n처음부터 5번째 변수까지 선택해 보겠습니다 .\n\nkwcs %>% \n  select(1:5)\n\n\nkwcs %>% \n  select(id, wt, area, hh_num, hm_01_gender) \n\n\n\n# A tibble: 6 × 5\n     id    wt area      hh_num hm_01_gender\n  <dbl> <dbl> <dbl+lbl>  <dbl> <dbl+lbl>   \n1     2 0.137 3 [대구]       1 2 [여성]    \n2     3 0.350 3 [대구]       1 2 [여성]    \n3     6 0.156 7 [울산]       1 2 [여성]    \n4     8 0.228 7 [울산]       3 1 [남성]    \n5     9 0.168 7 [울산]       1 1 [남성]    \n6    10 0.675 6 [대전]       4 2 [여성]    \n\n\n변수명에 job이 들어 있는 경우 선택해 보겠습니다.\n\nkwcs %>% select(contains(\"job\")) %>% head()\n\n# A tibble: 6 × 12\n  job1     job2  job3  job3_…¹ comp_s…² job_c1 job_c1…³ job_c…⁴ job_c…⁵ job_c2  \n  <dbl+lb> <dbl> <dbl> <chr>   <dbl+lb> <dbl+> <dbl+lb> <dbl+l> <dbl+l> <dbl+lb>\n1 1 [1개]  NA    NA    \"\"       2 [대… 60     NA     … NA      NA       1 [그…\n2 1 [1개]  NA    NA    \"\"      NA     … 70     NA     … NA      NA      NA     …\n3 1 [1개]  NA    NA    \"\"       1 [대… 70     NA     … NA      NA      NA     …\n4 1 [1개]  NA    NA    \"\"      NA     … 65     NA     … NA      NA      NA     …\n5 1 [1개]  NA    NA    \"\"       3 [대… NA      1 [가… NA      NA       1 [그…\n6 1 [1개]  NA    NA    \"\"       2 [대… NA      1 [가… NA      NA      NA     …\n# … with 2 more variables: job_c3 <dbl+lbl>, njob <chr>, and abbreviated\n#   variable names ¹​job3_etc, ²​comp_sjob, ³​job_c1_666, ⁴​job_c1_888, ⁵​job_c1_999\n\n\nselect안에서는 ” ” 안에 문자가 변수 명으로 인식됩니다. 이것은 매우 중요한 개념입니다. 아래의 두 코드는 같은 결과를 줍니다. 외부에서 문자를 입력 받았어도, 바로 적용할 수 있다는 의미 입니다.\n\nkwcs %>% select(\"job1\")\n\n# A tibble: 41,108 × 1\n   job1     \n   <dbl+lbl>\n 1 1 [1개]  \n 2 1 [1개]  \n 3 1 [1개]  \n 4 1 [1개]  \n 5 1 [1개]  \n 6 1 [1개]  \n 7 1 [1개]  \n 8 1 [1개]  \n 9 1 [1개]  \n10 1 [1개]  \n# … with 41,098 more rows\n\nkwcs %>% select(job1)\n\n# A tibble: 41,108 × 1\n   job1     \n   <dbl+lbl>\n 1 1 [1개]  \n 2 1 [1개]  \n 3 1 [1개]  \n 4 1 [1개]  \n 5 1 [1개]  \n 6 1 [1개]  \n 7 1 [1개]  \n 8 1 [1개]  \n 9 1 [1개]  \n10 1 [1개]  \n# … with 41,098 more rows\n\n\n제외하는 방법은 -를 사용합니다.\n\ntest <- kwcs %>%\n  select(id, wt, area) %>%\n  head() \n\n\ntest\n\n# A tibble: 6 × 3\n     id    wt area     \n  <dbl> <dbl> <dbl+lbl>\n1     2 0.137 3 [대구] \n2     3 0.350 3 [대구] \n3     6 0.156 7 [울산] \n4     8 0.228 7 [울산] \n5     9 0.168 7 [울산] \n6    10 0.675 6 [대전] \n\n\ntest라는 데이터셑을 만들었습니다. id, wt, area라는 변수를 갖은 데이터 입니다. 여기서 id라는 변수를 제외하겠습니다.\n\ntest %>% select(-1)\n\n# A tibble: 6 × 2\n     wt area     \n  <dbl> <dbl+lbl>\n1 0.137 3 [대구] \n2 0.350 3 [대구] \n3 0.156 7 [울산] \n4 0.228 7 [울산] \n5 0.168 7 [울산] \n6 0.675 6 [대전] \n\ntest %>% select(-id)\n\n# A tibble: 6 × 2\n     wt area     \n  <dbl> <dbl+lbl>\n1 0.137 3 [대구] \n2 0.350 3 [대구] \n3 0.156 7 [울산] \n4 0.228 7 [울산] \n5 0.168 7 [울산] \n6 0.675 6 [대전]"
  },
  {
    "objectID": "241_datamanipulation.html#filter",
    "href": "241_datamanipulation.html#filter",
    "title": "4  Data Manipulation",
    "section": "4.4 filter",
    "text": "4.4 filter\nfilter 는 특정 조건에 해당하는 행(row)를 선택합니다. 예를 들어 성별 중 남성만, 여성만 선택한 다는가 특정 연령 범위를 선택합니다. 이때 몇몇 조건문을 사용합니다. ==은 같다는 뜻입니다. 아래와 같이 활용합니다.\n\nkwcs %>%\n  select(TSEX, AGE) %>%\n  filter(AGE ==55) %>%\n  head()\n\n# A tibble: 6 × 2\n  TSEX        AGE\n  <dbl+lbl> <dbl>\n1 2 [여성]     55\n2 2 [여성]     55\n3 2 [여성]     55\n4 1 [남성]     55\n5 1 [남성]     55\n6 1 [남성]     55\n\n\n&는 and |는 or 을 의미합니다. 성별과 연령을 동시에 조건을 주도록 하겠습니다.\n\nkwcs %>% \n  select(TSEX, AGE) %>%\n  filter(TSEX ==1 & AGE <17) # or == |\n\n# A tibble: 0 × 2\n# … with 2 variables: TSEX <dbl+lbl>, AGE <dbl>\n\n\n!는 아니라는 표시입니다. 이것을 통해서 간단하게 여러 조건을 생략시킬 수 있습니다.\n\nkwcs %>%\n  select(TSEX, AGE) %>%\n  filter(TSEX !=1) %>%\n  filter(AGE >=16 & AGE <18)\n\n# A tibble: 0 × 2\n# … with 2 variables: TSEX <dbl+lbl>, AGE <dbl>\n\n\n%in%를 사용하면 여러 조건을 나열하는 방법으로 선택할 수 있습니다. 명목변수 등에 사용하기 편합니다.\n\nkwcs %>%\n  select(TSEX, AGE) %>%\n  filter(TSEX %in% c(1, 2)) %>% # %in% allow multiple filtering\n  filter(!AGE >16) # ! means negative condition\n\n# A tibble: 0 × 2\n# … with 2 variables: TSEX <dbl+lbl>, AGE <dbl>"
  },
  {
    "objectID": "241_datamanipulation.html#arrange",
    "href": "241_datamanipulation.html#arrange",
    "title": "4  Data Manipulation",
    "section": "4.5 arrange",
    "text": "4.5 arrange\narrange는 순차 정렬하는 함수입니다. 정방향, 역방향이 가능합니다.\n\nkwcs %>%\n  select(AGE) %>%\n  arrange(AGE) %>%\n  head()\n\n# A tibble: 6 × 1\n    AGE\n  <dbl>\n1    19\n2    19\n3    19\n4    19\n5    19\n6    19\n\nkwcs %>%\n  select(AGE) %>%\n  arrange(desc(AGE)) %>%\n  head()\n\n# A tibble: 6 × 1\n    AGE\n  <dbl>\n1    69\n2    69\n3    69\n4    69\n5    69\n6    69"
  },
  {
    "objectID": "241_datamanipulation.html#mutate",
    "href": "241_datamanipulation.html#mutate",
    "title": "4  Data Manipulation",
    "section": "4.6 mutate",
    "text": "4.6 mutate\nmutate는 변수를 변형하는 가장 기본이면서 자주 사용되는 함수 입니다. 반드시 익숙해여쟈 하는 함수 입니다. mutate 단독으로 쓰이기 보다는 여러 조건문인 ifelse, case_when, recode를 이용합니다. 될수 있으면 ifelse보다는 case_when을 이용하는 것이 향후 SQL 등을 사용할 때 더 편할 수 있어 추천합니다.  남녀가 현재는 1인 남자, 2가 여자 입니다. 이를 male, female로 바꾸겠습니다.\n\nkwcs %>%\n  select(TSEX, AGE) %>%\n  mutate(sexgp = case_when(\n    TSEX==1 ~ 'male', \n    TSEX==2 ~ 'female'\n  )) %>%\n  head()\n\n# A tibble: 6 × 3\n  TSEX        AGE sexgp \n  <dbl+lbl> <dbl> <chr> \n1 2 [여성]     54 female\n2 2 [여성]     64 female\n3 2 [여성]     65 female\n4 2 [여성]     57 female\n5 1 [남성]     38 male  \n6 2 [여성]     47 female\n\n\n이번에는 연령을 5세 단위로 바꾸어 보겠습니다. 그리고 test1이라는 데이터로 변형시켜보겠습니다.\n\ntest1 = kwcs %>%\n  select(TSEX, AGE) %>%\n  mutate(sexgp = case_when(\n    TSEX==1 ~ 'male', \n    TSEX==2 ~ 'female'\n  )) %>%\n  mutate(agegp = case_when(\n    AGE <25 ~ \"<25\",\n    AGE <30 ~ \"<30\", \n    AGE <35 ~ \"<35\", \n    AGE <40 ~ \"<40\", \n    AGE <45 ~ \"<45\", \n    AGE <50 ~ \"<50\",\n    AGE <55 ~ \"<55\", \n    AGE <60 ~ \"<60\",\n    TRUE ~ \"\\u226560\" # 나머지는 모두 >65 (\\u2265는 크거나 같다는 symbol)\n  )) %>%\n  slice(1:10)\n\ntest1에는 sexgp가 female , male 이라고 되어 있네요, 이때 female을 Female로 대문자 변화, male을 Male로 대문자 변환해 보겠습니다. recode를 사용하겠습니다.\n\ntest1 \n\n# A tibble: 10 × 4\n   TSEX        AGE sexgp  agegp\n   <dbl+lbl> <dbl> <chr>  <chr>\n 1 2 [여성]     54 female <55  \n 2 2 [여성]     64 female ≥60  \n 3 2 [여성]     65 female ≥60  \n 4 2 [여성]     57 female <60  \n 5 1 [남성]     38 male   <40  \n 6 2 [여성]     47 female <50  \n 7 2 [여성]     54 female <55  \n 8 2 [여성]     35 female <40  \n 9 2 [여성]     68 female ≥60  \n10 2 [여성]     69 female ≥60  \n\n\n만연 female, male 두개뿐이라면 이 방법이 쉬울 수 있습니다. case_when과 female, male을 적절히 사용하면 되겠습니다.\n\ntest1 %>%\n  mutate(Sexgp = recode(sexgp, \n                        \"female\" = \"Female\", \n                        \"male\"   = \"Male\"))\n\n# A tibble: 10 × 5\n   TSEX        AGE sexgp  agegp Sexgp \n   <dbl+lbl> <dbl> <chr>  <chr> <chr> \n 1 2 [여성]     54 female <55   Female\n 2 2 [여성]     64 female ≥60   Female\n 3 2 [여성]     65 female ≥60   Female\n 4 2 [여성]     57 female <60   Female\n 5 1 [남성]     38 male   <40   Male  \n 6 2 [여성]     47 female <50   Female\n 7 2 [여성]     54 female <55   Female\n 8 2 [여성]     35 female <40   Female\n 9 2 [여성]     68 female ≥60   Female\n10 2 [여성]     69 female ≥60   Female\n\n\n60세 미만을 young, 60세 이상을 old로 구분하여 young female, old female, young male, old male로 바꾸어 보겠습니다. case_when에는 처음에 사용한 것을 제외하고 나머지에서라는 뜻이 포함되어 있지요. 즉 ifelse 가 이미 숨어있습니다.\n\ntest1 %>%\n  mutate(intgp = case_when(\n    AGE <60 & TSEX == 1 ~ \"young male\", \n    AGE <60 & TSEX == 2 ~ \"young female\", \n    TSEX == 1 ~ \"old male\", \n    TSEX == 2 ~ \"old female\" \n  ))\n\n# A tibble: 10 × 5\n   TSEX        AGE sexgp  agegp intgp       \n   <dbl+lbl> <dbl> <chr>  <chr> <chr>       \n 1 2 [여성]     54 female <55   young female\n 2 2 [여성]     64 female ≥60   old female  \n 3 2 [여성]     65 female ≥60   old female  \n 4 2 [여성]     57 female <60   young female\n 5 1 [남성]     38 male   <40   young male  \n 6 2 [여성]     47 female <50   young female\n 7 2 [여성]     54 female <55   young female\n 8 2 [여성]     35 female <40   young female\n 9 2 [여성]     68 female ≥60   old female  \n10 2 [여성]     69 female ≥60   old female"
  },
  {
    "objectID": "241_datamanipulation.html#group_by",
    "href": "241_datamanipulation.html#group_by",
    "title": "4  Data Manipulation",
    "section": "4.7 group_by",
    "text": "4.7 group_by\ngroup_by는 데이터 탐색에서 가장 많이 사용되면, 연속변수는 summrise와 명목변수는 count와 같이 사용됩니다. group_by에 의해 변수값에 따라 정리가 되게 됩니다. 새로운 변수은 heal_prob1를 사용해 보겠습니다. heal_prob1는 요통여 부이고, 요통이 있으면 1, 없으면 2로 되어 있습니다. 결측값, 무응답이 많으니 heal_prob1 에 무응답이 있는 경우 제외하겠습니다. NA 결측값은 is.na(변수)로 찾을 수 있습니다. 우선 count로 해보겠습니다 .\n\nkwcs %>% \n  count(heal_prob1)\n\n# A tibble: 3 × 2\n  heal_prob1     n\n  <dbl+lbl>  <int>\n1  1 [있다]  11642\n2  2 [없다]  29437\n3 NA            29\n\n\nNA가 59개 있네요.  아래와 같이 sexgp별로 요통을 호소하는 사람이 얼마나 있는지 확인해 보겠습니다.\n\nkwcs %>%\n  filter(!is.na(heal_prob1)) %>% # is.na() 즉 NA이면에서 !는 NA가 아니면이라는 뜻\n  select(TSEX, AGE, heal_prob1) %>%\n  mutate(sexgp=case_when(\n    TSEX==1 ~ \"Men\", \n    TRUE ~ \"Female\"\n  )) %>%\n  group_by(sexgp) %>%\n  count(heal_prob1) \n\n# A tibble: 4 × 3\n# Groups:   sexgp [2]\n  sexgp  heal_prob1     n\n  <chr>  <dbl+lbl>  <int>\n1 Female 1 [있다]    6771\n2 Female 2 [없다]   15104\n3 Men    1 [있다]    4871\n4 Men    2 [없다]   14333\n\n\n연령별로는 어떠한 비율인지 확인해 보겠습니다.\n\nkwcs %>%\n  filter(!is.na(heal_prob1)) %>% # is.na() 즉 NA이면에서 !는 NA가 아니면이라는 뜻\n  select(TSEX, AGE, heal_prob1) %>%\n  mutate(sexgp=case_when(\n    TSEX==1 ~ \"Men\", \n    TRUE ~ \"Female\"\n  )) %>%\n  mutate(agegp = case_when(\n    AGE <25 ~ \"<25\",\n    AGE <30 ~ \"<30\", \n    AGE <35 ~ \"<35\", \n    AGE <40 ~ \"<40\", \n    AGE <45 ~ \"<45\", \n    AGE <50 ~ \"<50\",\n    AGE <55 ~ \"<55\", \n    AGE <60 ~ \"<60\",\n    TRUE ~ \"\\u226560\" # 나머지는 모두 >65 (\\u2265는 크거나 같다는 symbol)\n  ))  %>%\n  group_by(agegp) %>%\n  count(heal_prob1)\n\n# A tibble: 18 × 3\n# Groups:   agegp [9]\n   agegp heal_prob1     n\n   <chr> <dbl+lbl>  <int>\n 1 <25   1 [있다]     144\n 2 <25   2 [없다]    1263\n 3 <30   1 [있다]     397\n 4 <30   2 [없다]    2462\n 5 <35   1 [있다]     578\n 6 <35   2 [없다]    2954\n 7 <40   1 [있다]     891\n 8 <40   2 [없다]    3430\n 9 <45   1 [있다]    1103\n10 <45   2 [없다]    3679\n11 <50   1 [있다]    1427\n12 <50   2 [없다]    3892\n13 <55   1 [있다]    1718\n14 <55   2 [없다]    4043\n15 <60   1 [있다]    2046\n16 <60   2 [없다]    3616\n17 ≥60   1 [있다]    3338\n18 ≥60   2 [없다]    4098\n\n\n한눈에 파악하기 어렵네요. 그럼 어떻게 하는 것이 좋을 까요, mutate를 통해 율을 구해보는 것이 좋겠습니다.\n\nkwcs %>%\n  filter(!is.na(heal_prob1)) %>% # is.na() 즉 NA이면에서 !는 NA가 아니면이라는 뜻\n  select(TSEX, AGE, heal_prob1) %>%\n  mutate(sexgp=case_when(\n    TSEX==1 ~ \"Men\", \n    TRUE ~ \"Female\"\n  )) %>%\n  mutate(agegp = case_when(\n    AGE <25 ~ \"<25\",\n    AGE <30 ~ \"<30\", \n    AGE <35 ~ \"<35\", \n    AGE <40 ~ \"<40\", \n    AGE <45 ~ \"<45\", \n    AGE <50 ~ \"<50\",\n    AGE <55 ~ \"<55\", \n    AGE <60 ~ \"<60\",\n    TRUE ~ \"\\u226560\" # 나머지는 모두 >65 (\\u2265는 크거나 같다는 symbol)\n  ))  %>%\n  group_by(agegp) %>%\n  count(heal_prob1) %>%\n  mutate(prob = n/sum(n)) %>% #proportion을 구함\n  filter(heal_prob1 == 1) # 요통이 있다고한 사람을 비율만 남김. \n\n# A tibble: 9 × 4\n# Groups:   agegp [9]\n  agegp heal_prob1     n  prob\n  <chr> <dbl+lbl>  <int> <dbl>\n1 <25   1 [있다]     144 0.102\n2 <30   1 [있다]     397 0.139\n3 <35   1 [있다]     578 0.164\n4 <40   1 [있다]     891 0.206\n5 <45   1 [있다]    1103 0.231\n6 <50   1 [있다]    1427 0.268\n7 <55   1 [있다]    1718 0.298\n8 <60   1 [있다]    2046 0.361\n9 ≥60   1 [있다]    3338 0.449\n\n\n요통의 유병율이 연령이 증가할 수록 점차 증가하고 있네요, 남녀의 차이가 있을 까요? 남녀별 구분을 위해 group_by( )에 sexgp를 추가했습니다 .\n\nsmry1 = kwcs %>%\n  filter(!is.na(heal_prob1)) %>% # is.na() 즉 NA이면에서 !는 NA가 아니면이라는 뜻\n  select(TSEX, AGE, heal_prob1) %>%\n  mutate(sexgp=case_when(\n    TSEX==1 ~ \"Men\", \n    TRUE ~ \"Female\"\n  )) %>%\n  mutate(agegp = case_when(\n    AGE <25 ~ \"<25\",\n    AGE <30 ~ \"<30\", \n    AGE <35 ~ \"<35\", \n    AGE <40 ~ \"<40\", \n    AGE <45 ~ \"<45\", \n    AGE <50 ~ \"<50\",\n    AGE <55 ~ \"<55\", \n    AGE <60 ~ \"<60\",\n    TRUE ~ \"\\u226560\" # 나머지는 모두 >65 (\\u2265는 크거나 같다는 symbol)\n  ))  %>%\n  group_by(sexgp, agegp) %>% # 성별, 연령별\n  count(heal_prob1) %>%\n  mutate(prob = n/sum(n)) %>% #proportion을 구함\n  filter(heal_prob1 == 1) # 요통이 있다고한 사람을 비율만 남김. \nsmry1\n\n# A tibble: 18 × 5\n# Groups:   sexgp, agegp [18]\n   sexgp  agegp heal_prob1     n   prob\n   <chr>  <chr> <dbl+lbl>  <int>  <dbl>\n 1 Female <25   1 [있다]      79 0.105 \n 2 Female <30   1 [있다]     225 0.164 \n 3 Female <35   1 [있다]     314 0.180 \n 4 Female <40   1 [있다]     416 0.205 \n 5 Female <45   1 [있다]     558 0.236 \n 6 Female <50   1 [있다]     761 0.266 \n 7 Female <55   1 [있다]    1041 0.309 \n 8 Female <60   1 [있다]    1316 0.390 \n 9 Female ≥60   1 [있다]    2061 0.515 \n10 Men    <25   1 [있다]      65 0.0997\n11 Men    <30   1 [있다]     172 0.116 \n12 Men    <35   1 [있다]     264 0.148 \n13 Men    <40   1 [있다]     475 0.207 \n14 Men    <45   1 [있다]     545 0.225 \n15 Men    <50   1 [있다]     666 0.271 \n16 Men    <55   1 [있다]     677 0.284 \n17 Men    <60   1 [있다]     730 0.319 \n18 Men    ≥60   1 [있다]    1277 0.372 \n\n\n\nsummarise\n\nsummarise는 colum 별로 정리하여 나타낸는 함수 입니다. 이때 주로 같이 사용하는 함수는 mean, sd, median, max, min 등이며, quantile도 많이 사용됩니다.\n\nkwcs %>%\n  summarise(mean_age = mean(AGE), \n            std_age  = sd(AGE))\n\n# A tibble: 1 × 2\n  mean_age std_age\n     <dbl>   <dbl>\n1     47.0    12.4\n\n\ngroup_by와 함께 사용해 볼까요?\n\nkwcs %>%\n  filter(!is.na(heal_prob1)) %>% # is.na() 즉 NA이면에서 !는 NA가 아니면이라는 뜻\n  select(TSEX, AGE, heal_prob1) %>%\n  mutate(sexgp=case_when(\n    TSEX==1 ~ \"Men\", \n    TRUE ~ \"Female\"\n  )) %>%\n  group_by(sexgp) %>%\n  summarise(mean_age = mean(AGE), \n            std_age  = sd(AGE))\n\n# A tibble: 2 × 3\n  sexgp  mean_age std_age\n  <chr>     <dbl>   <dbl>\n1 Female     47.6    12.2\n2 Men        46.3    12.5\n\n\n중요한 부분인 group_by를 한후에 group을 해제하기 위해서는 ungroup()을 해주어야 합니다. 향후 Table 만들기 등에서 활용되니 기역해 주세요."
  },
  {
    "objectID": "241_datamanipulation.html#merge-join",
    "href": "241_datamanipulation.html#merge-join",
    "title": "4  Data Manipulation",
    "section": "4.8 merge, join",
    "text": "4.8 merge, join\n이번에는 두개의 테이블을 하나로 만들어 보겠습니다.\n\ntab1 <- tibble(id = c(1, 2), var1 = c(\"a1\", \"a2\"))\ntab2 <- tibble(id = c(2, 3), var2 = c(\"b1\", \"b2\"))\n\n\ntab1\n\n# A tibble: 2 × 2\n     id var1 \n  <dbl> <chr>\n1     1 a1   \n2     2 a2   \n\ntab2\n\n# A tibble: 2 × 2\n     id var2 \n  <dbl> <chr>\n1     2 b1   \n2     3 b2   \n\n\n이둘을 합쳐 보겠습니다. >full join\n\ntab1 %>%\n  full_join(tab2, by=c(\"id\"))\n\n# A tibble: 3 × 3\n     id var1  var2 \n  <dbl> <chr> <chr>\n1     1 a1    <NA> \n2     2 a2    b1   \n3     3 <NA>  b2   \n\n\n\ninner join\n\n\ntab1 %>%\n  inner_join(tab2, by=c(\"id\"))\n\n# A tibble: 1 × 3\n     id var1  var2 \n  <dbl> <chr> <chr>\n1     2 a2    b1   \n\n\n\nright join\n\n\ntab1 %>%\n  right_join(tab2, by=c(\"id\"))\n\n# A tibble: 2 × 3\n     id var1  var2 \n  <dbl> <chr> <chr>\n1     2 a2    b1   \n2     3 <NA>  b2   \n\n\n\nleft join\n\n\ntab1 %>%\n  left_join(tab2, by=c(\"id\"))\n\n# A tibble: 2 × 3\n     id var1  var2 \n  <dbl> <chr> <chr>\n1     1 a1    <NA> \n2     2 a2    b1   \n\n\n어떻게 결과가 예상과 같았나요? 아래와 같이 정리할 수 있습니다."
  },
  {
    "objectID": "245_datamanipulation_for_table.html#tidyverse",
    "href": "245_datamanipulation_for_table.html#tidyverse",
    "title": "5  보건학표_1",
    "section": "5.1 tidyverse",
    "text": "5.1 tidyverse\n데이터를 모으는 입장과 데이터를 분석하는 입장은 매우 다릅니다. 데이터를 모을 때는 모으는 연구자의 입장에서 하게되고, 이를 분석하는 연구자에 맞도록 변형하는 과정이 필요합니다. 또한 표를 만들거나 도표를 만드는 과정에서 데이터의 모양을 우리가 원하는 데로 변형할 필요가 있습니다. R에서 이과정을 쉽게 하기 위해 가장 많이 사용되는 것이 tidyverse 입니다. 이를 이용해서 실습해 보겠습니다\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"htmlTable\")) install.packages(\"htmlTable\")\nif(!require(\"haven\")) install.packages(\"haven\")\nif(!require(\"DT\")) install.packages(\"DT\")"
  },
  {
    "objectID": "245_datamanipulation_for_table.html#실습-데이터-준비",
    "href": "245_datamanipulation_for_table.html#실습-데이터-준비",
    "title": "5  보건학표_1",
    "section": "5.2 실습 데이터 준비",
    "text": "5.2 실습 데이터 준비\n데이터 표를 만드는 실습은 6차 근로환경조사 자료를 통해 실습할 것입니다.. 자료는 안전보건공단, 근로환경조사 원시자료 사이트 (http://kosha.or.kr/kosha/data/primitiveData.do) 에서 신청할 수 있습니다.. 데이터를 불러오겠습니다. 안전보건공단 홈페이에서 자료를 다운 받는게 원칙입니다. 다만 실습을 빠르게 진행하기 위해서, dspubs.org 페이지에 있는 파일을 이용하겠습니다.   kwcsData6th.rds   자신의 folder에 data 라는 folder가 있는지 확인하십시오. data라는 폴더에 다운로드하고, 불러오도록 하겠습니다.\n\nurl <- \"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/kwcsData6th.rds\"\ndownload.file(url, \"data/tutorKWCS.rds\")\n\n\nkwcs = readRDS(\"data/tutorKWCS.rds\")\n\nunicode chart\n표를 만들다 보면 크다, 작다, 같다 등은 표시가 쉬운 반면, 크거나 같다, 작거나 같다 등은 표시가 어렵습니다. 이 때 사용해야할 것이 unicode 입니다. 보건학에서 필요한 대표적 유니코드는 사실 2개입니다. 크거나 같다, 작거나 같다. 나머지는 키보드에 이미 있으니 이것을 사용하면 됩니다. 추가적인 unicode는 아래의 항목을 통해 살펴 볼수 있습니다.   Unicode Chart \n\ntibble(\n  \"symbole    \" = c(\"\\u2264\", \"\\u2265\", \"\\u00b1\"), \n  \"unicode    \" = c(\"u2264\", \"u2265\", \"u00b1\")\n) %>% \n  #addHtmlTableStyle(css.cell = c(\"width: 100;\",\"width: 100;\")) %>%\n  htmlTable(caption =\"Simple unicode and symbols\")\n\n\n\n\n\nSimple unicode and symbols\n\nsymbole    \nunicode    \n\n\n\n\n1\n≤\nu2264\n\n\n2\n≥\nu2265\n\n\n3\n±\nu00b1\n\n\n\n\n\n다음과 같이 사용할 수 있습니다. 여기사 \\는 “escape character”로 뒤에 오는 것이 문자가 아니라 약솓된 결과를 나타내 달라는 뜻입니다.\n\nprint(\"x \\u2264 10\")\n\n[1] \"x ≤ 10\"\n\n\n그럼 어떻게 문자를 사용할까요? print 명령을 위와 같이 사용하는 것도 좋지만, 변수를 생성하기에는 좋은 방법이 따로 있습니다. 좋은 방법이란 처음에는 어렵지만, 알고나면 엄청난 것들을 말합니다. paste와 sprintf 입니다. 어떤 것이 좋은가요?. 우리의 목표가 데이터 표현을 자동화 하는 것인데, 자동화를 위해서는 구조를 만들고 거기에 값을 대입 시키는 것이 기본입니다. 그러니, sprintf 를 더 자주 사용하게 됩니다. paste는 모두 붙여 주는 것이니, 쉽게 이해될 것이고, sprintf는 `%s` 마다 무언가를 넣어 붙여 주는 방식입니다. paste와 sprintf는 정말 자주 사용되는 함수이니 꼭 익숙해 지시기 바랍니다.\n\nxp1 = paste(\"x\", \"\\u2264\", \"10\")\nxp2 = sprintf(\"x %s 10\", \"\\u2264\")\nxp1\n\n[1] \"x ≤ 10\"\n\nxp2\n\n[1] \"x ≤ 10\"\n\n\n\n데이터 확인\n\n데이터의 변수를 확인하는 방법은 colnames() , names() 또는 head()를 하용하는 것입니다. 약 400개의 변수가 있으므로, 처음부터 10번째까지 \\[1:10\\] 변수를 찾아 보겠습니다.\n\ncolnames(kwcs)[1:10]\n\n [1] \"id\"           \"wt\"           \"area\"         \"hh_num\"       \"hm_01_gender\"\n [6] \"hm_01_year\"   \"hm_01_estat\"  \"hm_01_rel_t\"  \"hm_02_gender\" \"hm_02_year\"  \n\n\n그럼 45번째 변수 부터 50번째 변수까지 찾아 보겠습니다. []안을 채워보세요.\n\ncolnames(kwcs)[     ]\n\n\n\n[1] \"target\"      \"YEAR\"        \"ESTAT\"       \"AGE\"         \"country\"    \n[6] \"country_etc\"\n\n\n연령인 AGE가 있네요, 성별에 대한 AGE도 있습니다. 그럼 이것을 이용해서 실습을 해보겠습니다. 변수를 하나 선택하는 것은 데이터에 $ 표시를 하고 이후에 변수를 넣는 방식입니다. “데이터\\(변수\" 입니다. 10개만 보겠습니다. 숫자 처럼 보이네요, 확인하겠습니다. `class(kwcs\\)AGE)`를 이용해봅니다. numberic 으로 숫자입니다. 숫자여야 평균 표준편차 등의 계산이 가능합니다.\n\nkwcs$AGE[1:10]\n\n [1] 54 64 65 57 38 47 54 35 68 69\n\nclass(kwcs$AGE)\n\n[1] \"numeric\"\n\n\n이번에는 SEX 변수(variable)의 변수값(value)를 살펴 보겠습니다. class 가 무엇일까요? double 또는 interger 라고 나올 텐데요, R에서는 numberic 값에 interger와 double 속성을 사용한다고 생각하시면 됩니다. 다만 label을 붙여 놓아서 알기 쉽게 되어 있네요.\n\nkwcs$TSEX[1:10]\n\n<labelled<double>[10]>\n [1] 2 2 2 2 1 2 2 2 2 2\n\nLabels:\n value label\n     1  남성\n     2  여성\n\nclass(kwcs$TSEX)\n\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"double\"        \n\n\n국적에 대해서도 알아보겠습니다. character 이네요. 한국에서 시행한 조사라, 한국 국적은 빈칸으로, 이외에는 국적을 적었습니다. kwcs$country_etc를 해보면, 대부분 빈칸입니다. 빈칸인 경 ==\"\", 빈칸이 아닌 경우 !=\"\"을 이용해서 어떤 국적이 있는지 살펴 보겠습니다.\n\nclass(kwcs$country_etc)\n\n[1] \"character\"\n\nkwcs$country_etc[kwcs$country_etc != \"\"][1:10]\n\n [1] \"베트남\"   \"중국\"     \"캐나다\"   \"중국\"     \"중국\"     \"중국\"    \n [7] \"중국\"     \"파키스탄\" \"중국\"     \"중국\"    \n\n\n근로자의 지위를 알아 보겠습니다. 근로자 지위는 emp_stat 입니다. 1은 상용근로자, 2는 임시근로자, 3은 일용근로자 입니다.\n\nkwcs$emp_stat \n\n\nkwcs$emp_stat %>% head()\n\n<labelled<double>[6]>\n[1]  1  3  1 NA  1  1\n\nLabels:\n value      label\n     1 상용근로자\n     2 임시근로자\n     3 일용근로자\n\n\n매우 중요한 개념이 나옵니다. 빈칸과 NA 입니다. 모두 값에 대한 정보가 없다는 것입니다. 값에 대한 정보가 없으면 어떻게 해야 할지는 매우 중요한 개념입니다. 우선 여기서는 값에 대한 정보가 없는 것을 제외하고 분석해 보겠습니다. 값에 대한 정보가 없는 데이터는 제거하여, 새로운 데이터를 만들겠습니다. 이때 filter 라는 것을 이용합니다. is.na라는 것은 NA라는 것을 의미하고, 앞에 !는 그 반대를 말합니다.\n\nkwcs %>%\n  filter(!is.na(emp_stat))\n\n# A tibble: 27,908 × 459\n      id    wt area      hh_num hm_01_gender hm_01_year hm_01_estat  hm_01_rel_t\n   <dbl> <dbl> <dbl+lbl>  <dbl> <dbl+lbl>         <dbl> <dbl+lbl>    <dbl+lbl>  \n 1     2 0.137 3 [대구]       1 2 [여성]           1966 1 [임금 근… 0 [응답자 …\n 2     3 0.350 3 [대구]       1 2 [여성]           1956 1 [임금 근… 0 [응답자 …\n 3     6 0.156 7 [울산]       1 2 [여성]           1955 2 [일이 있… 0 [응답자 …\n 4     9 0.168 7 [울산]       1 1 [남성]           1982 1 [임금 근… 0 [응답자 …\n 5    10 0.675 6 [대전]       4 2 [여성]           1973 1 [임금 근… 0 [응답자 …\n 6    18 2.71  6 [대전]       4 1 [남성]           1972 1 [임금 근… 0 [응답자 …\n 7    19 3.83  3 [대구]       5 1 [남성]           1969 1 [임금 근… 3 [부모]  …\n 8    20 2.90  3 [대구]       4 1 [남성]           1960 1 [임금 근… 3 [부모]  …\n 9    22 2.37  3 [대구]       4 1 [남성]           1984 1 [임금 근… 0 [응답자 …\n10    23 2.26  3 [대구]       3 1 [남성]           1964 1 [임금 근… 3 [부모]  …\n# ℹ 27,898 more rows\n# ℹ 451 more variables: hm_02_gender <dbl+lbl>, hm_02_year <dbl>,\n#   hm_02_estat <dbl+lbl>, hm_02_rel_t <dbl+lbl>, hm_03_gender <dbl+lbl>,\n#   hm_03_year <dbl>, hm_03_estat <dbl+lbl>, hm_03_rel_t <dbl+lbl>,\n#   hm_04_gender <dbl+lbl>, hm_04_year <dbl>, hm_04_estat <dbl+lbl>,\n#   hm_04_rel_t <dbl+lbl>, hm_05_gender <dbl+lbl>, hm_05_year <dbl>,\n#   hm_05_estat <dbl+lbl>, hm_05_rel_t <dbl+lbl>, hm_06_gender <dbl+lbl>, …\n\n\n요통에 대한 변수는 heal_prob1 입니다. 이 변수를 확인해 보겠습니다. 1번은 있다, 2번은 업다, 8번은 무응답, 9번은 거절입니다. 이제 빈칸과 NA가 아니더라도 필요없는 정보가 있습니다. 모르거나/무응답했거나, 거절한 사람입니다. 이를 제거해 보겠습니다.\n\nkwcs %>% \n  filter(!is.na(emp_stat)) %>%\n  filter(!is.na(heal_prob1)) %>%\n  filter(heal_prob1 !=8) %>%\n  filter(heal_prob1 !=9)\n\n이번에는 heal_prob1 에 1, 2 인 사람만 포함시켜 보겠습니다. 어떤 것이 더 편한가요?\n\nkwcs %>% \n  filter(!is.na(emp_stat)) %>%\n  filter(!is.na(heal_prob1)) %>%\n  filter(heal_prob1 %in% c(1, 2))\n\n이번에는 sleep1이라는 변수를 살펴 보겠습니다. sleep1이라는 변수는 잠들기 어려운 것이 매일(1), 한주에 여러번 (2), 한달에 여러번(3), 드물게(5), 전혀 없음(5) 의 5점 척도 입니다. 이데 모름과 거절, 그리고 NA 값을 제거해 보겠습니다.\n그리고 dat라는 새로운 data를 만들고 kwcs를 제거 하겠습니다. dat를 이용해서 분석을 해보겠습니다. 지금까지 사용했던, TSEX, AGE, emp_stat, heal_prob1, sleep1 의 변수를 사용하겠습니다.\n\ndat <- kwcs %>% \n  filter(!is.na(emp_stat)) %>%\n  filter(!is.na(heal_prob1)) %>%\n  filter(heal_prob1 %in% c(1, 2)) %>%\n  filter(sleep1 %in% c(1:5)) %>%\n  select(TSEX, AGE, emp_stat, heal_prob1, sleep1, sleep2, sleep3)\n\n\nrm(kwcs)\n\ndat를 살펴보겠습니다.\n\nhead(dat)\n\n# A tibble: 6 × 7\n  TSEX        AGE emp_stat       heal_prob1 sleep1              sleep2   sleep3 \n  <dbl+lbl> <dbl> <dbl+lbl>      <dbl+lbl>  <dbl+lbl>           <dbl+lb> <dbl+l>\n1 2 [여성]     54 1 [상용근로자] 2 [없다]   5 [전혀없음]        5 [전혀… 5 [전…\n2 2 [여성]     64 3 [일용근로자] 2 [없다]   2 [한 주에 여러 번] 2 [한 … 2 [한 …\n3 2 [여성]     65 1 [상용근로자] 2 [없다]   5 [전혀없음]        5 [전혀… 5 [전…\n4 1 [남성]     38 1 [상용근로자] 2 [없다]   5 [전혀없음]        5 [전혀… 5 [전…\n5 2 [여성]     47 1 [상용근로자] 2 [없다]   5 [전혀없음]        4 [드물… 5 [전…\n6 1 [남성]     48 1 [상용근로자] 1 [있다]   5 [전혀없음]        5 [전혀… 5 [전…"
  },
  {
    "objectID": "245_datamanipulation_for_table.html#central-tendency-mean-median-mode",
    "href": "245_datamanipulation_for_table.html#central-tendency-mean-median-mode",
    "title": "5  보건학표_1",
    "section": "5.3 Central Tendency (mean, median, mode)",
    "text": "5.3 Central Tendency (mean, median, mode)\n대표값중 가장 많이 사용하는 것은 mean 과 median 입니다. 이것을 나타내는 표를 만들어 봅시다. 이것의 평균과 표준 편차를 을 구해 보겠습니다.\n\nmean(dat$AGE)\n\n[1] 44.42325\n\nsd(dat$AGE)\n\n[1] 12.25356\n\n\n이번에는 “tidyverse”를 통해 pipe 코드를 짜 보겠습니다. 데이터를 변형하는 것은 다른 시간에 수행하겠지만, 여기서는 select와 filter, mutate, group_by를 사용하겠습니다. kwcs$TSEX 는 kwcs에서 TSEX를 select하라는 것으로 다름과 같이 사용할 수 있습니다.\n\ndat %>% select(AGE) \n\n# A tibble: 27,891 × 1\n     AGE\n   <dbl>\n 1    54\n 2    64\n 3    65\n 4    38\n 5    47\n 6    48\n 7    23\n 8    28\n 9    36\n10    22\n# ℹ 27,881 more rows\n\n\n\n5.3.1 하나씩 반복\n이후 이것을 가지고 나와서 (pull), 이어 받고 (.) 평균을 구해보겠습니다. 이후 이것을 mean 과 sd 라는 변수에 assign 하겠습니다.\n\ndat %>% select(AGE) %>% pull(.) %>% mean(.)\n\n[1] 44.42325\n\ndat %>% select(AGE) %>% pull(.) %>% sd(.)\n\n[1] 12.25356\n\ndat %>% select(AGE) %>% pull(.) %>% mean(.) -> mean\ndat %>% select(AGE) %>% pull(.) %>% sd(.)   -> sd\n\n그럼 표현해 볼까요?\n\nmean\n\n[1] 44.42325\n\nsd\n\n[1] 12.25356\n\npaste(\"평균은 \", mean, \"표준편차는 \", sd)\n\n[1] \"평균은  44.4232548133807 표준편차는  12.2535577069047\"\n\n\n보기 불편하네요, 소숫점 2째 자리까지 표현하겠습니다. round를 이용합니다. 더 자세한것은 구글에게 물어 보세요.\n\nmean <- dat %>% select(AGE) %>% pull(.) %>% mean(.) %>% round(., 2) \nsd   <- dat %>% select(AGE) %>% pull(.) %>% sd(.)   %>% round(., 2)\npaste(\"평균은\", mean, \", 표준편차는\", sd)\n\n[1] \"평균은 44.42 , 표준편차는 12.25\"\n\nsprintf(\"평균은 %s, 표준편차는 %s\", mean, sd)\n\n[1] \"평균은 44.42, 표준편차는 12.25\"\n\n\nunicode를 이용해서 약속된 표현을 사용해 보겠습니다.\n\npaste(mean, \"\\u00b1\", sd)\n\n[1] \"44.42 ± 12.25\"\n\nsprintf(\"%s \\u00b1 %s\", mean, sd)\n\n[1] \"44.42 ± 12.25\"\n\n\nsprintf 에서 %s 대신에 %.2f 를 사용할 텐데요, 어떤지 살펴 봅시다. 네 %2.f 라는 것은 소수 2째 자리까지 살려서 표현하는 방식입니다. %.2f 는 어떨까요? 공부하는 방법입니다. 무언가를 더 해보는 것!\n\nsprintf(\"%.2f \\u00b1 %.2f\", mean, sd)\n\n[1] \"44.42 ± 12.25\"\n\n\n그럼 median 을 구해볼까요? 해보세요. 무언가를 해보는 것!\n\ndat %>% pull(AGE) %>% median(.)\n\n[1] 45\n\ndat %>% pull(AGE) %>% quantile(., c(0.5))\n\n50% \n 45 \n\n\n그럼 quantile을 구해볼까요?\n\ndat %>% pull(AGE) %>% quantile(., c(0.25, 0.5, 0.75)) \n\n25% 50% 75% \n 35  45  54 \n\np50 = dat %>% pull(AGE) %>% quantile(., c(0.50))\np25 = dat %>% pull(AGE) %>% quantile(., c(0.25)) \np75 = dat %>% pull(AGE) %>% quantile(., c(0.75)) \nsprintf(\"%.0f (%.0f-%.0f)\", p50, p25, p75)\n\n[1] \"45 (35-54)\"\n\n\n이것을 남녀를 나누어서 해보겠습니다. filter 명령문을 써보겠습니다. filter(TSEX==1)이라는 것은 TSEX==1 인 남자만을 고르라는 것입니다. 평균은 아래와 같이 구합니다. sd도 구해봅시다.\n\ndat %>% filter(TSEX==1) %>% pull(AGE) %>% mean(.)\n\n[1] 43.65223\n\ndat %>% filter(TSEX==2) %>% pull(AGE) %>% mean(.)\n\n[1] 45.10761\n\n\n\n\n5.3.2 Group_by summary and Table 1\n\nGroup_by\n\n남녀를 나누어서 평균을 구하는 다른 방법을 사용해 봅니다.\n\ndat %>%\n  group_by(TSEX) %>%\n  summarise(avg = mean(AGE), \n            std = sd(AGE))\n\n# A tibble: 2 × 3\n  TSEX        avg   std\n  <dbl+lbl> <dbl> <dbl>\n1 1 [남성]   43.7  12.2\n2 2 [여성]   45.1  12.3\n\n\n동일한 결과가 나오나요. 네 굉장합니다. 남녀를 나누어서 계산했네요. 지금은 2개의 집단을 나누지만, 만약 100개의 집단이라면 filter를 반복하면 어떻게 해야하나요ㅠㅠ. group_by 는 정말 대단한 명령어 입니다. group_by로 코드 파이프 안에서 다음과 같은 결과를 얻었습니다.\n\ndat %>%\n  group_by(TSEX) %>%\n  summarise(avg = mean(AGE), \n            std = sd(AGE)) %>%\n  mutate(smry = sprintf(\"%.2f \\u00b1 %.2f\", avg, std))\n\n# A tibble: 2 × 4\n  TSEX        avg   std smry         \n  <dbl+lbl> <dbl> <dbl> <chr>        \n1 1 [남성]   43.7  12.2 43.65 ± 12.21\n2 2 [여성]   45.1  12.3 45.11 ± 12.25\n\n\n그럼 중간값과 p25-p75를 표현해 봅시다.\n\ndat %>%\n  group_by(TSEX) %>%\n  summarise(avg = mean(AGE), \n            std = sd(AGE), \n            p25 = quantile(AGE, prob=c(0.25)), \n            p50 = quantile(AGE, prob=c(0.50)), \n            p75 = quantile(AGE, prob=c(0.75)), \n            ) %>%\n  mutate(smry1 = sprintf(\"%.1f \\u00b1 %.1f\", avg, std)) %>%\n  mutate(smry2 = sprintf(\"%.0f (%.0f-%.0f)\", p50, p25, p75)) \n\n# A tibble: 2 × 8\n  TSEX        avg   std   p25   p50   p75 smry1       smry2     \n  <dbl+lbl> <dbl> <dbl> <dbl> <dbl> <dbl> <chr>       <chr>     \n1 1 [남성]   43.7  12.2    34    43    53 43.7 ± 12.2 43 (34-53)\n2 2 [여성]   45.1  12.3    35    46    55 45.1 ± 12.3 46 (35-55)\n\n\n이제 필요한 것만 남겨 보겠습니다.\n\ndat %>%\n  group_by(TSEX) %>%\n  summarise(avg = mean(AGE), \n            std = sd(AGE), \n            p25 = quantile(AGE, prob=c(0.25)), \n            p50 = quantile(AGE, prob=c(0.50)), \n            p75 = quantile(AGE, prob=c(0.75)), \n            ) %>%\n  mutate(smry1 = sprintf(\"%.1f \\u00b1 %.1f\", avg, std)) %>%\n  mutate(smry2 = sprintf(\"%.0f (%.0f-%.0f)\", p50, p25, p75)) %>%\n  select(TSEX, smry1, smry2)\n\n# A tibble: 2 × 3\n  TSEX      smry1       smry2     \n  <dbl+lbl> <chr>       <chr>     \n1 1 [남성]  43.7 ± 12.2 43 (34-53)\n2 2 [여성]  45.1 ± 12.3 46 (35-55)\n\n\n이번에는 같은 내용을 수면에 대해서 해보겠습니다. 수면 점수가 높다믄 것은 잠들기 어렵다는 것이 전혀 없음(5점)에 가깝다는 것입니다. 그러니 역의 점수를 만들겠습니다. 1, 2, 3, 4, 5로 기록된 것을 5, 4, 3, 2, 1로 바꾸고 싶은 것입니다. 아래를 이용할 것입니다.\n\nx= 1:5\ny= 6-x\ny\n\n[1] 5 4 3 2 1\n\n\n\ndat %>%\n  mutate(sleep1in = 6-sleep1)\n\n# A tibble: 27,891 × 8\n   TSEX        AGE emp_stat       heal_prob1 sleep1     sleep2  sleep3  sleep1in\n   <dbl+lbl> <dbl> <dbl+lbl>      <dbl+lbl>  <dbl+lbl>  <dbl+l> <dbl+l>    <dbl>\n 1 2 [여성]     54 1 [상용근로자] 2 [없다]   5 [전혀없… 5 [전… 5 [전…        1\n 2 2 [여성]     64 3 [일용근로자] 2 [없다]   2 [한 주… 2 [한 … 2 [한 …        4\n 3 2 [여성]     65 1 [상용근로자] 2 [없다]   5 [전혀없… 5 [전… 5 [전…        1\n 4 1 [남성]     38 1 [상용근로자] 2 [없다]   5 [전혀없… 5 [전… 5 [전…        1\n 5 2 [여성]     47 1 [상용근로자] 2 [없다]   5 [전혀없… 4 [드… 5 [전…        1\n 6 1 [남성]     48 1 [상용근로자] 1 [있다]   5 [전혀없… 5 [전… 5 [전…        1\n 7 1 [남성]     23 2 [임시근로자] 2 [없다]   4 [드물게… 4 [드… 4 [드…        2\n 8 2 [여성]     28 1 [상용근로자] 2 [없다]   4 [드물게… 4 [드… 4 [드…        2\n 9 1 [남성]     36 1 [상용근로자] 2 [없다]   4 [드물게… 4 [드… 4 [드…        2\n10 2 [여성]     22 2 [임시근로자] 2 [없다]   5 [전혀없… 5 [전… 5 [전…        1\n# ℹ 27,881 more rows\n\n\n이번에는 mutate을 이용해 새로운 변수를 생성해 보겠습니다. 변형된 dat는 dat1에 할당합니다.\n\ndat1<-dat %>%\n  mutate(sleep1in = 6-sleep1, \n         sleep2in = 6-sleep2, \n         sleep3in = 6-sleep3 \n         ) %>%\n  mutate(sexgp = case_when(\n    TSEX ==1 ~ \"Men\", \n    TRUE ~ \"Women\"\n  ))\n\n이번에는 sexgp를 group으로 하여 성별, 연령의 대표값을 나나타내 봅시다.\n\ndat1 %>%\n  group_by(sexgp) %>%\n  summarise(\n            avg = mean(AGE), \n            std = sd(AGE), \n            p50 = quantile(AGE, prob=c(0.50)), \n            p25 = quantile(AGE, prob=c(0.25)), \n            p75 = quantile(AGE, prob=c(0.75))\n            ) %>%\n  mutate(smry1= sprintf(\"%.1f \\u00b1 %.1f\", avg, std),\n         smry2= sprintf(\"%.0f (%.0f-%.0f)\", p50, p25, p75)\n         ) %>%\n    select(sexgp,smry1, smry2)\n\n# A tibble: 2 × 3\n  sexgp smry1       smry2     \n  <chr> <chr>       <chr>     \n1 Men   43.7 ± 12.2 43 (34-53)\n2 Women 45.1 ± 12.3 46 (35-55)\n\n\nhtml Table로 만들면 복사해서 붙여 넣기 매우 편리합니다.\n\ndat1 %>%\n  group_by(TSEX) %>%\n  summarise(\n            avg = mean(AGE), \n            std = sd(AGE), \n            p50 = quantile(AGE, prob=c(0.50)), \n            p25 = quantile(AGE, prob=c(0.25)), \n            p75 = quantile(AGE, prob=c(0.75))\n            ) %>%\n  mutate(smry1= sprintf(\"%.1f \\u00b1 %.1f\", avg, std),\n         smry2= sprintf(\"%.0f (%.0f-%.0f)\", p50, p25, p75)\n         ) %>%\n    select(TSEX,smry1, smry2) %>%\n  htmlTable(caption =\"descritive statistics of study population\")\n\n\n\n\n\ndescritive statistics of study population\n\nTSEX\nsmry1\nsmry2\n\n\n\n\n1\n1\n43.7 ± 12.2\n43 (34-53)\n\n\n2\n2\n45.1 ± 12.3\n46 (35-55)\n\n\n\n\n\n그렇다면, sleep1 에 대해서도 만들어 보십시오.\n연령과 sleep1 에 대한 대표값을 함께 볼 수 있을까요? 지금까지 배운 방법으로 해보도록 하겠습니다.\n\ntab1 = dat1 %>%\n  group_by(TSEX) %>%\n  summarise(\n            avg = mean(AGE), \n            std = sd(AGE), \n            p50 = quantile(AGE, prob=c(0.50)), \n            p25 = quantile(AGE, prob=c(0.25)), \n            p75 = quantile(AGE, prob=c(0.75))\n            ) %>%\n  mutate(smry1= sprintf(\"%.1f \\u00b1 %.1f\", avg, std),\n         smry2= sprintf(\"%.0f (%.0f-%.0f)\", p50, p25, p75)\n         ) %>%\n    select(TSEX,smry1, smry2) %>%\n  mutate(variable = \"AGE\")\n\n\ntab2 = dat1 %>%\n  group_by(TSEX) %>%\n  summarise(\n            avg = mean(sleep1), \n            std = sd(sleep1), \n            p50 = quantile(sleep1, prob=c(0.50)), \n            p25 = quantile(sleep1, prob=c(0.25)), \n            p75 = quantile(sleep1, prob=c(0.75))\n            ) %>%\n  mutate(smry1= sprintf(\"%.1f \\u00b1 %.1f\", avg, std),\n         smry2= sprintf(\"%.0f (%.0f-%.0f)\", p50, p25, p75)\n         ) %>%\n    select(TSEX,smry1, smry2) %>%\n  mutate(variable = \"sleep1\")\n\n이 둘을 합쳐 보겠습니다.\n\nrbind(tab1, tab2)\n\n# A tibble: 4 × 4\n  TSEX      smry1       smry2      variable\n  <dbl+lbl> <chr>       <chr>      <chr>   \n1 1 [남성]  43.7 ± 12.2 43 (34-53) AGE     \n2 2 [여성]  45.1 ± 12.3 46 (35-55) AGE     \n3 1 [남성]  4.5 ± 0.7   5 (4-5)    sleep1  \n4 2 [여성]  4.4 ± 0.8   5 (4-5)    sleep1  \n\n\nsleep1까지 했는데요, sleep2, sleep3 까지 하려면 어떻게 해야 하나요? 이것을 2번더 반복해야 합니다. 만약에 변수가 100개라면 어떻게 해야할까요, 네 1000번 하면됩니다. 좀 익숙한 표를 만들기 위해서 반복하는 것이 가장 기본이 됩니다. 그래도 이제는 이 방법을 응용해서, 컴퓨터가 쉽게 작동하는 방식으로 생각해 보겠습니다.\n\n\n5.3.3 Long File and Table 1\n\nlong file\n\n이제 group 대신에 group이 될 파일을 선택해서 long file로 만들어 보겠습니다. 원하는 파일만 골라 봅니다. 성에 따른 연령, sleep1, 2, 3를 정리해 보겠습니다.\n\ndat1 %>% select(sexgp, AGE, sleep1in, sleep2in, sleep3in)\n\n# A tibble: 27,891 × 5\n   sexgp   AGE sleep1in sleep2in sleep3in\n   <chr> <dbl>    <dbl>    <dbl>    <dbl>\n 1 Women    54        1        1        1\n 2 Women    64        4        4        4\n 3 Women    65        1        1        1\n 4 Men      38        1        1        1\n 5 Women    47        1        2        1\n 6 Men      48        1        1        1\n 7 Men      23        2        2        2\n 8 Women    28        2        2        2\n 9 Men      36        2        2        2\n10 Women    22        1        1        1\n# ℹ 27,881 more rows\n\n\n이러한 파일을 wide라고 부릅니다. 이제 우리는 TSEX별, AGE와 Sleep1의 값을 얻고자 합니다. 그러면 TSEX와 AGE, TSEX와 Sleep이 하나의 group이 됩니다. group을 반복한것과 같습니다. 즉 group의 기본은 -로 제외하고, 이와 상관되는 변수는 모두 포함하여 variables라는 이름으로 만들고, 값은 values라는 변수로 변환시키는 것입니다.\n\ndat1 %>% select(sexgp, AGE, sleep1in, sleep2in, sleep3in) %>%\n  pivot_longer(-c(sexgp), names_to =\"variables\", values_to = \"values\")\n\n# A tibble: 111,564 × 3\n   sexgp variables values\n   <chr> <chr>      <dbl>\n 1 Women AGE           54\n 2 Women sleep1in       1\n 3 Women sleep2in       1\n 4 Women sleep3in       1\n 5 Women AGE           64\n 6 Women sleep1in       4\n 7 Women sleep2in       4\n 8 Women sleep3in       4\n 9 Women AGE           65\n10 Women sleep1in       1\n# ℹ 111,554 more rows\n\n\n평균에 대해서만 먼저 해보겠습니다.\n\ndat1%>% select(sexgp, AGE, sleep1, sleep2, sleep3) %>%\n  pivot_longer(-c(sexgp), names_to =\"variables\", values_to = \"values\") %>%\n  group_by(sexgp, variables) %>%\n  summarise(avg = mean(values, na.rm =TRUE), \n            std = sd(values, na.rm =TRUE), \n            p50 = quantile(values, prob = 0.50, na.rm =TRUE),\n            p25 = quantile(values, prob = 0.25, na.rm =TRUE), \n            p75 = quantile(values, prob = 0.75, na.rm =TRUE) \n            ) %>%\n  mutate(mean_std  = sprintf(\"%.1f\\u00b1%.1f\", avg, std), \n         median_IQR= sprintf(\"%.0f (%.0f-%.0f)\", p50, p25, p75)) %>%\n  select(sexgp, variables, mean_std)\n\n# A tibble: 8 × 3\n# Groups:   sexgp [2]\n  sexgp variables mean_std \n  <chr> <chr>     <chr>    \n1 Men   AGE       43.7±12.2\n2 Men   sleep1    4.5±0.7  \n3 Men   sleep2    4.5±0.8  \n4 Men   sleep3    4.3±0.9  \n5 Women AGE       45.1±12.3\n6 Women sleep1    4.4±0.8  \n7 Women sleep2    4.4±0.9  \n8 Women sleep3    4.2±0.9  \n\n\n무언가 복잡하지만 쉬워졌죠? 이말에 고개를 끄덕이셨다면 여러분은 이제 R coding에 빠져 들고 있는 것입니다. \n이제 이것을 사람이 보기 편한 상태인 wide로 바꾸겠습니다. pivot_wider를 사용하고 우리가 원하는 가로로 필요한 정보를 names로 하고 원하는 값을 values로 하겠습니다. 이것을 tab1으로 정의하겠습니다.\n\ndat1 %>% select(sexgp, AGE, sleep1, sleep2, sleep3) %>%\n  pivot_longer(-c(sexgp), names_to =\"variables\", values_to = \"values\") %>%\n  group_by(sexgp, variables) %>%\n  summarise(avg = mean(values, na.rm =TRUE), \n            std = sd(values, na.rm =TRUE)\n            ) %>%\n  mutate(mean_std  = sprintf(\"%.1f\\u00b1%.1f\", avg, std), \n         median_IQR= sprintf(\"%.0f (%.0f-%.0f)\", p50, p25, p75)) %>%\n  select(sexgp, variables, mean_std) %>%\n  pivot_wider(names_from = sexgp, values_from = c(mean_std)) -> tab1\ntab1 %>% htmlTable(caption = \"Table 1. Basic Characteristics of study population\")\n\n\n\n\n\nTable 1. Basic Characteristics of study population\n\nvariables\nMen\nWomen\n\n\n\n\n1\nAGE\n43.7±12.2\n45.1±12.3\n\n\n2\nsleep1\n4.5±0.7\n4.4±0.8\n\n\n3\nsleep2\n4.5±0.8\n4.4±0.9\n\n\n4\nsleep3\n4.3±0.9\n4.2±0.9"
  },
  {
    "objectID": "245_datamanipulation_for_table.html#count-and-distribution",
    "href": "245_datamanipulation_for_table.html#count-and-distribution",
    "title": "5  보건학표_1",
    "section": "5.4 Count and Distribution",
    "text": "5.4 Count and Distribution\nheal_prob1 이 요통 변수라고 했었는데요, 1이 요통이 있다. 2가 요통이 없다 입니다. 이것을 표로 만들어 보겠습니다. ### 하나씩 반복 하나씩 반복하는게 가장 기본입니다. count를 사용하겠습니다. 요통이 몇몇 있나요?\n\ndat1 %>% count(heal_prob1)\n\n# A tibble: 2 × 2\n  heal_prob1     n\n  <dbl+lbl>  <int>\n1 1 [있다]    6939\n2 2 [없다]   20952\n\n\n분율, 비율, 유병률을 나타내면 어떻게 될까요. 전체 합을 sum으로 만들고 이때 비율을 만들어야 합니다.\n\ndat1 %>% count(heal_prob1) %>%\n  mutate(total = sum(n)) %>%\n  mutate(prob = n/total)\n\n# A tibble: 2 × 4\n  heal_prob1     n total  prob\n  <dbl+lbl>  <int> <int> <dbl>\n1 1 [있다]    6939 27891 0.249\n2 2 [없다]   20952 27891 0.751\n\n\n남자만 해보겠습니다.\n\ndat1 %>% \n  filter(TSEX==1) %>%\n  count(heal_prob1) %>%\n  mutate(prob= n/sum(n)) \n\n# A tibble: 2 × 3\n  heal_prob1     n  prob\n  <dbl+lbl>  <int> <dbl>\n1 1 [있다]    2930 0.223\n2 2 [없다]   10185 0.777\n\n\n여자만 해보세요.\n\n5.4.1 Group_by summary and Table 1\ngroup_by를 이용해 보겠습니다. 이면 위에서 연령을 이용한 방법을 실습했는데, 그것과 동일합니다\n\ndat1 %>%\n  group_by(TSEX) %>%\n  count(heal_prob1) %>%\n  mutate(prob = n/sum(n))\n\n# A tibble: 4 × 4\n# Groups:   TSEX [2]\n  TSEX      heal_prob1     n  prob\n  <dbl+lbl> <dbl+lbl>  <int> <dbl>\n1 1 [남성]  1 [있다]    2930 0.223\n2 1 [남성]  2 [없다]   10185 0.777\n3 2 [여성]  1 [있다]    4009 0.271\n4 2 [여성]  2 [없다]   10767 0.729\n\n\n익숙한 형태로 표시해 보겠습니다. 그리고 필요한 것만 남겨 보겠습니다 .\n\ndat1 %>%\n  group_by(TSEX) %>%\n  count(heal_prob1) %>%\n  mutate(prob = n/sum(n)) %>%\n  mutate(smry1 = sprintf(\"%s (%.1f%%)\", n, prob*100)) %>%\n  select(TSEX, heal_prob1, smry1)\n\n# A tibble: 4 × 3\n# Groups:   TSEX [2]\n  TSEX      heal_prob1 smry1        \n  <dbl+lbl> <dbl+lbl>  <chr>        \n1 1 [남성]  1 [있다]   2930 (22.3%) \n2 1 [남성]  2 [없다]   10185 (77.7%)\n3 2 [여성]  1 [있다]   4009 (27.1%) \n4 2 [여성]  2 [없다]   10767 (72.9%)\n\n\n이번에는 연령을 5세단위로 바꾸로 각 연령의 분포를 확인해 보겠습니다.\n\ndat1 <-dat1 %>% \n  mutate(agegp = case_when(\n    AGE <25 ~ \"<25\",\n    AGE <30 ~ \"<30\", \n    AGE <35 ~ \"<35\", \n    AGE <40 ~ \"<40\", \n    AGE <45 ~ \"<45\", \n    AGE <50 ~ \"<50\",\n    AGE <55 ~ \"<55\", \n    AGE <60 ~ \"<60\",\n    TRUE ~ \"\\u226560\" # 나머지는 모두 >65 (\\u2265는 크거나 같다는 symbol)\n  )) \n\ndat1 %>%\n  group_by(TSEX) %>%\n  count(agegp) %>%\n  mutate(prob = n/sum(n)) %>%\n  mutate(smry1 = sprintf(\"%s (%.1f%%)\", n, prob*100)) %>%\n  select(TSEX, agegp, smry1) %>%\n  arrange(TSEX, agegp)\n\n# A tibble: 18 × 3\n# Groups:   TSEX [2]\n   TSEX      agegp smry1       \n   <dbl+lbl> <chr> <chr>       \n 1 1 [남성]  <25   604 (4.6%)  \n 2 1 [남성]  <30   1334 (10.2%)\n 3 1 [남성]  <35   1498 (11.4%)\n 4 1 [남성]  <40   1812 (13.8%)\n 5 1 [남성]  <45   1810 (13.8%)\n 6 1 [남성]  <50   1691 (12.9%)\n 7 1 [남성]  <55   1464 (11.2%)\n 8 1 [남성]  <60   1323 (10.1%)\n 9 1 [남성]  ≥60   1579 (12.0%)\n10 2 [여성]  <25   727 (4.9%)  \n11 2 [여성]  <30   1242 (8.4%) \n12 2 [여성]  <35   1477 (10.0%)\n13 2 [여성]  <40   1597 (10.8%)\n14 2 [여성]  <45   1730 (11.7%)\n15 2 [여성]  <50   2013 (13.6%)\n16 2 [여성]  <55   2167 (14.7%)\n17 2 [여성]  <60   1972 (13.3%)\n18 2 [여성]  ≥60   1851 (12.5%)\n\n\n\n\n5.4.2 Long File and Table 1\n그럼 이번에는 요통과 연령집단을 동시에 바꾸어 보겠습니다.\n\ndat1 %>%\n  mutate(backpain= case_when(heal_prob1==1 ~ \"pain\", \n                             TRUE ~ \"no-pain\")) %>%\n  select(sexgp, agegp, backpain) %>%\n  pivot_longer(-c(sexgp), names_to =\"variables\", values_to = \"values\")  %>%\n  group_by(sexgp, variables) %>%\n  count(values) %>%\n  mutate(prob = n/sum(n)) %>%\n  mutate(smry1 = sprintf(\"%s (%.1f%%)\", n, prob*100)) %>%\n  select(-n, -prob) %>%\n  pivot_wider(names_from = sexgp, values_from = smry1) -> tab2\n\nhtml 테이블로 살펴 보겠습니다.\n\ntab2 %>% htmlTable()\n\n\n\n\n\nvariables\nvalues\nMen\nWomen\n\n\n\n\n1\nagegp\n<25\n604 (4.6%)\n727 (4.9%)\n\n\n2\nagegp\n<30\n1334 (10.2%)\n1242 (8.4%)\n\n\n3\nagegp\n<35\n1498 (11.4%)\n1477 (10.0%)\n\n\n4\nagegp\n<40\n1812 (13.8%)\n1597 (10.8%)\n\n\n5\nagegp\n<45\n1810 (13.8%)\n1730 (11.7%)\n\n\n6\nagegp\n<50\n1691 (12.9%)\n2013 (13.6%)\n\n\n7\nagegp\n<55\n1464 (11.2%)\n2167 (14.7%)\n\n\n8\nagegp\n<60\n1323 (10.1%)\n1972 (13.3%)\n\n\n9\nagegp\n≥60\n1579 (12.0%)\n1851 (12.5%)\n\n\n10\nbackpain\nno-pain\n10185 (77.7%)\n10767 (72.9%)\n\n\n11\nbackpain\npain\n2930 (22.3%)\n4009 (27.1%)\n\n\n\n\n\n무슨 생각이 드시죠? tab1과 tab2를 합치면 좋겠다는 생각이 드시죠, tab1에는 values 라는 변수가 없습니다 .그래서 합치기 어렵습니다. values 라는 변수를 생성하고 합쳐 보겠습니다.\n\ntab1 = tab1 %>% mutate(values = \"\") %>% select(variables, values, Men, Women) \n\nrbind(tab1, tab2) %>%\n  htmlTable()\n\n\n\n\n\nvariables\nvalues\nMen\nWomen\n\n\n\n\n1\nAGE\n\n43.7±12.2\n45.1±12.3\n\n\n2\nsleep1\n\n4.5±0.7\n4.4±0.8\n\n\n3\nsleep2\n\n4.5±0.8\n4.4±0.9\n\n\n4\nsleep3\n\n4.3±0.9\n4.2±0.9\n\n\n5\nagegp\n<25\n604 (4.6%)\n727 (4.9%)\n\n\n6\nagegp\n<30\n1334 (10.2%)\n1242 (8.4%)\n\n\n7\nagegp\n<35\n1498 (11.4%)\n1477 (10.0%)\n\n\n8\nagegp\n<40\n1812 (13.8%)\n1597 (10.8%)\n\n\n9\nagegp\n<45\n1810 (13.8%)\n1730 (11.7%)\n\n\n10\nagegp\n<50\n1691 (12.9%)\n2013 (13.6%)\n\n\n11\nagegp\n<55\n1464 (11.2%)\n2167 (14.7%)\n\n\n12\nagegp\n<60\n1323 (10.1%)\n1972 (13.3%)\n\n\n13\nagegp\n≥60\n1579 (12.0%)\n1851 (12.5%)\n\n\n14\nbackpain\nno-pain\n10185 (77.7%)\n10767 (72.9%)\n\n\n15\nbackpain\npain\n2930 (22.3%)\n4009 (27.1%)"
  },
  {
    "objectID": "245_datamanipulation_for_table.html#정리-1",
    "href": "245_datamanipulation_for_table.html#정리-1",
    "title": "5  보건학표_1",
    "section": "5.5 정리 1",
    "text": "5.5 정리 1\n\n원하는 데이터를 불러옴\n원하는 변수를 선정\n\n관심 변수 (종속, 독립)\n\n변수 값을 살펴 보기\n\n숫자 인지 아닌지\nmissing value 가 얼마 인지\n제외할 변수가 얼마인지\n\n대표값 생성\n\ncount 함수 사용, prob 변수 생성\n\n표 생성\n\n반복\ngroup_by\nlong file"
  },
  {
    "objectID": "247_datamanipulation_for_table_2.html#데이터-준비",
    "href": "247_datamanipulation_for_table_2.html#데이터-준비",
    "title": "6  보건학표_2",
    "section": "6.1 데이터 준비",
    "text": "6.1 데이터 준비\n데이터 표를 만드는 실습은 6차 근로환경조사 자료를 통해 실습할 것입니다.. 자료는 안전보건공단, 근로환경조사 원시자료 사이트 (http://kosha.or.kr/kosha/data/primitiveData.do) 에서 신청할 수 있습니다..\n\nrm(list=ls())\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"htmlTable\")) install.packages(\"htmlTable\")\nif(!require(\"haven\")) install.packages(\"haven\")\nif(!require(\"broom\")) install.packages(\"broom\")\n\n데이터 표를 만드는 실습은 6차 근로환경조사 자료를 통해 실습할 것입니다.. 자료는 안전보건공단, 근로환경조사 원시자료 사이트 (http://kosha.or.kr/kosha/data/primitiveData.do) 에서 신청할 수 있습니다.. 데이터를 불러오겠습니다. 안전보건공단 홈페이에서 자료를 다운 받는게 원칙입니다. 다만 실습을 빠르게 진행하기 위해서, dspubs.org 페이지에 있는 파일을 이용하겠습니다.   kwcsData6th.rds   자신의 folder에 data 라는 folder가 있는지 확인하십시오. data라는 폴더에 다운로드하고, 불러오도록 하겠습니다.\n\nurl <- \"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/kwcsData6th.rds\"\ndownload.file(url, \"data/tutorKWCS.rds\")\n\n\nkwcs = readRDS(\"data/tutorKWCS.rds\")"
  },
  {
    "objectID": "247_datamanipulation_for_table_2.html#분석-주제-선정",
    "href": "247_datamanipulation_for_table_2.html#분석-주제-선정",
    "title": "6  보건학표_2",
    "section": "6.2 분석 주제 선정",
    "text": "6.2 분석 주제 선정\n우선 수면 장애를 중심으로 연구를 하기로 했습니다. 이때 장시간 근무가 수면장애를 일으킬 수 있다는 가정을 했습니다. 기존 연구에서 유사한 내용이 있었기 때문입니다. 그 이유를 생각해보니, 오래 일하면 어떤 방식으로든 수면시간 자체가 줄어들 것으로 보였기 때문입니다. 우선 sleep disturbance, long working hours per week 이 정해졌습니다. 이후 연구방법론의 혼란변수 선정 방법이나, 연구자의 Domain knowledge에 따른 선정, 새로운 가설 개발 (originality) 위한 변수 선정등을 고려할 수 있습니다. 이러한 내용은 각각의 연구방법론 수업에서 학습하시기를 바랍니다.  여러 토론을 하다보니, 일을 마치고 집에 돌아온 후 해야할 일이 많은 경우 수면장애가 발생할 것 같은 느낌이 들었습니다. 이 것을 확인해 보고자 합니다.    즉, 장시간근로와 일과 삶의 균형에 따른 수면장애 관련성을 연구해 보고 싶습니다.\n\n  이제 data manipulation, long wide 변환을 통해 실습을 해보겠습니다.\n\n6.2.1 근로환경조사와 recode (선택사항)\n번외로 분석을 빠르게 진행하기 위해 recode에 대한 복습을 해 보겠습니다. recode는 아주 단순한 변경에 사용되는데요, 근로환경조사는 이미 변수를 1, 2, 3, 4, 5 의 순서를 Likert 척도로 사용하고 있습니다.\n\n근로환경조사 Likert 척도\n\n\n\n\n\n\n\n\n근로환경조사\nLikert 척도\n\n\n\n\n얼마나 자주 그렇습니까(freq)?\n“4.Always”,  “3.Often”, “2.Sometimes”, “1.Rarely”, “0.Never”\n\n\n얼마나 자주 그렇습니까(time)?\n“5.All of the time”, “4.Most of the time”, “3.More than half of the time”, “2.less than half of the time”, “1.Some of the time”, “0.At no time”\n\n\n얼마나 적당 합니까?\n“3.Very well”, “3.Well”, “2.Not very well”, “0.Not at all well”\n\n\n\n이것을 이용하면 매우 쉽게 정리할 수 있습니다. \n\nwwa4: 집안일 때문에 집중하기 어렵다가\n\n1: 항상그렇다,\n2: 대부분그렇다.\n3: 가끔 그렇다\n4: 별로 그렇지 않다.\n5: 전혀 그렇지 않다 로 되어 있습니다.\n\n\n\nrecode\n\n아래의 함수 결과를 살펴보면, 숫자의 경우 순서에 따라 문자를 어사인하는 것을 볼 수 있습니다. 데이터가 정형화되어 정리되어 있는 경우, 그리고 숫자인경우, case_when 이나 ifelse보다 더 쉬운 방법입니다.\n\nvars = c(1, 1, 1, 3, 3, 3, 2, 2, 2, 4, 4, 4)\nr1 = recode(vars, `1`= \"a\", `2`= \"b\", `3`= \"c\", `4`= \"d\")\nr1\n\n [1] \"a\" \"a\" \"a\" \"c\" \"c\" \"c\" \"b\" \"b\" \"b\" \"d\" \"d\" \"d\"\n\nr2 = recode(vars, \"a\", \"b\", \"c\", \"d\")\nr1 == r2\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n문자일 때는 어떨까요? 문자는 순서를 특정화 해주지 않았으므로 어렵습니다. 근로환경조사는 대부분 문자로 저장되어 있습니다.\n\nvars_char = as.character(vars)\nr3 = recode(vars_char, \"a\", \"b\", \"c\", \"d\")\nr3_n = recode(as.numeric(vars_char), \"a\", \"b\", \"c\", \"d\")\n\n이를 as.numeric을 통해 숫자로 변형시켜준 후 시행하면 잘 작동합니다.\n\nvars_char = as.character(vars)\nr3_n = recode(as.numeric(vars_char), \"a\", \"b\", \"c\", \"d\")\n\n이제 kwcs 에서 wwa1부터 wwa5까지를 가져와 역순으로 Likert 척도를 구해보겠습니다.\n\nwwas = kwcs %>% select(TSEX, contains(\"wwa\"))\nwwas %>% head()\n\n# A tibble: 6 × 11\n  TSEX      wwa1     wwa2    wwa3    wwa4    wwa5    wwa1gp wwa2gp wwa3gp wwa4gp\n  <dbl+lbl> <dbl+lb> <dbl+l> <dbl+l> <dbl+l> <dbl+l> <chr>  <chr>  <chr>  <chr> \n1 2 [여성]  5 [전혀… 4 [별… 5 [전… 5 [전… 5 [전… 0.Nev… 1.Rar… 0.Nev… 0.Nev…\n2 2 [여성]  4 [별로… 3 [가… 3 [가… 4 [별… 4 [별… 1.Rar… 2.Som… 2.Som… 1.Rar…\n3 2 [여성]  4 [별로… 4 [별… 4 [별… 4 [별… 4 [별… 1.Rar… 1.Rar… 1.Rar… 1.Rar…\n4 2 [여성]  5 [전혀… 5 [전… 5 [전… 5 [전… 5 [전… 0.Nev… 0.Nev… 0.Nev… 0.Nev…\n5 1 [남성]  5 [전혀… 5 [전… 5 [전… 5 [전… 5 [전… 0.Nev… 0.Nev… 0.Nev… 0.Nev…\n6 2 [여성]  5 [전혀… 4 [별… 4 [별… 5 [전… 5 [전… 0.Nev… 1.Rar… 1.Rar… 0.Nev…\n# ℹ 1 more variable: wwa5gp <chr>\n\n\nQuizz1: wwa1 변수에서 1의 값을 갖는 것은 남녀 별로 각각 몇명일까요? (script 안에 count 를 이용해서 작성해 보세요)\n\nwwas %>%\n  group_by(TSEX) %>%\n  [ script     ]\n\n이제 recode를 이용해 변환해 보겠습니다.\n\nwwa 관련 변수를\n\n1: 항상그렇다 → “4.Always”\n2: 대부분그렇다 → “3.Often”\n3: 가끔 그렇다 → “2.Sometimes”\n4: 별로 그렇지 않다 → “1.Rarely”\n5: 전혀 그렇지 않다 →“0.Never” 로 변경해 보겠습니다.  첫번째는 case_when으로 두번째는 recode로 변경했습니다. 누가 더 쉬운가요?\n\n\n\ntest = wwas %>%\n  mutate(wwa1gp1 = case_when(\n    wwa1 ==1 ~ \"4.Always\",\n    wwa1 ==2 ~ \"3.Often\", \n    wwa1 ==3 ~ \"2.Sometimes\", \n    wwa1 ==4 ~ \"1.Rarely\",\n    wwa1 ==5 ~ \"0.Never\" \n  )) %>%\n  mutate(wwa1gp2 = recode(as.numeric(wwa1), \n                          \"4.Always\",\"3.Often\",\"2.Sometimes\", \"1.Rarely\",\"0.Never\" \n                          )) \ntest %>%\n  mutate(diff = wwa1gp1 == wwa1gp2) %>%\n  pull(diff) %>% table(.)\n\n.\n TRUE \n41108 \n\n\n코드가 길어지는 것은 누군가와 소통하는데 장벽이 될수 있습니다. 반복적인 것은 함수를 만들어 요약할 수 있습니다.\n\nLikerts = function(x){\n  recode(as.numeric(x), \n         \"4.Always\",\"3.Often\",\"2.Sometimes\", \"1.Rarely\",\"0.Never\" \n         )\n}\n\nwwas %>%\n  mutate(wwa1gp1 = recode(as.numeric(wwa1), \n                          \"4.Always\",\"3.Often\",\"2.Sometimes\", \"1.Rarely\",\"0.Never\" \n                          )) %>%\n  mutate(wwa1gp2 = Likerts(wwa1)) %>%\n  mutate(diff = wwa1gp1 == wwa1gp2) %>%\n  pull(diff) %>% table(.)\n\n.\n TRUE \n41108 \n\n\n\n\n6.2.2 근로환경조사 기본 변수 생성\ndata manipulation에서 실습한 내용을 통해 변수를 생성해 보겠습니다.\n\n# data manip function\nLikert5f = function(x){dplyr::recode(as.numeric(x), \n         \"4.Always\",\"3.Often\",\"2.Sometimes\", \"1.Rarely\",\"0.Never\" )}\n\n# data step start ----------\n\ndat1 = kwcs %>%\n  filter(AGE <70) %>%\n  filter(AGE >18) %>%\n  # sleep --------\n  filter(!is.na(sleep1)&!is.na(sleep2)&!is.na(sleep3)) %>%\n  filter(sleep1 %in% c(1:5), \n         sleep2 %in% c(1:5), \n         sleep3 %in% c(1:5)) %>%\n  mutate(sleep1in = 5-sleep1, \n         sleep2in = 5-sleep2,\n         sleep3in = 5-sleep3\n         ) %>%\n  mutate(sleepgp = case_when(\n    sleep1in + sleep2in+ sleep3in >=6 ~ \"1.sleep disturbance\", \n    TRUE ~ \"0.non distrubance\"\n  )) %>%\n  # work life balances -------------\n  filter(!is.na(wbalance), !is.na(wwa1), !is.na(wwa2), !is.na(wwa3), !is.na(wwa4), !is.na(wwa5)) %>%\n  filter(!is.na(wbalance)) %>%\n  mutate(wbalancegp = case_when(\n    wbalance %in% c(1, 2) ~ \"0.balance\", \n    TRUE ~ \"1.non balance\"\n  )) %>%\n  mutate(wwa1gp=Likert5f(wwa1), wwa2gp=Likert5f(wwa2),  wwa3gp=Likert5f(wwa3), \n         wwa4gp=Likert5f(wwa4), wwa5gp=Likert5f(wwa5), \n         ) %>%\n  # job and sex, agegp  ----------\n  filter(!is.na(job1))%>%\n  filter(job1 %in% c(1, 2, 3)) %>%\n  mutate(sexgp = case_when(\n    TSEX ==1 ~ \"Men\", \n    TRUE ~ \"Women\"\n  )) %>%\n  mutate(agegp = case_when(AGE <25 ~ \"<25\", AGE <30 ~ \"<30\",  AGE <35 ~ \"<35\", \n    AGE <40 ~ \"<40\", AGE <45 ~ \"<45\", AGE <50 ~ \"<50\",AGE <55 ~ \"<55\", AGE <60 ~ \"<60\",\n    TRUE ~ \"\\u226560\" # 나머지는 모두 >65 (\\u2265는 크거나 같다는 symbol)\n  )) %>%\n  filter(!is.na(edu)) %>% filter(edu %in% c(1:7)) %>%\n  mutate(edugp = case_when(\n    edu %in% c(1:3) ~    \"2.middle school or below\",\n    edu %in% c(4  ) ~    \"1.high school\",\n    edu %in% c(5:7)   ~  \"0.university or more\"\n  )) %>%\n  mutate(njob=case_when(\n    job1 %in% c(2, 3) ~ \"1.njob\", \n    TRUE ~ \"0.one-job\")) %>%\n  # back pains -----------\n  mutate(backpain= case_when(\n    heal_prob1==1 ~ \"pain\", \n    TRUE ~ \"no-pain\")) %>%\n  # emp_type,working hours, shiftwork, work life balance -------\n  filter(!is.na(emp_type)) %>%\n  filter(emp_type %in% c(1:4)) %>%\n  mutate(empgp = case_when(\n    emp_type ==1 ~ \"2.own-account worker\", \n    emp_type ==2 ~ \"1.employer/self-employer\", \n    emp_type ==3 ~ \"0.paid-worker\", \n    emp_type ==4 ~ \"3.unpaind family work\"\n  )) %>%\n  filter(!is.na(wtime_week)) %>%\n  mutate(whgp=case_when(\n    wtime_week < 35 ~ \"<35\", \n    wtime_week < 40 ~ \"<40\", \n    wtime_week < 52 ~ \"<52\", \n    wtime_week < 60 ~ \"<60\", \n    TRUE ~ \"\\u226560\", \n  )) %>%\n  filter(!is.na(wtime_length5)) %>%\n  filter(wtime_length5 %in% c(1, 2)) %>%\n  mutate(shiftWork = case_when(\n    wtime_length5 ==1 ~ \"1.shift work\", \n    TRUE ~ \"0.non shift work\"\n  )) %>%\n  filter(!is.na(wtime_resilience)) %>%\n  mutate(shortReturn = case_when(\n    wtime_resilience ==1 ~ \"1.short return\", \n    TRUE ~ \"0.non short return\"\n  )) %>%\n  filter(!is.na(satisfaction)) %>%\n  filter(satisfaction %in% c(1:4)) %>%\n  mutate(satisInv = 5-satisfaction) %>%\n  mutate(shiftShort=case_when(\n    shiftWork == \"1.shift work\" & shortReturn == \"1.short return\" ~ \"3.shift work with short return\", \n    shiftWork == \"1.shift work\" & shortReturn != \"1.short return\" ~ \"2.shift work\", \n    shiftWork != \"1.shift work\" & shortReturn == \"1.short return\" ~ \"1.day work with short return\", \n    shiftWork != \"1.shift work\" & shortReturn != \"1.short return\" ~ \"0.day work\", \n  ))"
  },
  {
    "objectID": "247_datamanipulation_for_table_2.html#근로환경조사-table-1-만들기",
    "href": "247_datamanipulation_for_table_2.html#근로환경조사-table-1-만들기",
    "title": "6  보건학표_2",
    "section": "6.3 근로환경조사 Table 1 만들기",
    "text": "6.3 근로환경조사 Table 1 만들기\n지금까지 작업을 통해 분석한 변수를 선정합니다. 이때 정리할 부분은 무엇으로 구분할지, 관심변수는 무엇인지 이중 명목변수와 연속변수는 무엇인지를 구분하는 것입니다.\n\n## Variables selection\nstratas  = c(\"sleepgp\")\ncatVars = c(\n  # interesting\n  #\"wbalancegp\",\n  \"wwa1gp\", \"wwa2gp\",\"wwa3gp\",\"wwa4gp\",\"wwa5gp\",\n  # confounder\n  #\"whgp\", \n  \"shortReturn\",#\"shiftWork\" , #\"njob\", \n  # demograhpics\n  \"sexgp\", #\"agegp\", \n  \"edugp\", \"empgp\" #, \"backpain\" \n)\nconVars = c(\"AGE\",\"satisfaction\")\n\n\n6.3.1 변수 값 요약하기\n아래 코드가 뜻하는 것은 무엇일까요? 하나하나 살펴보아야 합니다. Table을 만들때 long file 형태로 만든 것입니다.\n\ncatTab = dat1 %>%\n  select(stratas, all_of(catVars)) %>%\n  pivot_longer(-c(stratas), names_to = \"variables\", values_to =\"values\")%>%\n  group_by( variables, values) %>%\n  count(!!sym(stratas)) %>%\n  mutate(prob = n/sum(n), \n         smry= sprintf(\"%.0f (%.1f%%)\", n, prob*100)\n         ) %>%\n  select(-n, -prob) %>%\n  ungroup() %>%\n  pivot_wider(names_from = stratas, values_from =smry) \n\n아래 코드가 뜻하는 것은 무엇일까요? 하나하나 살펴보아야 합니다. Table을 만들때 long file 형태로 만든 것입니다.\n\nconTab = \n  dat1 %>%\n  select(stratas, all_of(conVars)) %>%\n  pivot_longer(-c(stratas), names_to = \"variables\", values_to =\"values\")%>%\n  group_by( !!sym(stratas), variables) %>%\n  summarise(avg = mean(values, na.rm =TRUE), \n            std = sd(values, na.rm =TRUE) \n            ) %>%\n  mutate(smry  = sprintf(\"%.1f\\u00b1%.1f\", avg, std)) %>%\n  select(stratas, variables, smry)%>%\n  ungroup() %>%\n  pivot_wider(names_from = stratas, values_from =smry) %>%\n  mutate(values =\"\") \n\n명목변수 요약 표 catTab과 연속편수 요약표 conTab을 합쳐 보겠습니다. rbind는 종으로 합친다는 뜻입니다.\n\ntabDat = rbind(catTab, conTab)\n\n\n\n6.3.2 P value 구하기.\n이제 P value를 구해보겠습니다. 명목변수는 카이검정, 연속변수는 T검정을 해보겠습니다.\n\nChisq-Test\n\nCross Table을 만든 다음, chisq.test() 명령을 통해 수행합니다.\n\ntab= data.frame(A = c(1000, 2000), \n           B = c(100,  300))\ntab\n\n     A   B\n1 1000 100\n2 2000 300\n\nchisq.test(tab)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tab\nX-squared = 10.821, df = 1, p-value = 0.001003\n\n\n\nT.Test\n\n두 연속변수로 구성된 Data를 만들고 t.test() 명령을 통해 수행합니다.\n\ntab = data.frame(A=rnorm(100), \n           B=rnorm(100)+0.3)\nt.test(tab)\n\n\n    One Sample t-test\n\ndata:  tab\nt = 3.5089, df = 199, p-value = 0.0005564\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.1156374 0.4123731\nsample estimates:\nmean of x \n0.2640053 \n\n\n이것을 이용해서 카이검정과 T검정을 수행해보겠습니다.  가장 정확하면서 쉬운 방식은 반복하는 것입니다. 우리가 이미 변수 값을 long file을 통해 정리하는 것을 실습하였기 때문에, 이 방식도 long file을 이용하겠습니다. \n이때 중요한 개념임 map, nest, tidy가 나옵니다. \n\nmap, nest, tidy\n\n각 백터 값에 대해서 함수를 실행한다.\n\n1:5 %>%\n  map(function(x){x^2})\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\ntibble(a=1:5, b=6:10) %>%\n  map(sum)\n\n$a\n[1] 15\n\n$b\n[1] 40\n\n\n이번에는 좀 복잡한 것을 해보겠습니다 아래와 같이 두 집단에 대해서 size와 power를 각 20명에게 측정했다고 가정합시다. 그리고 size와 power를 동시에 측정하고 측정 순서대로 데이터를 정리한 것입니다. 이후 G1과 G2의 Size와 Power에 대해 T.test를 수행하겠습니다.\n\ntest = tibble(\n  Variables= rep(c(\"Size\",\"Power\"), each=20), \n  G1= c(rnorm(20), rnorm(20)), \n  G2= c(rnorm(20)-0.3, rnorm(20)+0.3), \n  )\n\n일반적 방식은 아래와 같습니다. 이것을 반복하면 되는 것입니다.\n\ntest %>% filter(Variables==\"Size\") %>%\n  select(-Variables) %>%\n  t.test(.)\n\n\n    One Sample t-test\n\ndata:  .\nt = 1.2216, df = 39, p-value = 0.2292\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1110549  0.4497403\nsample estimates:\nmean of x \n0.1693427 \n\nt.test(test[test$Variables==\"Size\", ]$G1, \n  test[test$Variables==\"Size\", ]$G2)\n\n\n    Welch Two Sample t-test\n\ndata:  test[test$Variables == \"Size\", ]$G1 and test[test$Variables == \"Size\", ]$G2\nt = 0.88965, df = 37.99, p-value = 0.3793\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.3154596  0.8100985\nsample estimates:\n mean of x  mean of y \n0.29300245 0.04568298 \n\n\n이에 이것을 반복 수행하기 위해, data를 slice 하여 하나의 list로 만드는과정을 하겠습니다. nest의 과정입니다. 이후 각 데이터에 map을 이용해서 반복 수행을 하겠습니다. 그리고 그 값을 정리하여 unnest하는 과정이 필요합니다. 필요한 값인 p. value만 남겨 정리합니다.\n\ntest1 = test %>%\n  nest(dat = -Variables) %>%\n  mutate(fit=map(dat, ~t.test(.x)), \n         tidied=map(fit, tidy)) \ntest1 %>% unnest(tidied) %>%\n  select(Variables, p.value)\n\n# A tibble: 2 × 2\n  Variables p.value\n  <chr>       <dbl>\n1 Size      0.229  \n2 Power     0.00790\n\n\n지금까지 사용한 것을 가지고, 명목변수에서는 Chisq-Test, 연속변수에서 t.test 를 수행하여 정리하겠습니다.\n\n명목변수, chisq.test\n\n\ncatPvalue = \n  dat1 %>%\n  select(stratas, catVars) %>%\n  pivot_longer(-c(stratas), names_to = \"variables\", values_to =\"values\")%>%\n  group_by(variables, values) %>%\n  count(!!sym(stratas)) %>%\n  pivot_wider(names_from = stratas, values_from =n) %>%\n  ungroup() %>%\n  select(-values) %>%\n  nest(dat = -variables) %>%\n  mutate(\n    fit = map(dat, \n              ~chisq.test(.x)), \n    tidied = map(fit, tidy)\n  ) %>%\n  unnest(tidied) %>%\n  select(variables, p.value) %>%\n  mutate(p.value = ifelse(p.value <0.001, \"<0.001\", sprintf(\"%.3f\", p.value)))\n\n\n연속변수, t.test\n\n\nconPvalue=dat1 %>%\n  mutate(stratas = !!sym(stratas)) %>%\n  select(stratas, conVars) %>%\n  pivot_longer(-c(stratas), names_to = \"variables\", values_to =\"values\") %>%\n  nest(dat = -variables) %>%\n  mutate(\n    fit   =map(dat, ~t.test(.$values ~ .$stratas)), \n    tidied=map(fit, tidy)\n  ) %>%\n  unnest(tidied) %>%\n  select(variables, p.value) %>%\n  mutate(p.value = ifelse(p.value <0.001, \"<0.001\", sprintf(\"%.3f\", p.value)))\n\n\nP.valeu 합치기\n\n\ntabPvalue = rbind(catPvalue, conPvalue)\n\n\n표가 어떤 순서대로 표현될지에 대한 순서를 정합니다.\n\n\nvarOrder = tibble(\"variables\"=c(catVars, conVars)) %>%\n  mutate(order = row_number())\nvarOrder\n\n# A tibble: 11 × 2\n   variables    order\n   <chr>        <int>\n 1 wwa1gp           1\n 2 wwa2gp           2\n 3 wwa3gp           3\n 4 wwa4gp           4\n 5 wwa5gp           5\n 6 shortReturn      6\n 7 sexgp            7\n 8 edugp            8\n 9 empgp            9\n10 AGE             10\n11 satisfaction    11"
  },
  {
    "objectID": "247_datamanipulation_for_table_2.html#table-1-정리하기",
    "href": "247_datamanipulation_for_table_2.html#table-1-정리하기",
    "title": "6  보건학표_2",
    "section": "6.4 Table 1 정리하기",
    "text": "6.4 Table 1 정리하기\n이제 변수값 요약과, p.value를 합치고, 순서에 맞게 정리하겠습니다.\n\ntab1 = tabDat %>%\n  left_join(tabPvalue, by=c(\"variables\")) %>%\n  left_join(varOrder, by = c(\"variables\")) %>%\n  arrange(order, values) %>%\n  group_by(variables) %>%\n  mutate(ranks = row_number()) %>%\n  mutate(p.value   = ifelse(ranks==min(ranks), p.value,   \"\")) %>% \n  mutate(variables = ifelse(ranks==min(ranks), variables, \"\")) %>%\n  ungroup() %>%\n  select(-order, -ranks)%>%\n  mutate(values = str_replace(values, \"[:digit:]\\\\.\", \"\"))\n\n이것을 htmlTable로 나타내면 다음과 같습니다.\n\ntab1 %>% htmlTable()\n\n\n\n\n\nvariables\nvalues\n0.non distrubance\n1.sleep disturbance\np.value\n\n\n\n\n1\nwwa1gp\nNever\n12222 (94.7%)\n684 (5.3%)\n<0.001\n\n\n2\n\nRarely\n12316 (94.5%)\n714 (5.5%)\n\n\n\n3\n\nSometimes\n9112 (90.3%)\n981 (9.7%)\n\n\n\n4\n\nOften\n3456 (82.8%)\n717 (17.2%)\n\n\n\n5\n\nAlways\n634 (70.0%)\n272 (30.0%)\n\n\n\n6\nwwa2gp\nNever\n12432 (95.4%)\n597 (4.6%)\n<0.001\n\n\n7\n\nRarely\n13682 (94.1%)\n863 (5.9%)\n\n\n\n8\n\nSometimes\n8882 (89.2%)\n1072 (10.8%)\n\n\n\n9\n\nOften\n2460 (79.4%)\n638 (20.6%)\n\n\n\n10\n\nAlways\n284 (58.9%)\n198 (41.1%)\n\n\n\n11\nwwa3gp\nNever\n13472 (95.2%)\n682 (4.8%)\n<0.001\n\n\n12\n\nRarely\n13938 (93.1%)\n1040 (6.9%)\n\n\n\n13\n\nSometimes\n7749 (89.9%)\n868 (10.1%)\n\n\n\n14\n\nOften\n2314 (79.5%)\n597 (20.5%)\n\n\n\n15\n\nAlways\n267 (59.6%)\n181 (40.4%)\n\n\n\n16\nwwa4gp\nNever\n18708 (94.1%)\n1169 (5.9%)\n<0.001\n\n\n17\n\nRarely\n14799 (91.6%)\n1362 (8.4%)\n\n\n\n18\n\nSometimes\n3348 (85.0%)\n592 (15.0%)\n\n\n\n19\n\nOften\n805 (80.7%)\n193 (19.3%)\n\n\n\n20\n\nAlways\n80 (60.6%)\n52 (39.4%)\n\n\n\n21\nwwa5gp\nNever\n19061 (93.9%)\n1245 (6.1%)\n<0.001\n\n\n22\n\nRarely\n14770 (91.6%)\n1349 (8.4%)\n\n\n\n23\n\nSometimes\n3180 (85.3%)\n550 (14.7%)\n\n\n\n24\n\nOften\n682 (79.8%)\n173 (20.2%)\n\n\n\n25\n\nAlways\n47 (48.0%)\n51 (52.0%)\n\n\n\n26\nshortReturn\nnon short return\n36183 (92.5%)\n2927 (7.5%)\n<0.001\n\n\n27\n\nshort return\n1557 (77.9%)\n441 (22.1%)\n\n\n\n28\nsexgp\nMen\n17892 (93.1%)\n1327 (6.9%)\n<0.001\n\n\n29\n\nWomen\n19848 (90.7%)\n2041 (9.3%)\n\n\n\n30\nedugp\nuniversity or more\n19597 (92.9%)\n1502 (7.1%)\n<0.001\n\n\n31\n\nhigh school\n14943 (91.9%)\n1318 (8.1%)\n\n\n\n32\n\nmiddle school or below\n3200 (85.4%)\n548 (14.6%)\n\n\n\n33\nempgp\npaid-worker\n25786 (92.4%)\n2122 (7.6%)\n<0.001\n\n\n34\n\nemployer/self-employer\n2539 (91.7%)\n229 (8.3%)\n\n\n\n35\n\nown-account worker\n8359 (90.5%)\n880 (9.5%)\n\n\n\n36\n\nunpaind family work\n1056 (88.5%)\n137 (11.5%)\n\n\n\n37\nAGE\n\n46.8±12.4\n49.7±11.9\n<0.001\n\n\n38\nsatisfaction\n\n2.1±0.5\n2.4±0.6\n<0.001\n\n\n\n\n\n\n6.4.1 htmlTable visulaization\n테이블이 너무 길기 때문에, wwa2, wwa3, wwa4, wwa5, 는 제외하고 표를 만들겠습니다.\n\ntab2 = tab1 %>% slice(-c(6:25))\ntab2 %>% \n  `rownames<-`(NULL)  %>%\n  htmlTable(rnames = FALSE) \n\n\n\n\n\nvariables\nvalues\n0.non distrubance\n1.sleep disturbance\np.value\n\n\n\n\nwwa1gp\nNever\n12222 (94.7%)\n684 (5.3%)\n<0.001\n\n\n\nRarely\n12316 (94.5%)\n714 (5.5%)\n\n\n\n\nSometimes\n9112 (90.3%)\n981 (9.7%)\n\n\n\n\nOften\n3456 (82.8%)\n717 (17.2%)\n\n\n\n\nAlways\n634 (70.0%)\n272 (30.0%)\n\n\n\nshortReturn\nnon short return\n36183 (92.5%)\n2927 (7.5%)\n<0.001\n\n\n\nshort return\n1557 (77.9%)\n441 (22.1%)\n\n\n\nsexgp\nMen\n17892 (93.1%)\n1327 (6.9%)\n<0.001\n\n\n\nWomen\n19848 (90.7%)\n2041 (9.3%)\n\n\n\nedugp\nuniversity or more\n19597 (92.9%)\n1502 (7.1%)\n<0.001\n\n\n\nhigh school\n14943 (91.9%)\n1318 (8.1%)\n\n\n\n\nmiddle school or below\n3200 (85.4%)\n548 (14.6%)\n\n\n\nempgp\npaid-worker\n25786 (92.4%)\n2122 (7.6%)\n<0.001\n\n\n\nemployer/self-employer\n2539 (91.7%)\n229 (8.3%)\n\n\n\n\nown-account worker\n8359 (90.5%)\n880 (9.5%)\n\n\n\n\nunpaind family work\n1056 (88.5%)\n137 (11.5%)\n\n\n\nAGE\n\n46.8±12.4\n49.7±11.9\n<0.001\n\n\nsatisfaction\n\n2.1±0.5\n2.4±0.6\n<0.001\n\n\n\n\n\n변수 이름을 변경하요 보기 좋게 변환시키고, cgroup을 이용하여 colum 부분을 조정합니다. Caption과 Footer를 작성하여 마무리 합니다.\n\ntab2 %>%\n  setNames(c(\"\", \"\", \"None\", \"Disturbance\", \"P value\")) %>%\n  htmlTable(\n    cgroup = c(\"\",  \"Sleep disturbance\", \"\"), \n    n.cgroup = c(2, 2, 1), \n    tfoot = \"P value calculated by Chisq-Test and T-Test\", \n    rnames = FALSE, \n    caption = \"Basic Characteristics according to Sleep disturbance\"\n  ) \n\n\n\n\n\nBasic Characteristics according to Sleep disturbance\n\n \nSleep disturbance \n\n\n\n\n \n \nNone\nDisturbance \n \nP value\n\n\n\n\nwwa1gp\nNever \n \n12222 (94.7%)\n684 (5.3%) \n \n<0.001\n\n\n\nRarely \n \n12316 (94.5%)\n714 (5.5%) \n \n\n\n\n\nSometimes \n \n9112 (90.3%)\n981 (9.7%) \n \n\n\n\n\nOften \n \n3456 (82.8%)\n717 (17.2%) \n \n\n\n\n\nAlways \n \n634 (70.0%)\n272 (30.0%) \n \n\n\n\nshortReturn\nnon short return \n \n36183 (92.5%)\n2927 (7.5%) \n \n<0.001\n\n\n\nshort return \n \n1557 (77.9%)\n441 (22.1%) \n \n\n\n\nsexgp\nMen \n \n17892 (93.1%)\n1327 (6.9%) \n \n<0.001\n\n\n\nWomen \n \n19848 (90.7%)\n2041 (9.3%) \n \n\n\n\nedugp\nuniversity or more \n \n19597 (92.9%)\n1502 (7.1%) \n \n<0.001\n\n\n\nhigh school \n \n14943 (91.9%)\n1318 (8.1%) \n \n\n\n\n\nmiddle school or below \n \n3200 (85.4%)\n548 (14.6%) \n \n\n\n\nempgp\npaid-worker \n \n25786 (92.4%)\n2122 (7.6%) \n \n<0.001\n\n\n\nemployer/self-employer \n \n2539 (91.7%)\n229 (8.3%) \n \n\n\n\n\nown-account worker \n \n8359 (90.5%)\n880 (9.5%) \n \n\n\n\n\nunpaind family work \n \n1056 (88.5%)\n137 (11.5%) \n \n\n\n\nAGE\n \n \n46.8±12.4\n49.7±11.9 \n \n<0.001\n\n\nsatisfaction\n \n \n2.1±0.5\n2.4±0.6 \n \n<0.001\n\n\n\nP value calculated by Chisq-Test and T-Test"
  },
  {
    "objectID": "247_datamanipulation_for_table_2.html#kwcsfunction",
    "href": "247_datamanipulation_for_table_2.html#kwcsfunction",
    "title": "6  보건학표_2",
    "section": "6.5 함수만들기",
    "text": "6.5 함수만들기\n지금가지 했던 일련의 과정을 함수로 정리하겠습니다.\n\nkwcsTabf = function(dat1, stratas, catVars, conVars){\nvarOrder = tibble(\"variables\"=c(catVars, conVars)) %>%\n  mutate(order = row_number())\n  \ncatTab = dat1 %>%\n  select(stratas, all_of(catVars)) %>%\n  pivot_longer(-c(stratas), names_to = \"variables\", values_to =\"values\")%>%\n  group_by( variables, values) %>%\n  count(!!sym(stratas)) %>%\n  mutate(prob = n/sum(n), \n         smry= sprintf(\"%.0f (%.1f%%)\", n, prob*100)\n  ) %>%\n  select(-n, -prob) %>%\n  ungroup() %>%\n  pivot_wider(names_from = stratas, values_from =smry) \n\nconTab = \n  dat1 %>%\n  select(stratas, all_of(conVars)) %>%\n  pivot_longer(-c(stratas), names_to = \"variables\", values_to =\"values\")%>%\n  group_by( !!sym(stratas), variables) %>%\n  summarise(avg = mean(values, na.rm =TRUE), \n            std = sd(values, na.rm =TRUE) \n  ) %>%\n  mutate(smry  = sprintf(\"%.1f\\u00b1%.1f\", avg, std)) %>%\n  select(stratas, variables, smry)%>%\n  ungroup() %>%\n  pivot_wider(names_from = stratas, values_from =smry) %>%\n  mutate(values =\"\") \ntabDat = rbind(catTab, conTab)\n\n\ncatPvalue = \n  dat1 %>%\n  select(stratas, catVars) %>%\n  pivot_longer(-c(stratas), names_to = \"variables\", values_to =\"values\")%>%\n  group_by(variables, values) %>%\n  count(!!sym(stratas)) %>%\n  pivot_wider(names_from = stratas, values_from =n) %>%\n  ungroup() %>%\n  select(-values) %>%\n  nest(dat = -variables) %>%\n  mutate(\n    fit = map(dat, \n              ~chisq.test(.x)), \n    tidied = map(fit, tidy)\n  ) %>%\n  unnest(tidied) %>%\n  select(variables, p.value) %>%\n  mutate(p.value = ifelse(p.value <0.001, \"<0.001\", sprintf(\"%.3f\", p.value)))\n\nconPvalue=dat1 %>%\n  mutate(stratas = !!sym(stratas)) %>%\n  select(stratas, conVars) %>%\n  pivot_longer(-c(stratas), names_to = \"variables\", values_to =\"values\") %>%\n  nest(dat = -variables) %>%\n  mutate(\n    fit   =map(dat, ~t.test(.$values ~ .$stratas)), \n    tidied=map(fit, tidy)\n  ) %>%\n  unnest(tidied) %>%\n  select(variables, p.value) %>%\n  mutate(p.value = ifelse(p.value <0.001, \"<0.001\", sprintf(\"%.3f\", p.value)))\n\ntabPvalue = rbind(catPvalue, conPvalue)\n\ntab1 = tabDat %>%\n  left_join(tabPvalue, by=c(\"variables\")) %>%\n  left_join(varOrder, by = c(\"variables\")) %>%\n  arrange(order, values) %>%\n  group_by(variables) %>%\n  mutate(ranks = row_number()) %>%\n  mutate(p.value   = ifelse(ranks==min(ranks), p.value,   \"\")) %>% \n  mutate(variables = ifelse(ranks==min(ranks), variables, \"\")) %>%\n  ungroup() %>%\n  select(-order, -ranks)%>%\n  mutate(values = str_replace(values, \"[:digit:]\\\\.\", \"\"))\nreturn(tab1)\n}\n\n층화변수, 명목변수, 연속변수를 구성하여 함수를 실행해 보겠습니다.\n\n## Variables selection\nstratas  = c(\"sleepgp\")\ncatVars = c(\n  # interesting\n  #\"wbalancegp\",\n  \"wwa1gp\", \"wwa2gp\",\"wwa3gp\",\"wwa4gp\",\"wwa5gp\",\n  # confounder\n  #\"whgp\", \n  \"shortReturn\",\"shiftWork\" , \"njob\", \n  # demograhpics\n  \"sexgp\", \"agegp\", \n  \"edugp\", \"empgp\" #, backpain\" \n)\nconVars = c(\"AGE\",\"satisfaction\")\n\ntab1 = kwcsTabf(dat1=dat1, stratas = stratas, catVars = catVars, conVars = conVars)\ntab1 %>% \n  setNames(c(\"\", \"\", \"None\", \"Disturbance\", \"P value\")) %>%\n  htmlTable(\n    cgroup = c(\"\",  \"Sleep disturbance\", \"\"), \n    n.cgroup = c(2, 2, 1), \n    tfoot = \"P value calculated by Chisq-Test and T-Test\", \n    rnames = FALSE, \n    caption = \"Basic Characteristics according to Sleep disturbance\"\n  ) \n\n\n\n\n\nBasic Characteristics according to Sleep disturbance\n\n \nSleep disturbance \n\n\n\n\n \n \nNone\nDisturbance \n \nP value\n\n\n\n\nwwa1gp\nNever \n \n12222 (94.7%)\n684 (5.3%) \n \n<0.001\n\n\n\nRarely \n \n12316 (94.5%)\n714 (5.5%) \n \n\n\n\n\nSometimes \n \n9112 (90.3%)\n981 (9.7%) \n \n\n\n\n\nOften \n \n3456 (82.8%)\n717 (17.2%) \n \n\n\n\n\nAlways \n \n634 (70.0%)\n272 (30.0%) \n \n\n\n\nwwa2gp\nNever \n \n12432 (95.4%)\n597 (4.6%) \n \n<0.001\n\n\n\nRarely \n \n13682 (94.1%)\n863 (5.9%) \n \n\n\n\n\nSometimes \n \n8882 (89.2%)\n1072 (10.8%) \n \n\n\n\n\nOften \n \n2460 (79.4%)\n638 (20.6%) \n \n\n\n\n\nAlways \n \n284 (58.9%)\n198 (41.1%) \n \n\n\n\nwwa3gp\nNever \n \n13472 (95.2%)\n682 (4.8%) \n \n<0.001\n\n\n\nRarely \n \n13938 (93.1%)\n1040 (6.9%) \n \n\n\n\n\nSometimes \n \n7749 (89.9%)\n868 (10.1%) \n \n\n\n\n\nOften \n \n2314 (79.5%)\n597 (20.5%) \n \n\n\n\n\nAlways \n \n267 (59.6%)\n181 (40.4%) \n \n\n\n\nwwa4gp\nNever \n \n18708 (94.1%)\n1169 (5.9%) \n \n<0.001\n\n\n\nRarely \n \n14799 (91.6%)\n1362 (8.4%) \n \n\n\n\n\nSometimes \n \n3348 (85.0%)\n592 (15.0%) \n \n\n\n\n\nOften \n \n805 (80.7%)\n193 (19.3%) \n \n\n\n\n\nAlways \n \n80 (60.6%)\n52 (39.4%) \n \n\n\n\nwwa5gp\nNever \n \n19061 (93.9%)\n1245 (6.1%) \n \n<0.001\n\n\n\nRarely \n \n14770 (91.6%)\n1349 (8.4%) \n \n\n\n\n\nSometimes \n \n3180 (85.3%)\n550 (14.7%) \n \n\n\n\n\nOften \n \n682 (79.8%)\n173 (20.2%) \n \n\n\n\n\nAlways \n \n47 (48.0%)\n51 (52.0%) \n \n\n\n\nshortReturn\nnon short return \n \n36183 (92.5%)\n2927 (7.5%) \n \n<0.001\n\n\n\nshort return \n \n1557 (77.9%)\n441 (22.1%) \n \n\n\n\nshiftWork\nnon shift work \n \n35056 (91.9%)\n3073 (8.1%) \n \n<0.001\n\n\n\nshift work \n \n2684 (90.1%)\n295 (9.9%) \n \n\n\n\nnjob\none-job \n \n37471 (91.9%)\n3317 (8.1%) \n \n<0.001\n\n\n\nnjob \n \n269 (84.1%)\n51 (15.9%) \n \n\n\n\nsexgp\nMen \n \n17892 (93.1%)\n1327 (6.9%) \n \n<0.001\n\n\n\nWomen \n \n19848 (90.7%)\n2041 (9.3%) \n \n\n\n\nagegp\n<25 \n \n1359 (96.6%)\n48 (3.4%) \n \n<0.001\n\n\n\n<30 \n \n2697 (94.3%)\n163 (5.7%) \n \n\n\n\n\n<35 \n \n3308 (93.6%)\n225 (6.4%) \n \n\n\n\n\n<40 \n \n3995 (92.3%)\n334 (7.7%) \n \n\n\n\n\n<45 \n \n4414 (92.2%)\n372 (7.8%) \n \n\n\n\n\n<50 \n \n4934 (92.7%)\n388 (7.3%) \n \n\n\n\n\n<55 \n \n5300 (91.9%)\n465 (8.1%) \n \n\n\n\n\n<60 \n \n5099 (90.0%)\n566 (10.0%) \n \n\n\n\n\n≥60 \n \n6634 (89.2%)\n807 (10.8%) \n \n\n\n\nedugp\nuniversity or more \n \n19597 (92.9%)\n1502 (7.1%) \n \n<0.001\n\n\n\nhigh school \n \n14943 (91.9%)\n1318 (8.1%) \n \n\n\n\n\nmiddle school or below \n \n3200 (85.4%)\n548 (14.6%) \n \n\n\n\nempgp\npaid-worker \n \n25786 (92.4%)\n2122 (7.6%) \n \n<0.001\n\n\n\nemployer/self-employer \n \n2539 (91.7%)\n229 (8.3%) \n \n\n\n\n\nown-account worker \n \n8359 (90.5%)\n880 (9.5%) \n \n\n\n\n\nunpaind family work \n \n1056 (88.5%)\n137 (11.5%) \n \n\n\n\nAGE\n \n \n46.8±12.4\n49.7±11.9 \n \n<0.001\n\n\nsatisfaction\n \n \n2.1±0.5\n2.4±0.6 \n \n<0.001\n\n\n\nP value calculated by Chisq-Test and T-Test"
  },
  {
    "objectID": "248_datamanipulation_for_table_3.html#데이터-준비",
    "href": "248_datamanipulation_for_table_3.html#데이터-준비",
    "title": "7  보건학표_3",
    "section": "7.1 데이터 준비",
    "text": "7.1 데이터 준비\n데이터 표를 만드는 실습은 6차 근로환경조사 자료를 통해 실습할 것입니다.. 자료는 안전보건공단, 근로환경조사 원시자료 사이트 (http://kosha.or.kr/kosha/data/primitiveData.do) 에서 신청할 수 있습니다..\n\nrm(list=ls())\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"htmlTable\")) install.packages(\"htmlTable\")\nif(!require(\"haven\")) install.packages(\"haven\")\nif(!require(\"broom\")) install.packages(\"broom\")\n\n데이터 표를 만드는 실습은 6차 근로환경조사 자료를 통해 실습할 것입니다.. 자료는 안전보건공단, 근로환경조사 원시자료 사이트 (http://kosha.or.kr/kosha/data/primitiveData.do) 에서 신청할 수 있습니다.. 데이터를 불러오겠습니다. 안전보건공단 홈페이에서 자료를 다운 받는게 원칙입니다. 다만 실습을 빠르게 진행하기 위해서, dspubs.org 페이지에 있는 파일을 이용하겠습니다.   kwcsData6th.rds   자신의 folder에 data 라는 folder가 있는지 확인하십시오. data라는 폴더에 다운로드하고, 불러오도록 하겠습니다.\n\nurl <- \"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/kwcsData6th.rds\"\ndownload.file(url, \"data/tutorKWCS.rds\")\n\n\nkwcs = readRDS(\"data/tutorKWCS.rds\")\n\n\n7.1.1 근로환경조사 기본 변수 생성 (선택)\ndata manipulation에서 실습한 내용을 통해 변수를 실습을 위한 변수를 생성해 보겠습니다. 이에 대한 자세한 과정은 이전 챔터에 있습니다. \nTable 1 변수 생성과정 챕터\n\n# data manip function\nLikert5f = function(x){dplyr::recode(as.numeric(x), \n         \"4.Always\",\"3.Often\",\"2.Sometimes\", \"1.Rarely\",\"0.Never\" )}\n\n# data step start ----------\n\ndat1 = kwcs %>%\n  filter(AGE <70) %>%\n  filter(AGE >18) %>%\n  # sleep --------\n  filter(!is.na(sleep1)&!is.na(sleep2)&!is.na(sleep3)) %>%\n  filter(sleep1 %in% c(1:5), \n         sleep2 %in% c(1:5), \n         sleep3 %in% c(1:5)) %>%\n  mutate(sleep1in = 5-sleep1, \n         sleep2in = 5-sleep2,\n         sleep3in = 5-sleep3\n         ) %>%\n  mutate(sleepgp = case_when(\n    sleep1in + sleep2in+ sleep3in >=6 ~ \"1.sleep disturbance\", \n    TRUE ~ \"0.non distrubance\"\n  )) %>%\n  # work live balances -------------\n  filter(!is.na(wbalance), !is.na(wwa1), !is.na(wwa2), !is.na(wwa3), !is.na(wwa4), !is.na(wwa5)) %>%\n  filter(!is.na(wbalance)) %>%\n  mutate(wbalancegp = case_when(\n    wbalance %in% c(1, 2) ~ \"0.balance\", \n    TRUE ~ \"1.non balance\"\n  )) %>%\n  mutate(wwa1gp=Likert5f(wwa1), wwa2gp=Likert5f(wwa2),  wwa3gp=Likert5f(wwa3), \n         wwa4gp=Likert5f(wwa4), wwa5gp=Likert5f(wwa5), \n         ) %>%\n  # job and sex, agegp  ----------\n  filter(!is.na(job1))%>%\n  filter(job1 %in% c(1, 2, 3)) %>%\n  mutate(sexgp = case_when(\n    TSEX ==1 ~ \"Men\", \n    TRUE ~ \"Women\"\n  )) %>%\n  mutate(agegp = case_when(AGE <25 ~ \"<25\", AGE <30 ~ \"<30\",  AGE <35 ~ \"<35\", \n    AGE <40 ~ \"<40\", AGE <45 ~ \"<45\", AGE <50 ~ \"<50\",AGE <55 ~ \"<55\", AGE <60 ~ \"<60\",\n    TRUE ~ \"\\u226560\" # 나머지는 모두 >65 (\\u2265는 크거나 같다는 symbol)\n  )) %>%\n  filter(!is.na(edu)) %>% filter(edu %in% c(1:7)) %>%\n  mutate(edugp = case_when(\n    edu %in% c(1:3) ~    \"2.middle school or below\",\n    edu %in% c(4  ) ~    \"1.high school\",\n    edu %in% c(5:7)   ~  \"0.university or more\"\n  )) %>%\n  mutate(njob=case_when(\n    job1 %in% c(2, 3) ~ \"1.njob\", \n    TRUE ~ \"0.one-job\")) %>%\n  # pains back -----------\n  mutate(backpain= case_when(\n    heal_prob1==1 ~ \"pain\", \n    TRUE ~ \"no-pain\")) %>%\n  # emp_type,working hours, shiftwork, work life balance -------\n  filter(!is.na(emp_type)) %>%\n  filter(emp_type %in% c(1:4)) %>%\n  mutate(empgp = case_when(\n    emp_type ==1 ~ \"2.own-account worker\", \n    emp_type ==2 ~ \"1.employer/self-employer\", \n    emp_type ==3 ~ \"0.paid-worker\", \n    emp_type ==4 ~ \"3.unpaind family work\"\n  )) %>%\n  filter(!is.na(wtime_week)) %>%\n  mutate(whgp=case_when(\n    wtime_week < 35 ~ \"<35\", \n    wtime_week < 40 ~ \"<40\", \n    wtime_week < 52 ~ \"<52\", \n    wtime_week < 60 ~ \"<60\", \n    TRUE ~ \"\\u226560\", \n  )) %>%\n  filter(!is.na(wtime_length5)) %>%\n  filter(wtime_length5 %in% c(1, 2)) %>%\n  mutate(shiftWork = case_when(\n    wtime_length5 ==1 ~ \"1.shift work\", \n    TRUE ~ \"0.non shift work\"\n  )) %>%\n  filter(!is.na(wtime_resilience)) %>%\n  mutate(shortReturn = case_when(\n    wtime_resilience ==1 ~ \"1.short return\", \n    TRUE ~ \"0.non short return\"\n  )) %>%\n  filter(!is.na(satisfaction)) %>%\n  filter(satisfaction %in% c(1:4)) %>%\n  mutate(satisInv = 5-satisfaction) %>%\n  mutate(shiftShort=case_when(\n    shiftWork == \"1.shift work\" & shortReturn == \"1.short return\" ~ \"3.shift work with short return\", \n    shiftWork == \"1.shift work\" & shortReturn != \"1.short return\" ~ \"2.shift work\", \n    shiftWork != \"1.shift work\" & shortReturn == \"1.short return\" ~ \"1.day work with short return\", \n    shiftWork != \"1.shift work\" & shortReturn != \"1.short return\" ~ \"0.day work\", \n  )) \n\n코드를 간단히 하기 위해 dat1을 저장하겠습니다.\n\nsaveRDS(dat1, \"data/kwcsData1.rds\")\n\n\n\n7.1.2 함수를 source file 로 저장하기\n이전에 만들었던 함수 kwcsTabf.R 이라는 파일로 저장하겠습니다.  이전에 만들었던 함수는 kwcs table 1 함수 만들기에 있습니다. 그 함수를 source 파일로 저장하겠습니다.\n\n\n\nsource file 사용\n\n함수를 R script로 저장 (“source/kwcsTabf.R”)\nsource file 불러오기 (source(“source/kwcsTabf.R”))\n함수 사용하기.\n\n\n\nurl1 <-\"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/kwcsTabf.R\"\ndownload.file(url1, \"source/kwcsTabf.R\")\n\nurl2 <-\"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/oddsTabf.R\"\ndownload.file(url2, \"source/oddsTabf.R\")\n\n\nsource(\"source/kwcsTabf.R\")\n\n\nrm(list=ls())\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"htmlTable\")) install.packages(\"htmlTable\")\nif(!require(\"haven\")) install.packages(\"haven\")\nif(!require(\"broom\")) install.packages(\"broom\")\nsource(\"source/kwcsTabf.R\")\ndat1 = readRDS(\"data/kwcsData1.rds\")\n\n층화변수, 명목변수, 연속변수를 구성하여 함수를 실행해 보겠습니다.\n\n## Variables selection\nstratas  = c(\"sleepgp\")\ncatVars = c(\n  # interesting\n  #\"wbalancegp\",\n  \"wwa1gp\", \"wwa2gp\",\"wwa3gp\",\"wwa4gp\",\"wwa5gp\",\n  # confounder\n  #\"whgp\", \n  \"shortReturn\",\"shiftWork\" , \"njob\", \n  # demograhpics\n  \"sexgp\", \"agegp\", \n  \"edugp\", \"empgp\" #, backpain\" \n)\nconVars = c(\"AGE\",\"satisfaction\")\n\ntab1 = kwcsTabf(dat1=dat1, stratas = stratas, catVars = catVars, conVars = conVars)\ntab1 %>% \n  setNames(c(\"\", \"\", \"None\", \"Disturbance\", \"P value\")) %>%\n  htmlTable(\n    cgroup = c(\"\",  \"Sleep disturbance\", \"\"), \n    n.cgroup = c(2, 2, 1), \n    tfoot = \"P value calculated by Chisq-Test and T-Test\", \n    rnames = FALSE, \n    caption = \"Basic Characteristics according to Sleep disturbance\"\n  ) \n\n\n\n\n\nBasic Characteristics according to Sleep disturbance\n\n \nSleep disturbance \n\n\n\n\n \n \nNone\nDisturbance \n \nP value\n\n\n\n\nwwa1gp\nNever \n \n12222 (94.7%)\n684 (5.3%) \n \n<0.001\n\n\n\nRarely \n \n12316 (94.5%)\n714 (5.5%) \n \n\n\n\n\nSometimes \n \n9112 (90.3%)\n981 (9.7%) \n \n\n\n\n\nOften \n \n3456 (82.8%)\n717 (17.2%) \n \n\n\n\n\nAlways \n \n634 (70.0%)\n272 (30.0%) \n \n\n\n\nwwa2gp\nNever \n \n12432 (95.4%)\n597 (4.6%) \n \n<0.001\n\n\n\nRarely \n \n13682 (94.1%)\n863 (5.9%) \n \n\n\n\n\nSometimes \n \n8882 (89.2%)\n1072 (10.8%) \n \n\n\n\n\nOften \n \n2460 (79.4%)\n638 (20.6%) \n \n\n\n\n\nAlways \n \n284 (58.9%)\n198 (41.1%) \n \n\n\n\nwwa3gp\nNever \n \n13472 (95.2%)\n682 (4.8%) \n \n<0.001\n\n\n\nRarely \n \n13938 (93.1%)\n1040 (6.9%) \n \n\n\n\n\nSometimes \n \n7749 (89.9%)\n868 (10.1%) \n \n\n\n\n\nOften \n \n2314 (79.5%)\n597 (20.5%) \n \n\n\n\n\nAlways \n \n267 (59.6%)\n181 (40.4%) \n \n\n\n\nwwa4gp\nNever \n \n18708 (94.1%)\n1169 (5.9%) \n \n<0.001\n\n\n\nRarely \n \n14799 (91.6%)\n1362 (8.4%) \n \n\n\n\n\nSometimes \n \n3348 (85.0%)\n592 (15.0%) \n \n\n\n\n\nOften \n \n805 (80.7%)\n193 (19.3%) \n \n\n\n\n\nAlways \n \n80 (60.6%)\n52 (39.4%) \n \n\n\n\nwwa5gp\nNever \n \n19061 (93.9%)\n1245 (6.1%) \n \n<0.001\n\n\n\nRarely \n \n14770 (91.6%)\n1349 (8.4%) \n \n\n\n\n\nSometimes \n \n3180 (85.3%)\n550 (14.7%) \n \n\n\n\n\nOften \n \n682 (79.8%)\n173 (20.2%) \n \n\n\n\n\nAlways \n \n47 (48.0%)\n51 (52.0%) \n \n\n\n\nshortReturn\nnon short return \n \n36183 (92.5%)\n2927 (7.5%) \n \n<0.001\n\n\n\nshort return \n \n1557 (77.9%)\n441 (22.1%) \n \n\n\n\nshiftWork\nnon shift work \n \n35056 (91.9%)\n3073 (8.1%) \n \n<0.001\n\n\n\nshift work \n \n2684 (90.1%)\n295 (9.9%) \n \n\n\n\nnjob\none-job \n \n37471 (91.9%)\n3317 (8.1%) \n \n<0.001\n\n\n\nnjob \n \n269 (84.1%)\n51 (15.9%) \n \n\n\n\nsexgp\nMen \n \n17892 (93.1%)\n1327 (6.9%) \n \n<0.001\n\n\n\nWomen \n \n19848 (90.7%)\n2041 (9.3%) \n \n\n\n\nagegp\n<25 \n \n1359 (96.6%)\n48 (3.4%) \n \n<0.001\n\n\n\n<30 \n \n2697 (94.3%)\n163 (5.7%) \n \n\n\n\n\n<35 \n \n3308 (93.6%)\n225 (6.4%) \n \n\n\n\n\n<40 \n \n3995 (92.3%)\n334 (7.7%) \n \n\n\n\n\n<45 \n \n4414 (92.2%)\n372 (7.8%) \n \n\n\n\n\n<50 \n \n4934 (92.7%)\n388 (7.3%) \n \n\n\n\n\n<55 \n \n5300 (91.9%)\n465 (8.1%) \n \n\n\n\n\n<60 \n \n5099 (90.0%)\n566 (10.0%) \n \n\n\n\n\n≥60 \n \n6634 (89.2%)\n807 (10.8%) \n \n\n\n\nedugp\nuniversity or more \n \n19597 (92.9%)\n1502 (7.1%) \n \n<0.001\n\n\n\nhigh school \n \n14943 (91.9%)\n1318 (8.1%) \n \n\n\n\n\nmiddle school or below \n \n3200 (85.4%)\n548 (14.6%) \n \n\n\n\nempgp\npaid-worker \n \n25786 (92.4%)\n2122 (7.6%) \n \n<0.001\n\n\n\nemployer/self-employer \n \n2539 (91.7%)\n229 (8.3%) \n \n\n\n\n\nown-account worker \n \n8359 (90.5%)\n880 (9.5%) \n \n\n\n\n\nunpaind family work \n \n1056 (88.5%)\n137 (11.5%) \n \n\n\n\nAGE\n \n \n46.8±12.4\n49.7±11.9 \n \n<0.001\n\n\nsatisfaction\n \n \n2.1±0.5\n2.4±0.6 \n \n<0.001\n\n\n\nP value calculated by Chisq-Test and T-Test"
  },
  {
    "objectID": "248_datamanipulation_for_table_3.html#logistic-regression-table",
    "href": "248_datamanipulation_for_table_3.html#logistic-regression-table",
    "title": "7  보건학표_3",
    "section": "7.2 Logistic Regression Table",
    "text": "7.2 Logistic Regression Table\n일과 삶의 균형이 수면장애와 관련이 있는지 Logistic regression 을 수행해보고 이를 표로 만들어 보겠습니다. \nR 로 로지스틱회귀분석을 수행하는 것은, 3가지 파트로 구성됩니다.\n\nLogistic Regression with R\n\ndata\nfamily\nmodel formula\n\n\n이때 family는 “binomial”로 formula에서 종속변수의 값을 지정해 주고, ~ 이후에 보정변수를 +로 넣는 방법입니다. 아래의 예시를 보겠습니다.\n\nmod1 = dat1 %>% \n  glm(data=.,              # data 는 . 을 통해 상속 받고, \n      family=\"binomial\",   # binomial로 구성\n      formula = sleepgp == \"1.sleep disturbance\"  # 모형 설정\n                ~ wwa1gp)\nsummary(mod1) # 모형 결과 정리\n\n\nCall:\nglm(formula = sleepgp == \"1.sleep disturbance\" ~ wwa1gp, family = \"binomial\", \n    data = .)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.8450  -0.4522  -0.3357  -0.3300   2.4238  \n\nCoefficients:\n                  Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -2.88303    0.03929 -73.378   <2e-16 ***\nwwa1gp1.Rarely     0.03526    0.05500   0.641    0.521    \nwwa1gp2.Sometimes  0.65426    0.05170  12.655   <2e-16 ***\nwwa1gp3.Often      1.31024    0.05681  23.062   <2e-16 ***\nwwa1gp4.Always     2.03679    0.08245  24.704   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 23305  on 41107  degrees of freedom\nResidual deviance: 22258  on 41103  degrees of freedom\nAIC: 22268\n\nNumber of Fisher Scoring iterations: 5\n\n\n우선 summary결과는 어떤 모델을 사용했는지 보여줍니다. 여기서 표를 만드는 이 수업의 목적에 가장 필요한 것은 Coefficients에 있습니다.  각 변수에 대해서 Estimation에 logistic regression coefficients 값을 보여주고 있습니다. 이 값을 exp(.) 한 값이 Odds Ratio 가 될 것입니다. 우리가 0.Never를 기준으로 하여 값을 산출한 것이고, 이 값은 summary에 표시되어 있지 않습니다.  그럼 coefficients 값을 가져오고, 여기에 exp(.)을 해서 odds ratio 값을 얻어 보겠습니다. 95% 신뢰구간은 confint.default(model)을 이용해서 구할 수 있습니다. p value 값도 가져오겠습니다.\n\nmod1$coefficients %>% exp(.)\n\n      (Intercept)    wwa1gp1.Rarely wwa1gp2.Sometimes     wwa1gp3.Often \n       0.05596465        1.03589255        1.92371829        3.70707693 \n   wwa1gp4.Always \n       7.66594719 \n\nconfint.default(mod1) %>% exp(.)\n\n                       2.5 %     97.5 %\n(Intercept)       0.05181673 0.06044461\nwwa1gp1.Rarely    0.93002700 1.15380884\nwwa1gp2.Sometimes 1.73834069 2.12886464\nwwa1gp3.Often     3.31643846 4.14372813\nwwa1gp4.Always    6.52209339 9.01041165\n\nmod1 %>% tidy() %>% select(term, p.value) # p value\n\n# A tibble: 5 × 2\n  term                p.value\n  <chr>                 <dbl>\n1 (Intercept)       0        \n2 wwa1gp1.Rarely    5.21e-  1\n3 wwa1gp2.Sometimes 1.05e- 36\n4 wwa1gp3.Often     1.11e-117\n5 wwa1gp4.Always    9.62e-135\n\n\n각각을 엑셀등에 붙여 넣기 하여 표를 만들면 됩니다.\n\n7.2.1 로지스틱회귀분석 표 함수 1\n반복해서 만드는 것도 좋지만, 함수를 만들어 사용하는 것이 편리할 때도 있습니다. 만약 여러 데이터를 탐색적으로 분석할 때, 어떠한 변수가 유의한지 반복적으로 보고서를 만들때, 실수와 시간을 줄이려면 함수를 만들 필요가 있습니다.  앞서서 만들 Odds Ratio, 95% CI, p value 를 합쳐 보겠습니다. 이것을 합치면, 우리가 원하는 표의 기본이 형성되었습니다.\n\ncbind(mod1$coefficients%>% exp(), confint.default(mod1)%>% exp(), mod1 %>% tidy() %>% select(p.value)) \n\n                  mod1$coefficients %>% exp()      2.5 %     97.5 %\n(Intercept)                        0.05596465 0.05181673 0.06044461\nwwa1gp1.Rarely                     1.03589255 0.93002700 1.15380884\nwwa1gp2.Sometimes                  1.92371829 1.73834069 2.12886464\nwwa1gp3.Often                      3.70707693 3.31643846 4.14372813\nwwa1gp4.Always                     7.66594719 6.52209339 9.01041165\n                        p.value\n(Intercept)        0.000000e+00\nwwa1gp1.Rarely     5.214505e-01\nwwa1gp2.Sometimes  1.048593e-36\nwwa1gp3.Often     1.108932e-117\nwwa1gp4.Always    9.619530e-135\n\n\n좀 코드가 길어보이니 함수를 만들어 보겠습니다. 이름을 mod summry function 을 줄여 modsmryf 으로 하겠습니다.\n\nmodsmryf=function(mod) {\n  cbind(mod$coefficients %>% exp(.), confint.default(mod)%>% exp(.), mod %>% tidy() %>% select(p.value))}\n\n\nmodsmryf(mod1)\n\n                  mod$coefficients %>% exp(.)      2.5 %     97.5 %\n(Intercept)                        0.05596465 0.05181673 0.06044461\nwwa1gp1.Rarely                     1.03589255 0.93002700 1.15380884\nwwa1gp2.Sometimes                  1.92371829 1.73834069 2.12886464\nwwa1gp3.Often                      3.70707693 3.31643846 4.14372813\nwwa1gp4.Always                     7.66594719 6.52209339 9.01041165\n                        p.value\n(Intercept)        0.000000e+00\nwwa1gp1.Rarely     5.214505e-01\nwwa1gp2.Sometimes  1.048593e-36\nwwa1gp3.Often     1.108932e-117\nwwa1gp4.Always    9.619530e-135\n\n\n이번에는 두번째 모델을 만들어 보겠습니다. 성별과 연령을 보정한 모델입니다. 이 모델도 앞서 만든 modsmryf 함수로 잘 표현될 수 있는 지 확인해 보겠습니다.\n\nmod2 = dat1 %>%\n  glm(data=., \n      family=\"binomial\", \n      formula = sleepgp == \"1.sleep disturbance\" \n                ~ wwa1gp + \n                  AGE + sexgp +satisfaction\n        )\nmodsmryf(mod2)\n\n                  mod$coefficients %>% exp(.)       2.5 %      97.5 %\n(Intercept)                       0.003804483 0.003053111 0.004740768\nwwa1gp1.Rarely                    1.001621017 0.898377792 1.116729143\nwwa1gp2.Sometimes                 1.833495422 1.654865417 2.031407164\nwwa1gp3.Often                     3.420379336 3.053476051 3.831369432\nwwa1gp4.Always                    6.907050334 5.845309980 8.161644888\nAGE                               1.013712744 1.010590103 1.016845033\nsexgpWomen                        1.532246823 1.421896138 1.651161618\nsatisfaction                      2.288037866 2.145736519 2.439776380\n                        p.value\n(Intercept)        0.000000e+00\nwwa1gp1.Rarely     9.767193e-01\nwwa1gp2.Sometimes  4.552779e-31\nwwa1gp3.Often     3.967119e-100\nwwa1gp4.Always    5.123559e-114\nAGE                5.044326e-18\nsexgpWomen         4.563279e-29\nsatisfaction      7.930347e-141\n\n\n이번엔 세번째는 shiftwork, njob을 보정해 보겠습니다. model 3를 의미하는 mod3로 저장하고 만들어 보겠습니다.\n\nmod3 = dat1 %>%\n  glm(data=., \n      family=\"binomial\", \n      formula = sleepgp == \"1.sleep disturbance\" \n                ~ wwa1gp + \n                  AGE + sexgp +satisfaction+\n                  shiftWork + njob\n        )\nmodsmryf(mod3)\n\n                      mod$coefficients %>% exp(.)       2.5 %      97.5 %\n(Intercept)                           0.003567629 0.002858947 0.004451981\nwwa1gp1.Rarely                        1.012175630 0.907728389 1.128641032\nwwa1gp2.Sometimes                     1.877721635 1.694025217 2.081337693\nwwa1gp3.Often                         3.491071324 3.114898897 3.912672415\nwwa1gp4.Always                        7.046449183 5.959274630 8.331961382\nAGE                                   1.014223241 1.011097186 1.017358962\nsexgpWomen                            1.542246850 1.431082137 1.662046702\nsatisfaction                          2.272860173 2.131360030 2.423754454\nshiftWork1.shift work                 1.510263841 1.324733670 1.721777683\nnjob1.njob                            1.588563409 1.150750845 2.192945340\n                            p.value\n(Intercept)            0.000000e+00\nwwa1gp1.Rarely         8.275949e-01\nwwa1gp2.Sometimes      3.779474e-33\nwwa1gp3.Often         1.843134e-102\nwwa1gp4.Always        1.977308e-115\nAGE                    3.049355e-19\nsexgpWomen             7.361792e-30\nsatisfaction          2.554569e-138\nshiftWork1.shift work  7.049028e-10\nnjob1.njob             4.899856e-03\n\n\n이제 모델1, 2, 3가 만들어 졌습니다. 이걸 반복해서 만들고, 엑셀등에 붙여 넣기 하여 표를 만들면 됩니다.\n\n\n7.2.2 로지스틱 회귀분석 표 2\n앞선 함수를 조금더 업데이트 하겠습니다. 보통 보기편한 방식은 OR (95%CI) 로 표시하는 것입니다. 또한, 기준이되는 변수 값에는 “1.00 (reference)”로 표시하는 것이 필요합니다. 그럴려면, 사용된 변수의 모든 변수값의 종류는 표에 나타내어야 하는 basic matrix 가 필요합니다. 따라서 \n\n로지스틱 회귀분석 함수\n\n사용된 변수의 Basic Matrix 만들기\n기준된 변수 값에는 “1.00 (reference)” 값 주기\n나머진 변수 에는 OR (95% CI) 나타내기\n\n\n를 수행하면 됩니다.\n\nbasic matrix\n\n명목변수(factor 등)과 연속변수를 나누어 basic matrix 를 만들겠습니다. 이는 logistic regression model 의 xlevels 와 model에 있습니다.\n\nmod3$xlevels \n\n$wwa1gp\n[1] \"0.Never\"     \"1.Rarely\"    \"2.Sometimes\" \"3.Often\"     \"4.Always\"   \n\n$sexgp\n[1] \"Men\"   \"Women\"\n\n$shiftWork\n[1] \"0.non shift work\" \"1.shift work\"    \n\n$njob\n[1] \"0.one-job\" \"1.njob\"   \n\nmod3$model %>% \n  slice(1:2) %>% \n  select(where(is.numeric))\n\n  AGE satisfaction\n1  54            2\n2  64            2\n\n\n이것을 data.frame 형식으로 만들겠습니다. list 형식으로, list 뒤에 [1] 을 통해 이름을, [[1]]을 통해 list 값을 가져오겠습니다.\n\nt1 = mod3$xlevel\ntibble(names(t1)[1], values=t1[[1]])\n\n# A tibble: 5 × 2\n  `names(t1)[1]` values     \n  <chr>          <chr>      \n1 wwa1gp         0.Never    \n2 wwa1gp         1.Rarely   \n3 wwa1gp         2.Sometimes\n4 wwa1gp         3.Often    \n5 wwa1gp         4.Always   \n\n\n이것을 함수로 나타내면, 아래와 같고, 이를 반복하면 다음과 같습니다 . 그런데 몇번 반복해야 할 까요?\n\ntest = function(x){\n  tibble(names(t1)[x], values=t1[[x]])\n}\nfor (i in 1:4){\nprint(test(i))\n}\n\n# A tibble: 5 × 2\n  `names(t1)[x]` values     \n  <chr>          <chr>      \n1 wwa1gp         0.Never    \n2 wwa1gp         1.Rarely   \n3 wwa1gp         2.Sometimes\n4 wwa1gp         3.Often    \n5 wwa1gp         4.Always   \n# A tibble: 2 × 2\n  `names(t1)[x]` values\n  <chr>          <chr> \n1 sexgp          Men   \n2 sexgp          Women \n# A tibble: 2 × 2\n  `names(t1)[x]` values          \n  <chr>          <chr>           \n1 shiftWork      0.non shift work\n2 shiftWork      1.shift work    \n# A tibble: 2 × 2\n  `names(t1)[x]` values   \n  <chr>          <chr>    \n1 njob           0.one-job\n2 njob           1.njob   \n\n\n몇 번 반복할지는 변수의 갯수가 몇개인지에 따라 달라집니다. 이를 통해 lapply, map을 이용해서 수행하겠습니다.\n\nlength(mod3$xlevels) # 4개의 list가 있음을 알수 있습니다. \n\n[1] 4\n\nlapply(1:4, test)\n\n[[1]]\n# A tibble: 5 × 2\n  `names(t1)[x]` values     \n  <chr>          <chr>      \n1 wwa1gp         0.Never    \n2 wwa1gp         1.Rarely   \n3 wwa1gp         2.Sometimes\n4 wwa1gp         3.Often    \n5 wwa1gp         4.Always   \n\n[[2]]\n# A tibble: 2 × 2\n  `names(t1)[x]` values\n  <chr>          <chr> \n1 sexgp          Men   \n2 sexgp          Women \n\n[[3]]\n# A tibble: 2 × 2\n  `names(t1)[x]` values          \n  <chr>          <chr>           \n1 shiftWork      0.non shift work\n2 shiftWork      1.shift work    \n\n[[4]]\n# A tibble: 2 × 2\n  `names(t1)[x]` values   \n  <chr>          <chr>    \n1 njob           0.one-job\n2 njob           1.njob   \n\nmap(1:4, test)\n\n[[1]]\n# A tibble: 5 × 2\n  `names(t1)[x]` values     \n  <chr>          <chr>      \n1 wwa1gp         0.Never    \n2 wwa1gp         1.Rarely   \n3 wwa1gp         2.Sometimes\n4 wwa1gp         3.Often    \n5 wwa1gp         4.Always   \n\n[[2]]\n# A tibble: 2 × 2\n  `names(t1)[x]` values\n  <chr>          <chr> \n1 sexgp          Men   \n2 sexgp          Women \n\n[[3]]\n# A tibble: 2 × 2\n  `names(t1)[x]` values          \n  <chr>          <chr>           \n1 shiftWork      0.non shift work\n2 shiftWork      1.shift work    \n\n[[4]]\n# A tibble: 2 × 2\n  `names(t1)[x]` values   \n  <chr>          <chr>    \n1 njob           0.one-job\n2 njob           1.njob   \n\nmap(1:length(mod3$xlevels), test)\n\n[[1]]\n# A tibble: 5 × 2\n  `names(t1)[x]` values     \n  <chr>          <chr>      \n1 wwa1gp         0.Never    \n2 wwa1gp         1.Rarely   \n3 wwa1gp         2.Sometimes\n4 wwa1gp         3.Often    \n5 wwa1gp         4.Always   \n\n[[2]]\n# A tibble: 2 × 2\n  `names(t1)[x]` values\n  <chr>          <chr> \n1 sexgp          Men   \n2 sexgp          Women \n\n[[3]]\n# A tibble: 2 × 2\n  `names(t1)[x]` values          \n  <chr>          <chr>           \n1 shiftWork      0.non shift work\n2 shiftWork      1.shift work    \n\n[[4]]\n# A tibble: 2 × 2\n  `names(t1)[x]` values   \n  <chr>          <chr>    \n1 njob           0.one-job\n2 njob           1.njob   \n\n\n이것을 통해 basic matrix를 만듭니다.\n\nt1 = mod3$xlevels\nbm1 = map(1:length(t1), function(x){\n  tibble(variables = names(t1)[x], \n         values = t1[[x]])\n}) %>% do.call(rbind, .)\nbm2 = mod3$model %>% \n  slice(1:2) %>% \n  select(where(is.numeric)) %>% \n  pivot_longer(-c()) %>% \n  select(variables = name) %>%\n  mutate(values =\"\") %>% unique()\nbm0 =rbind(bm1, bm2) %>%\n  mutate(keys=paste0(variables, values))\nbm0 %>% htmlTable()\n\n\n\n\n\nvariables\nvalues\nkeys\n\n\n\n\n1\nwwa1gp\n0.Never\nwwa1gp0.Never\n\n\n2\nwwa1gp\n1.Rarely\nwwa1gp1.Rarely\n\n\n3\nwwa1gp\n2.Sometimes\nwwa1gp2.Sometimes\n\n\n4\nwwa1gp\n3.Often\nwwa1gp3.Often\n\n\n5\nwwa1gp\n4.Always\nwwa1gp4.Always\n\n\n6\nsexgp\nMen\nsexgpMen\n\n\n7\nsexgp\nWomen\nsexgpWomen\n\n\n8\nshiftWork\n0.non shift work\nshiftWork0.non shift work\n\n\n9\nshiftWork\n1.shift work\nshiftWork1.shift work\n\n\n10\nnjob\n0.one-job\nnjob0.one-job\n\n\n11\nnjob\n1.njob\nnjob1.njob\n\n\n12\nAGE\n\nAGE\n\n\n13\nsatisfaction\n\nsatisfaction\n\n\n\n\n\nodds ratio 를 정리해 보겠습니다.\n\nmm = modsmryf(mod3)\nmm1 = mm%>% \n  data.frame() %>% \n  setNames(c(\"or\", \"ll\", \"ul\", \"pvalue\")) %>%\n  mutate(keys=rownames(mm)) \n\n이상의 것을 합쳐서 하나의 테이블로 만듭니다.\n\nbm0 %>%\n  left_join(mm1, by=c(\"keys\")) %>%\n  mutate(OR95CI = case_when(\n    is.na(or) ~ \"1.00 (reference)\", \n    TRUE ~ sprintf(\"%.2f (%.2f-%.2f)\", round(or, 2), round(ll, 2), round(ul, 2))\n  )) %>%\n  select(variables, values, OR95CI, pvalue) %>%\n  mutate(pvalue = ifelse(pvalue <0.001, \"<0.001\", sprintf(\"%.3f\", pvalue))) %>%\n  htmlTable()\n\n\n\n\n\nvariables\nvalues\nOR95CI\npvalue\n\n\n\n\n1\nwwa1gp\n0.Never\n1.00 (reference)\n\n\n\n2\nwwa1gp\n1.Rarely\n1.01 (0.91-1.13)\n0.828\n\n\n3\nwwa1gp\n2.Sometimes\n1.88 (1.69-2.08)\n<0.001\n\n\n4\nwwa1gp\n3.Often\n3.49 (3.11-3.91)\n<0.001\n\n\n5\nwwa1gp\n4.Always\n7.05 (5.96-8.33)\n<0.001\n\n\n6\nsexgp\nMen\n1.00 (reference)\n\n\n\n7\nsexgp\nWomen\n1.54 (1.43-1.66)\n<0.001\n\n\n8\nshiftWork\n0.non shift work\n1.00 (reference)\n\n\n\n9\nshiftWork\n1.shift work\n1.51 (1.32-1.72)\n<0.001\n\n\n10\nnjob\n0.one-job\n1.00 (reference)\n\n\n\n11\nnjob\n1.njob\n1.59 (1.15-2.19)\n0.005\n\n\n12\nAGE\n\n1.01 (1.01-1.02)\n<0.001\n\n\n13\nsatisfaction\n\n2.27 (2.13-2.42)\n<0.001\n\n\n\n\n\n이제 됬습니다. 이것을 통해 표를 만들면 되겠습니다. 반복 작업이 필요할 수도 있으니, 함수로 만들겠습니다. 다만 회귀분석 모델이 error가 있거나 하는 상황이 있으므로, argumet 값이 missing이 아닌 경우에 함수가 실행되게 하겠습니다. 또한 xlevels가 없는 경우, numeric 함수가 없을 경우에도 작동할 수 있도록, 아래와 같이 if else 를 사용해서 함수를 만듭니다. 또한 p value가 0.05보다 작은 경우 bold 로 표시하기 위해 html 언어인 <b> ... </b> 를 사용하고, reference인 경우는 <i>...</i>를 이용해서 italic 폰트를 구성하였습니다.\n\noddf=function(a){\nif(!missing(a)){  \nmm = modsmryf(a)\nmm1 = mm%>% \n  data.frame() %>% \n  setNames(c(\"or\", \"ll\", \"ul\", \"pvalue\")) %>%\n  mutate(keys=rownames(mm)) \nif(!any(is.na(a$xlevels))){\n  t1 = a$xlevels\n  bm1 = map(1:length(t1),function(x){tibble(variables= names(t1)[x], values = t1[[x]])}) %>% do.call(rbind, .)\n} else {\n  t1 = data.frame();bm1=data.frame()\n}\nif(nrow(a$model %>% select(where(is.numeric))%>% unique()) >0){\n    bm2 = a$model %>% slice(1:2)%>%select(where(is.numeric))%>% pivot_longer(-c()) %>% select(variables = name) %>% mutate(values=\"\") %>% unique()\n} else {\n    bm2 = data.frame()  \n}\nbm0 = rbind(bm1, bm2) %>% mutate(keys= paste0(variables, values))\n\natab= bm0 %>% \n  left_join(mm1, by=c(\"keys\")) %>%\n  mutate(OR95CI = case_when(\n    is.na(or) ~ \"<i>1.00 (reference)</i>\", \n    pvalue < 0.05 ~ sprintf(\"<b>%.2f (%.2f-%.2f)</b>\", round(or, 2), round(ll, 2), round(ul, 2)), \n    TRUE ~ sprintf(\"%.2f (%.2f-%.2f)\", round(or, 2), round(ll, 2), round(ul, 2))\n  )) %>%\n  mutate(values = case_when(\n    pvalue <0.05 ~ sprintf(\"<b>%s</b>\", values), \n    TRUE ~ values\n  )) %>%\n  select(variables, values, OR95CI) \n  return(atab)\n} else {\n  atab = data.frame(\"variables\"=c(NA), \"values\"=c(NA), \"OR95CI\"=c(NA))\n  return(atab)\n}\n} %>% suppressWarnings() \n\n잘 작동하는 지 살펴 보겠습니다 .\n\noddf(mod1) %>% htmlTable()\n\n\n\n\n\nvariables\nvalues\nOR95CI\n\n\n\n\n1\nwwa1gp\n0.Never\n1.00 (reference)\n\n\n2\nwwa1gp\n1.Rarely\n1.04 (0.93-1.15)\n\n\n3\nwwa1gp\n2.Sometimes\n1.92 (1.74-2.13)\n\n\n4\nwwa1gp\n3.Often\n3.71 (3.32-4.14)\n\n\n5\nwwa1gp\n4.Always\n7.67 (6.52-9.01)\n\n\n\n\n\n\n\n7.2.3 로지스틱 회귀분석 3\n그런데 만약 model을 여러개를 한번에 나타내려면 어떻게 하면될까요? 네 반복문을 사용해서 list를 만들고, 옆으로 붙이면 됩니다. reduce해서 cbind하면됩니다. \n\n여러 모델 표 구성하기\n\n표 반복해서 만들기\nlist 횡 병합하기 (join)\n모델 갯수 만큼 모델명 만들기\n\n\n\noddsf= function(...){\n  arglist = list(...)\n    #mod_list = mget(ls()) %>%\n    #      list.filter(length(.)>1)\n  tt = map(arglist, oddf) %>%\n      reduce(full_join, by=c(\"variables\", \"values\"))\n  vl = c(length(tt)-2)\n  tt = tt %>% setNames(c(\"Variables\", \"Values\", paste0(\"Model.\", as.roman(1:vl))))\n  return(tt)\n}\n\n잘 작동하는지 보겠습니다.\n\noddsf(mod1, mod2) %>% htmlTable()\n\n\n\n\n\nVariables\nValues\nModel.I\nModel.II\n\n\n\n\n1\nwwa1gp\n0.Never\n1.00 (reference)\n1.00 (reference)\n\n\n2\nwwa1gp\n1.Rarely\n1.04 (0.93-1.15)\n1.00 (0.90-1.12)\n\n\n3\nwwa1gp\n2.Sometimes\n1.92 (1.74-2.13)\n1.83 (1.65-2.03)\n\n\n4\nwwa1gp\n3.Often\n3.71 (3.32-4.14)\n3.42 (3.05-3.83)\n\n\n5\nwwa1gp\n4.Always\n7.67 (6.52-9.01)\n6.91 (5.85-8.16)\n\n\n6\nsexgp\nMen\n\n1.00 (reference)\n\n\n7\nsexgp\nWomen\n\n1.53 (1.42-1.65)\n\n\n8\nAGE\n\n\n1.01 (1.01-1.02)\n\n\n9\nsatisfaction\n\n\n2.29 (2.15-2.44)\n\n\n\n\n\noddsf 함수 만으로는 조금 부족해 보입니다. 이제 이것을 좀더 보기 편하게 꾸며 보겠습니다. \n\n표 꾸미기\n\n표 이름 만들기\n중복 항목 지우기\nNA 값을 빈칸으로 만들기\n\n\n\noddsTabf = function(...){\n  arglist = list(...)\n  mod1 = arglist[[1]]\n  tt = map(arglist, oddf) %>%\n    reduce(full_join, by=c(\"variables\", \"values\"))\n  vl = c(length(tt)-2)\n  ys =  mod1$formula[2] %>% as.character() %>% str_replace(., \"\\\\=\\\\=\", \"being reference of\") %>%\n    str_replace_all(., '\\\\\"', \"\")\n  tt = tt %>% setNames(c(\"Variables\", \"Values\", paste0(\"Model.\", as.roman(1:vl))))\n  tt %>%  `rownames<-`(NULL) %>%\n    group_by(Variables) %>%\n    mutate(rank = row_number()) %>%\n    mutate(Variables = ifelse(rank == min(rank), Variables, \"\")) %>%\n    mutate_at(., vars(starts_with(\"Model\")), ~replace(., is.na(.), \"\")) %>%  \n    ungroup() %>% select(-rank) %>%\n    addHtmlTableStyle(align = 'll') %>%\n    htmlTable(\n      caption = sprintf(\"Table. OR(95%%CI) for %s\", ys)\n      \n    )\n  \n}\n\n잘 작동하는지 알아보겠습니다.\n\noddsTabf(mod1, mod2, mod3)\n\n\n\n\n\nTable. OR(95%CI) for sleepgp being reference of 1.sleep disturbance\n\nVariables\nValues\nModel.I\nModel.II\nModel.III\n\n\n\n\n1\nwwa1gp\n0.Never\n1.00 (reference)\n1.00 (reference)\n1.00 (reference)\n\n\n2\n\n1.Rarely\n1.04 (0.93-1.15)\n1.00 (0.90-1.12)\n1.01 (0.91-1.13)\n\n\n3\n\n2.Sometimes\n1.92 (1.74-2.13)\n1.83 (1.65-2.03)\n1.88 (1.69-2.08)\n\n\n4\n\n3.Often\n3.71 (3.32-4.14)\n3.42 (3.05-3.83)\n3.49 (3.11-3.91)\n\n\n5\n\n4.Always\n7.67 (6.52-9.01)\n6.91 (5.85-8.16)\n7.05 (5.96-8.33)\n\n\n6\nsexgp\nMen\n\n1.00 (reference)\n1.00 (reference)\n\n\n7\n\nWomen\n\n1.53 (1.42-1.65)\n1.54 (1.43-1.66)\n\n\n8\nAGE\n\n\n1.01 (1.01-1.02)\n1.01 (1.01-1.02)\n\n\n9\nsatisfaction\n\n\n2.29 (2.15-2.44)\n2.27 (2.13-2.42)\n\n\n10\nshiftWork\n0.non shift work\n\n\n1.00 (reference)\n\n\n11\n\n1.shift work\n\n\n1.51 (1.32-1.72)\n\n\n12\nnjob\n0.one-job\n\n\n1.00 (reference)\n\n\n13\n\n1.njob\n\n\n1.59 (1.15-2.19)\n\n\n\n\n\n이 것을 source 파일로 저장하겠습니다.\n\nsource(\"source/oddsTabf.R\")\n\n수고하셨습니다."
  },
  {
    "objectID": "250_smry_for_table.html#install-package-from-github",
    "href": "250_smry_for_table.html#install-package-from-github",
    "title": "8  보건학표 만들기 요약",
    "section": "8.1 install package from github",
    "text": "8.1 install package from github\ntidyvere and htmlTable, and broom 가 데이터를 변환하고 표시하는데 자주 사용됩니다. devtools 은 “github”에 있는 함수를 불러오는데 사용됩니다.  install_github 을 통해 패키지를 설치하겠습니다.\n\nrm(list=ls())\n#basic requirment\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"htmlTable\")) install.packages(\"htmlTable\")\nif(!require(\"broom\")) install.packages(\"broom\")\nif(!require(\"labelled\")) install.packages(\"labelled\")\n# packages from github\nif(!require(\"devtools\")) install.packages(\"devtools\")\nlibrary(devtools)\ninstall_github(\"jinhaslab/tabf\", force = TRUE, quiet = TRUE)\nlibrary(tabf)"
  },
  {
    "objectID": "250_smry_for_table.html#데이터-준비",
    "href": "250_smry_for_table.html#데이터-준비",
    "title": "8  보건학표 만들기 요약",
    "section": "8.2 데이터 준비",
    "text": "8.2 데이터 준비\n데이터 표를 만드는 실습은 6차 근로환경조사 자료를 통해 실습할 것입니다.. 자료는 안전보건공단, 근로환경조사 원시자료 사이트 (http://kosha.or.kr/kosha/data/primitiveData.do) 에서 신청할 수 있습니다..\n데이터 표를 만드는 실습은 6차 근로환경조사 자료를 통해 실습할 것입니다.. 자료는 안전보건공단, 근로환경조사 원시자료 사이트 (http://kosha.or.kr/kosha/data/primitiveData.do) 에서 신청할 수 있습니다.. 데이터를 불러오겠습니다. 안전보건공단 홈페이에서 자료를 다운 받는게 원칙입니다. 다만 실습을 빠르게 진행하기 위해서, dspubs.org 페이지에 있는 파일을 이용하겠습니다.   kwcsData6th.rds   자신의 folder에 data 라는 folder가 있는지 확인하십시오. data라는 폴더에 다운로드하고, 불러오도록 하겠습니다.\n\nurl <- \"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/kwcsData6th.rds\"\ndownload.file(url, \"data/tutorKWCS.rds\")\nkwcs = readRDS(\"data/tutorKWCS.rds\")\n\n\n8.2.1 근로환경조사 기본 변수 생성 (선택)\ndata manipulation에서 실습한 내용을 통해 변수를 실습을 위한 변수를 생성해 보겠습니다. 이에 대한 자세한 과정은 이전 챔터에 있습니다. \nTable 1 변수 생성과정 챕터\n\n# data manip function\nLikert5f = function(x){dplyr::recode(as.numeric(x), \n         \"4.Always\",\"3.Often\",\"2.Sometimes\", \"1.Rarely\",\"0.Never\" )}\n\n# data step start ----------\n\ndat1 = kwcs %>%\n  filter(AGE <70) %>%\n  filter(AGE >18) %>%\n  # sleep --------\n  filter(!is.na(sleep1)&!is.na(sleep2)&!is.na(sleep3)) %>%\n  filter(sleep1 %in% c(1:5), \n         sleep2 %in% c(1:5), \n         sleep3 %in% c(1:5)) %>%\n  mutate(sleep1in = 5-sleep1, \n         sleep2in = 5-sleep2,\n         sleep3in = 5-sleep3\n         ) %>%\n  mutate(sleepgp = case_when(\n    sleep1in + sleep2in+ sleep3in >=6 ~ \"1.sleep disturbance\", \n    TRUE ~ \"0.non distrubance\"\n  )) %>%\n  # work live balances -------------\n  filter(!is.na(wbalance), !is.na(wwa1), !is.na(wwa2), !is.na(wwa3), !is.na(wwa4), !is.na(wwa5)) %>%\n  filter(!is.na(wbalance)) %>%\n  mutate(wbalancegp = case_when(\n    wbalance %in% c(1, 2) ~ \"0.balance\", \n    TRUE ~ \"1.non balance\"\n  )) %>%\n  mutate(wwa1gp=Likert5f(wwa1), wwa2gp=Likert5f(wwa2),  wwa3gp=Likert5f(wwa3), \n         wwa4gp=Likert5f(wwa4), wwa5gp=Likert5f(wwa5), \n         ) %>%\n  # job and sex, agegp  ----------\n  filter(!is.na(job1))%>%\n  filter(job1 %in% c(1, 2, 3)) %>%\n  mutate(sexgp = case_when(\n    TSEX ==1 ~ \"Men\", \n    TRUE ~ \"Women\"\n  )) %>%\n  mutate(agegp = case_when(AGE <25 ~ \"<25\", AGE <30 ~ \"<30\",  AGE <35 ~ \"<35\", \n    AGE <40 ~ \"<40\", AGE <45 ~ \"<45\", AGE <50 ~ \"<50\",AGE <55 ~ \"<55\", AGE <60 ~ \"<60\",\n    TRUE ~ \"\\u226560\" # 나머지는 모두 >65 (\\u2265는 크거나 같다는 symbol)\n  )) %>%\n  filter(!is.na(edu)) %>% filter(edu %in% c(1:7)) %>%\n  mutate(edugp = case_when(\n    edu %in% c(1:3) ~    \"2.middle school or below\",\n    edu %in% c(4  ) ~    \"1.high school\",\n    edu %in% c(5:7)   ~  \"0.university or more\"\n  )) %>%\n  mutate(njob=case_when(\n    job1 %in% c(2, 3) ~ \"1.njob\", \n    TRUE ~ \"0.one-job\")) %>%\n  # pains back -----------\n  mutate(backpain= case_when(\n    heal_prob1==1 ~ \"pain\", \n    TRUE ~ \"no-pain\")) %>%\n  # emp_type,working hours, shiftwork, work life balance -------\n  filter(!is.na(emp_type)) %>%\n  filter(emp_type %in% c(1:4)) %>%\n  mutate(empgp = case_when(\n    emp_type ==1 ~ \"2.own-account worker\", \n    emp_type ==2 ~ \"1.employer/self-employer\", \n    emp_type ==3 ~ \"0.paid-worker\", \n    emp_type ==4 ~ \"3.unpaind family work\"\n  )) %>%\n  filter(!is.na(wtime_week)) %>%\n  mutate(whgp=case_when(\n    wtime_week < 35 ~ \"<35\", \n    wtime_week < 40 ~ \"<40\", \n    wtime_week < 52 ~ \"<52\", \n    wtime_week < 60 ~ \"<60\", \n    TRUE ~ \"\\u226560\", \n  )) %>%\n  filter(!is.na(wtime_length5)) %>%\n  filter(wtime_length5 %in% c(1, 2)) %>%\n  mutate(shiftWork = case_when(\n    wtime_length5 ==1 ~ \"1.shift work\", \n    TRUE ~ \"0.non shift work\"\n  )) %>%\n  filter(!is.na(wtime_resilience)) %>%\n  mutate(shortReturn = case_when(\n    wtime_resilience ==1 ~ \"1.short return\", \n    TRUE ~ \"0.non short return\"\n  )) %>%\n  filter(!is.na(satisfaction)) %>%\n  filter(satisfaction %in% c(1:4)) %>%\n  mutate(satisInv = 5-satisfaction) %>%\n  mutate(shiftShort=case_when(\n    shiftWork == \"1.shift work\" & shortReturn == \"1.short return\" ~ \"3.shift work with short return\", \n    shiftWork == \"1.shift work\" & shortReturn != \"1.short return\" ~ \"2.shift work\", \n    shiftWork != \"1.shift work\" & shortReturn == \"1.short return\" ~ \"1.day work with short return\", \n    shiftWork != \"1.shift work\" & shortReturn != \"1.short return\" ~ \"0.day work\", \n  )) \n\n코드를 간단히 하기 위해 dat1을 저장하겠습니다.\n\nsaveRDS(dat1, \"data/kwcsData1.rds\")\n\n\ndat1 = readRDS(\"data/kwcsData1.rds\")"
  },
  {
    "objectID": "250_smry_for_table.html#변수-선정-그리고-표-1",
    "href": "250_smry_for_table.html#변수-선정-그리고-표-1",
    "title": "8  보건학표 만들기 요약",
    "section": "8.3 변수 선정 그리고 표 1",
    "text": "8.3 변수 선정 그리고 표 1\n변수는 종속변수, 독립변수로 나눌수 있고, 독립변수는 성격에 따라 연속변수와 명목변수로 나눌 수 있습니다. 그렇게 구별해 보겠습니다.\n\nstratas  = c(\"sleepgp\")\ncatVars = c(\"wwa1gp\", \"shortReturn\",\"shiftWork\" , \"njob\", \"sexgp\",  \"edugp\", \"empgp\")\nconVars = c(\"AGE\",\"satisfaction\")\n\ntabf() 은 우리가 설정한 것에 따라 표1을 만들어 줍니다. htmlTable()은 붙여 넣기 좋게 화면에 출력해 줍니다.\n\ntab1 = tabf(dat1=dat1, stratas = stratas, catVars = catVars, conVars = conVars)\ntab1\n\n# A tibble: 22 × 5\n   variables      values       `0.non distrubance` `1.sleep disturbance` p.value\n   <chr>          <chr>        <chr>               <chr>                 <chr>  \n 1 \"AGE\"          \"\"           46.8±12.4           49.7±11.9             \"<0.00…\n 2 \"wwa1gp\"       \"Never\"      12222 (94.7%)       684 (5.3%)            \"<0.00…\n 3 \"\"             \"Rarely\"     12316 (94.5%)       714 (5.5%)            \"\"     \n 4 \"\"             \"Sometimes\"  9112 (90.3%)        981 (9.7%)            \"\"     \n 5 \"\"             \"Often\"      3456 (82.8%)        717 (17.2%)           \"\"     \n 6 \"\"             \"Always\"     634 (70.0%)         272 (30.0%)           \"\"     \n 7 \"satisfaction\" \"\"           2.1±0.5             2.4±0.6               \"<0.00…\n 8 \"shortReturn\"  \"non short … 36183 (92.5%)       2927 (7.5%)           \"<0.00…\n 9 \"\"             \"short retu… 1557 (77.9%)        441 (22.1%)           \"\"     \n10 \"shiftWork\"    \"non shift … 35056 (91.9%)       3073 (8.1%)           \"<0.00…\n# ℹ 12 more rows\n\n\nhtmlTable()을 이용해서 보기 좋게 만들어 보겠습니다.\n\ntab1 %>% \n  setNames(c(\"\", \"\", \"None\", \"Disturbance\", \"P value\")) %>%\n  htmlTable(\n    cgroup = c(\"\",  \"Sleep disturbance\", \"\"), \n    n.cgroup = c(2, 2, 1), \n    tfoot = \"P value calculated by Chisq-Test and T-Test\", \n    rnames = FALSE, \n    caption = \"Basic Characteristics according to Sleep disturbance\"\n  ) \n\n\n\n\n\nBasic Characteristics according to Sleep disturbance\n\n \nSleep disturbance \n\n\n\n\n \n \nNone\nDisturbance \n \nP value\n\n\n\n\nAGE\n \n \n46.8±12.4\n49.7±11.9 \n \n<0.001\n\n\nwwa1gp\nNever \n \n12222 (94.7%)\n684 (5.3%) \n \n<0.001\n\n\n\nRarely \n \n12316 (94.5%)\n714 (5.5%) \n \n\n\n\n\nSometimes \n \n9112 (90.3%)\n981 (9.7%) \n \n\n\n\n\nOften \n \n3456 (82.8%)\n717 (17.2%) \n \n\n\n\n\nAlways \n \n634 (70.0%)\n272 (30.0%) \n \n\n\n\nsatisfaction\n \n \n2.1±0.5\n2.4±0.6 \n \n<0.001\n\n\nshortReturn\nnon short return \n \n36183 (92.5%)\n2927 (7.5%) \n \n<0.001\n\n\n\nshort return \n \n1557 (77.9%)\n441 (22.1%) \n \n\n\n\nshiftWork\nnon shift work \n \n35056 (91.9%)\n3073 (8.1%) \n \n<0.001\n\n\n\nshift work \n \n2684 (90.1%)\n295 (9.9%) \n \n\n\n\nnjob\none-job \n \n37471 (91.9%)\n3317 (8.1%) \n \n<0.001\n\n\n\nnjob \n \n269 (84.1%)\n51 (15.9%) \n \n\n\n\nsexgp\nMen \n \n17892 (93.1%)\n1327 (6.9%) \n \n<0.001\n\n\n\nWomen \n \n19848 (90.7%)\n2041 (9.3%) \n \n\n\n\nedugp\nuniversity or more \n \n19597 (92.9%)\n1502 (7.1%) \n \n<0.001\n\n\n\nhigh school \n \n14943 (91.9%)\n1318 (8.1%) \n \n\n\n\n\nmiddle school or below \n \n3200 (85.4%)\n548 (14.6%) \n \n\n\n\nempgp\npaid-worker \n \n25786 (92.4%)\n2122 (7.6%) \n \n<0.001\n\n\n\nemployer/self-employer \n \n2539 (91.7%)\n229 (8.3%) \n \n\n\n\n\nown-account worker \n \n8359 (90.5%)\n880 (9.5%) \n \n\n\n\n\nunpaind family work \n \n1056 (88.5%)\n137 (11.5%) \n \n\n\n\n\nP value calculated by Chisq-Test and T-Test"
  },
  {
    "objectID": "250_smry_for_table.html#로지스틱-회귀-분석",
    "href": "250_smry_for_table.html#로지스틱-회귀-분석",
    "title": "8  보건학표 만들기 요약",
    "section": "8.4 로지스틱 회귀 분석",
    "text": "8.4 로지스틱 회귀 분석\n로지스틱 회귀 분석은 오즈비와 95% 신뢰구간을 구하는데 사용합니다. 여기서는 수면장애의 오즈를 “일을 하지 않을 때에도 걱정하는” 정도에 따라 분석해 보겠습니다. 첫번째 모델을 걱정 변수만 넣어서, 두번째 모델은 연령, 성별, 근무 만족도를 넣어서, 3번째는 교대 근무와, n-job 여부를 넣어서 분석하겠습니다.\n\nmod1 = dat1 %>%\n  glm(data=.,family=\"binomial\",formula = sleepgp == \"1.sleep disturbance\"  \n      ~ wwa1gp)\nmod2 = dat1 %>%\n  glm(data=.,family=\"binomial\",formula = sleepgp == \"1.sleep disturbance\"\n      ~ wwa1gp + AGE + sexgp +satisfaction)\nmod3 = dat1 %>%\n  glm(data=.,family=\"binomial\",formula = sleepgp == \"1.sleep disturbance\"\n      ~ wwa1gp + AGE + sexgp +satisfaction + shiftWork + njob)\n\n각 모델이 잘 이루어졌는지 확인해 봅니다 . 로지스틱 회귀분서의 자세한 내용은 통계학 강의를 꼭 참고해 주세요!.\n\nsummary(mod1)\n\n\nCall:\nglm(formula = sleepgp == \"1.sleep disturbance\" ~ wwa1gp, family = \"binomial\", \n    data = .)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.8450  -0.4522  -0.3357  -0.3300   2.4238  \n\nCoefficients:\n                  Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -2.88303    0.03929 -73.378   <2e-16 ***\nwwa1gp1.Rarely     0.03526    0.05500   0.641    0.521    \nwwa1gp2.Sometimes  0.65426    0.05170  12.655   <2e-16 ***\nwwa1gp3.Often      1.31024    0.05681  23.062   <2e-16 ***\nwwa1gp4.Always     2.03679    0.08245  24.704   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 23305  on 41107  degrees of freedom\nResidual deviance: 22258  on 41103  degrees of freedom\nAIC: 22268\n\nNumber of Fisher Scoring iterations: 5\n\n\noddsf() 라는 함수로 결과를 확인해 보겠습니다 .\n\noddsf(mod1, mod2, mod3) %>% htmlTable()\n\n\n\n\n\nVariables\nValues\nModel.I\nModel.II\nModel.III\n\n\n\n\n1\nwwa1gp\n0.Never\n1.00 (reference)\n1.00 (reference)\n1.00 (reference)\n\n\n2\nwwa1gp\n1.Rarely\n1.04 (0.93-1.15)\n1.00 (0.90-1.12)\n1.01 (0.91-1.13)\n\n\n3\nwwa1gp\n2.Sometimes\n1.92 (1.74-2.13)\n1.83 (1.65-2.03)\n1.88 (1.69-2.08)\n\n\n4\nwwa1gp\n3.Often\n3.71 (3.32-4.14)\n3.42 (3.05-3.83)\n3.49 (3.11-3.91)\n\n\n5\nwwa1gp\n4.Always\n7.67 (6.52-9.01)\n6.91 (5.85-8.16)\n7.05 (5.96-8.33)\n\n\n6\nsexgp\nMen\n\n1.00 (reference)\n1.00 (reference)\n\n\n7\nsexgp\nWomen\n\n1.53 (1.42-1.65)\n1.54 (1.43-1.66)\n\n\n8\nAGE\n\n\n1.01 (1.01-1.02)\n1.01 (1.01-1.02)\n\n\n9\nsatisfaction\n\n\n2.29 (2.15-2.44)\n2.27 (2.13-2.42)\n\n\n10\nshiftWork\n0.non shift work\n\n\n1.00 (reference)\n\n\n11\nshiftWork\n1.shift work\n\n\n1.51 (1.32-1.72)\n\n\n12\nnjob\n0.one-job\n\n\n1.00 (reference)\n\n\n13\nnjob\n1.njob\n\n\n1.59 (1.15-2.19)\n\n\n\n\n\n예상한대로 결과가 나왔다면 oddsTabf()를 이용해서 복사 붙여 넣기를 하겠습니다.\n\noddsTabf(mod1, mod2, mod3)"
  },
  {
    "objectID": "310_ggplot_basic.html#ggplot-basic",
    "href": "310_ggplot_basic.html#ggplot-basic",
    "title": "9  기본 플롯 시각화 (ggplot2)",
    "section": "9.1 ggplot basic",
    "text": "9.1 ggplot basic\nBefore drawing basic tables and graphs, let’s take a look at the most used package, ggplot2. Let’s practice ggplot2 using the built-in data iris.\n\nlibrary(tidyverse)\ndata(iris)\n\nThe main syntax for ggplot2 are included by axis and layer. Let’s Take It from the beginning.\n\n9.1.1 axis\n2-denominational plot needs 3 components of data, axis,layer. First we draw x-axis and y-axis, and putting information into that using layer.\n\niris %>% # data what we used\n  ggplot(aes(x = Sepal.Length, y = Sepal.Width))\n\n\n\n\nIn this way, the X and Y axes were created. Let’s change the axis names to xlab, ylab and use ggtitle to change the title.\n\niris %>% \n  ggplot(aes(x = Sepal.Length, y = Sepal.Width)) +\n  xlab(\"sepal length\") + ylab(\"sepal width\") +\n  ggtitle(\"Iris flow and its characteristics\")\n\n\n\n\n\n\n9.1.2 layer, geom_x()\nLayers are what you want to put inside the frame. geom_x() format. _x() can be points, lines, smoothing, histograms, densities, boxplots, and bars.\n\n\n\ngeom_x()\ncontents\n\n\n\n\ngeom_point()\nscatter plot\n\n\ngeom_line()\nline plot\n\n\ngeom_smooth()\nprediction line\n\n\ngeom_histogram()\nhistogram\n\n\ngeom_density()\ndensity line\n\n\ngeom_boxplot()\nboxplot plot\n\n\ngeom_bar()\nbar chart\n\n\n\n\n\n9.1.3 geom_point()\nThis is the most basic scatterplot. Plot Sepal.Length on the x-axis and Sepal.Width on the y-axis. The layer to use in this case is geom_point().\n\niris %>%\n  ggplot(aes(x=Sepal.Length, y = Sepal.Width)) +\n  geom_point()\n\n\n\n\nYou can put a description here using xlalb and ylab. Create a title using ggtitle(). .\n\niris %>% \n  ggplot(aes(x = Sepal.Length, y = Sepal.Width)) +\n  xlab(\"Sepal length\") + \n  ylab(\"Sepal width\") +\n  ggtitle(\" Type of iris according to sepal lenght and width\") +\n  geom_point()\n\n\n\n\nLet’s go further. There is no interpretation in scatter plots. At this time, we will use different colors for each type of iris. We will create a frame by putting color = Species inside aes( ).\n\niris %>% \n\n  ggplot(aes(x = Sepal.Length, y = Sepal.Width, \n             color = Species)) + \n  xlab(\"Sepal length\") + \n  ylab(\"Sepal width\") +\n  ggtitle(\" Type of iris according to sepal lenght and width\") +\n  geom_point()\n\n\n\n\nHow about it? A little bit better. This time, we will add a trend line geom_smooth() to examine the relationship between sepal length and width by type. Also try adding geom_line() . It is omitted here because it is not logically useful.\n\n\n9.1.4 geom_smooth()\n\niris %>% #\n  ggplot(aes(x = Sepal.Length, y = Sepal.Width, \n             color = Species)) + \n  xlab(\"Sepal length\") + \n  ylab(\"Sepal width\") +\n  ggtitle(\" Type of iris according to sepal lenght and width\") +\n  geom_point() +\n  geom_smooth() \n\n\n\n\nThe following code demonstrated example of adding a linear trend line as well as polynomial line.\n\niris %>%\n  ggplot(aes(x = Sepal.Length, y = Sepal.Width, \n             color = Species)) + \n  xlab(\"Sepal length\") + \n  ylab(\"Sepal width\") +\n  ggtitle(\" Type of iris according to sepal lenght and width\") +\n  geom_point() +\n  geom_smooth(method = 'lm', formula = y ~ poly(x, 5), se = FALSE, linetype = 1) + # how abou se = TRUE\n  geom_smooth(method = 'lm', formula = y ~ x, se = FALSE, linetype = 2)  \n\n\n\n\n\n\n9.1.5 faceting\nDrawing on one screen has advantages, but also has complexities. In this case, faceting is used. facet_wrap()\n\niris %>%\n  ggplot(aes(x = Sepal.Length, y = Sepal.Width, \n             color = Species)) + \n  xlab(\"Sepal length\") + \n  ylab(\"Sepal width\") +\n  ggtitle(\" Type of iris according to sepal lenght and width\") +\n  geom_point() +\n  geom_smooth(method = 'lm', formula = y ~ poly(x, 5), se = FALSE, linetype = 1) +\n  geom_smooth(method = 'lm', formula = y ~ x, se = FALSE, linetype = 2)  +\n  facet_wrap(Species~.)\n\n\n\n\n\n\n9.1.6 geom_bar()\nTo make a barplot of counts, we will use geom_bar().\n\niris %>% \n  ggplot(aes(x = Species,  \n             fill = Species)) +\n  xlab(\"Type of iris\") + \n  ylab(\"Counts\") +\n  ggtitle(\"Number of iris according to its types.\") +\n  geom_bar()\n\n\n\n\nLet’s draw the count number of iris with Sepal.Width equal to or greater than 3. We will use filter(Sepal.Width > 3).\n\niris %>% \n  filter(Sepal.Width >3) %>% \n  ggplot(aes(x = Species,  \n             fill = Species)) +\n  xlab(\"Type of iris\") + \n  ylab(\"Counts\") +\n  ggtitle(\"Number of iris according to its types.\") +\n  geom_bar()\n\n\n\n\ncoord_polar is polar coordinate system for pie chart.\n\niris %>% \n  filter(Sepal.Width >3) %>% \n  ggplot(aes(x = Species,  \n             fill = Species)) +\n  xlab(\"Type of iris\") + \n  ylab(\"Counts\") +\n  ggtitle(\"Number of iris according to its types.\") +\n  geom_bar() +\n  geom_bar(width =1) + coord_polar()\n\n\n\n\n\n\n9.1.7 geom_density() , geom_histogram()\nLet’s plot the distribution of sepal length. I drew histogram and density.\n\niris %>% \n  ggplot(aes(x = Sepal.Length)) + \n  xlab(\"Sepal Length of Iris\") + \n  ylab(\"density\") +\n  ggtitle(\"Distribution of Sepal Length \") +\n  geom_histogram(aes(y = ..density..))+\n  geom_density()\n\n\n\n\nI see heterogeneity. Can you see it. Let’s put fill = Species inside aes() to distinguish them. virginica has broad leaves. How is it different from the previous code of color = Species?\n\niris %>% \n  ggplot(aes(x = Sepal.Length, fill = Species)) + \n  xlab(\"sepal length of iris\") + \n  ylab(\"density\") +\n  ggtitle(\"Distribution of sepal length of iris\") +\n  geom_histogram(aes(y = ..density..), alpha = 0.3)+\n  geom_density(stat=\"density\", alpha = 0.3) +\n  theme_minimal()\n\n\n\n\nOf course, you can try faceting. Is faceting good in this case? Is it good not to have it? It may be depend on your purpose.\n\niris %>% \n  ggplot(aes(x = Sepal.Length, fill = Species)) + \n  xlab(\"sepal length of iris\") + \n  ylab(\"density\") +\n  ggtitle(\"Distribution of sepal length of iris\") +\n  geom_histogram(aes(y = ..density..), alpha = 0.3)+\n  geom_density(stat=\"density\", alpha = 0.3) +\n  theme_minimal() + # my favorit theme\n  facet_wrap(Species~.)\n\n\n\n\n\n\n9.1.8 geom_boxplot()\nLet’s draw a boxplot of the sepal width distribution for each iris flower.\n\niris %>% \n  ggplot(aes(x = Species, y = Sepal.Width, \n             color = Species)) +\n  geom_boxplot()\n\n\n\n\n\n\n9.1.9 3d plot\nIn this practice, it seems that the types of iris can be distinguished according to the width and length of the sepals. plot_ly is better than ggplot in interactive part and 3d part. Let’s do it.\n\nlibrary(plotly)\niris %>%\nplot_ly( \n        x = ~Sepal.Length, y = ~Petal.Length, z = ~Petal.Width,  \n        color = ~Species,  # Color separation by Species. \n        type = \"scatter3d\",   # 3d plot\n        alpha = 0.8\n        ) %>%  \n        layout(\n               scene = list(xaxis = list(title = 'Sepal Length'), \n                            yaxis = list(title = 'Petal Length'),\n                            zaxis = list(title = 'Petal Width')))"
  },
  {
    "objectID": "310_ggplot_basic.html#visualzation-example-1",
    "href": "310_ggplot_basic.html#visualzation-example-1",
    "title": "9  기본 플롯 시각화 (ggplot2)",
    "section": "9.2 Visualzation example 1",
    "text": "9.2 Visualzation example 1\n\n9.2.1 simple machine learning decision tree\nDo you think that it is possible to make a decision tree depending on the length and width of the iris? Now that we’ve come this far, let’s just take a look at the flow using simple example. Approximately, decision trees are machine learning methods used for classification. The models aim to predict the value of Y by learning simple decision steps. There are importance weight to make each step of decision. So, it can be visualize bar chart.\n\n9.2.1.1 Divide the data into training and test by 7:3.\nI will prepare two data set, one is for training, the other is for testing. The proportion of training set is 70% from original data.\n\nif(!require(\"caret\")) install.packages(\"caret\")\nlibrary(caret) # \ndata(iris)\nset.seed(2020)\ntrain_index <- createDataPartition(\n           y= iris$Species, \n           p = .7,  \n           list = FALSE,\n           times = 1)  \ntrain_data <- iris[ train_index,]  \ntest_data  <- iris[-train_index,]  \n\n\n\n9.2.1.2 10 fold cross validation\nCross-validation is a resampling method for limited data set. I will use 10 fold cross validation.\n\nfitControl <- trainControl(method = \"cv\", # cross validation\n                           number = 10    ) # 10 times\n\n\n\n9.2.1.3 machine learning model\ncheck accuracy and model performance using confusion matrix. What is average accuracy for model performance.\n\nset.seed(2020)\nDTFit1 <- train(data = train_data, #\n                \n                Species ~ ., # . means all remain variable\n                method = 'rpart', # https://topepo.github.io/caret/available-models.html \n               trControl = fitControl) #  cross validation\nconfusionMatrix(DTFit1) # \n\nCross-Validated (10 fold) Confusion Matrix \n\n(entries are percentual average cell counts across resamples)\n \n            Reference\nPrediction   setosa versicolor virginica\n  setosa       33.3        0.0       0.0\n  versicolor    0.0       31.4       1.9\n  virginica     0.0        1.9      31.4\n                            \n Accuracy (average) : 0.9619\n\n\nOur goal is visualization, what kind of plot are needed? The basic bar plot is great choice.\n\n\n9.2.1.4 barplot for importance\nThe importance weight differs among variables. Let’s draw importance using bar plot.\n\nfit1_imp <- varImp(DTFit1)\nfit1_imp %>% \nggplot(mapping = aes(x = Overall)) + \n  geom_boxplot() + \n  labs(title = \"Importance\") \n\n\n\n\nWhat is most important feature for classification of iris. Petal? Sepal?\nrpart package gives us nice plot for decision tree. It can be used for data explorer\n\nlibrary(rpart)\nlibrary(rattle)\nfancyRpartPlot(DTFit1$finalModel)\n\n\n\n\nTo check this classification steps, we also use 3d plot, as we discussed."
  },
  {
    "objectID": "310_ggplot_basic.html#summary",
    "href": "310_ggplot_basic.html#summary",
    "title": "9  기본 플롯 시각화 (ggplot2)",
    "section": "9.3 summary",
    "text": "9.3 summary\n\nbasic visualization\n\naxis and layer\n\ntry your idea using different layer\n\nexplore data get idea\ndiscussion with friend\ntry your predict model\nvisualization your model output"
  },
  {
    "objectID": "320_kwcs_tutor_plot.html#install-package-from-github",
    "href": "320_kwcs_tutor_plot.html#install-package-from-github",
    "title": "10  근로환경조사를 이용한 시각화 실습",
    "section": "10.1 install package from github",
    "text": "10.1 install package from github\ntidyvere and htmlTable, and broom 가 데이터를 변환하고 표시하는데 자주 사용됩니다. devtools 은 “github”에 있는 함수를 불러오는데 사용됩니다.  install_github 을 통해 패키지를 설치하겠습니다.\n\nrm(list=ls())\n#basic requirment\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"htmlTable\")) install.packages(\"htmlTable\")\nif(!require(\"broom\")) install.packages(\"broom\")\nif(!require(\"labelled\")) install.packages(\"labelled\")\n# packages from github\nif(!require(\"devtools\")) install.packages(\"devtools\")\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif(!require(\"plot3D\")) install.packages(\"plot3D\")\nlibrary(devtools)\nif(!require(\"tabf\")) install_github(\"jinhaslab/tabf\", force = TRUE, quiet = TRUE)\nlibrary(tabf)"
  },
  {
    "objectID": "320_kwcs_tutor_plot.html#데이터-준비",
    "href": "320_kwcs_tutor_plot.html#데이터-준비",
    "title": "10  근로환경조사를 이용한 시각화 실습",
    "section": "10.2 데이터 준비",
    "text": "10.2 데이터 준비\n  kwcs5th.rds   자신의 folder에 data 라는 folder가 있는지 확인하십시오. data라는 폴더에 다운로드하고, 불러오도록 하겠습니다.\n\nurl.data <- \"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/kwcs5th.rds\"\ndownload.file(url.data, \"data/kwcs5th.rds\")\n\n\na = readRDS(\"data/kwcs5th.rds\")\n\n데이터에서 필요한 자료를 불러오고 변환하겠습니다.   kwcs5th 설문지  \n\n# 근로환경조사 5차\n# 변수생성: 온콜 여부/빈도, 우울, 연령, 성별, 교육, 근무시간, 종사상지위, 고용형태, 소득#\na0<-a %>%\n    select(Q35, AGE, TSEX, TEF1, Q22_1, Q05, Q06, EF11, EF12, Q69,Q62_1_8, Q35, # 기존 실습\n           Q16_1, Q26_7, Q26_8, Q49_15, Q49_2, Q49_1) # 새로 추가된 실습과제 \n# 변수 구획 정하기 ------\n\nWh_breaks <- c(-Inf, 35, 45, 55, 65, Inf)\nWh_labels <- c('<35','35-44','45-54','55-64','>=65')\ninc_break <- c(-Inf, 100, 200, 300, 400, Inf)\ninc_label <- c('<100', '100-199', '200-299', '300-399', '>400')\n\n# 데이터 step ------\na1 <-a0 %>%\n   filter(AGE <65 ) %>% \n   filter(!is.na(Q22_1)) %>% \n   filter(!TEF1    == 9) %>%\n   filter(!Q69     == 9) %>%\n   filter(!Q62_1_8 == 9) %>% \n\n   mutate(oncall    = Q35) %>%\n   mutate(oncallgp  = ifelse(oncall %in% c(1, 2, 3), \"on call\", \"non-on call\")) %>%\n   mutate(oncallgp3 = ifelse(oncall %in% c(1,2,3), \"several times a month\", \n                      ifelse(oncall %in% c(4), \"rarely\", \"none\"))) %>%\n   mutate(oncallgp3 = factor(oncallgp3, \n                         levels=c(\"none\", \"rarely\", \"several times a month\")))%>%\n   mutate(agegp     = ifelse(AGE <30, '<30', \n                             ifelse(AGE <40, '30-49', \n                                    ifelse(AGE <50, '40-49', \n                                           ifelse(AGE < 60, '50-59', '≥60'))))) %>%\n   mutate(Wh=cut(Q22_1, breaks=Wh_breaks, include.lowest=TRUE, right=FALSE, \n                        labels=Wh_labels)) %>% \n   mutate(Wh=structure(Wh, label='Working hours')) %>%\n   mutate(Gender=factor(TSEX, levels=c(1,2), labels=c('Men', 'Women') )) %>%\n   mutate(Education=factor(TEF1, levels=c(1,2,3,4), \n                        labels=(c('Primary', 'Middle', 'High', 'University')))) %>%\n   mutate(Statusw=ifelse(Q05 %in% c(1,2), 'Self employer', \n                         ifelse(Q05 %in% c(3), 'Paid worker', \n                                'Family workers and others'))) %>%\n   mutate(inc1=ifelse(EF11 <10000, EF11, ifelse(EF12<10, EF12*100-50, NA) )) %>%\n   mutate(inc=cut(inc1, breaks=inc_break, labels=inc_label)) %>%\n   mutate(job_st = factor(Q69, levels = c(1:4), \n                  labels = c(\"Very satisfied\", \"Satisfied\", \"Unsatisfied\", \"Very unsatisfied\"))) %>%\n   mutate(job_st = structure(job_st, label = 'Job Satisfaction')) %>%\n   mutate(depression = Q62_1_8)%>%\n   mutate(depression = structure(factor(depression, levels=c(1, 2), \n                                     labels=c(\"Depression\", \"Non depression\"))))"
  },
  {
    "objectID": "320_kwcs_tutor_plot.html#실습-자료-시각화",
    "href": "320_kwcs_tutor_plot.html#실습-자료-시각화",
    "title": "10  근로환경조사를 이용한 시각화 실습",
    "section": "10.3 실습 자료 시각화",
    "text": "10.3 실습 자료 시각화\n이 데이터를 이용해서 다음과 같은 그래프를 그려보려고 합니다. 저번 시간에 oncall 의 횟수가 늘어나면 우울할 오즈비가 증가하는 것을 표로 나타a내었는데요, 그래프를 보면 주 45시간 미만이면서 자영업과 가족종사자에서는 그렇지 않은 반면에, 장시간 근로자에서는 oncall 이 늘어날 수록 우울해 지는 것을 볼 수 있습니다. 왜 그럴까요? 라는 질문이 저절로 나오게 됩니다. 이처럼 그래프를 통해 변화되는 양상을 보여주는 것은 데이터의 속성을 탐구하게 만드는 원동력이 되고는 합니다. 따라서 데이터를 탐구할 때 그래프를 이용해 소통하는 작업을 자주 하는 것이 좋습니다.\n\n\n\n그림1\n\n\n우선 oncallgp3는 명목변수여서 그래프를 그리기 어렵습니다. 따라서 해당하는 연속변수를 만들어서 그래프를 그리겠습니다. 또한 장시간 근무를 45시간을 기준으로 (주 40시간이 기준이고, 하루 한시간 정도 추가 근무를 하면 45시간이 됩니다.)\n\n a2<- a1 %>%\n  mutate(oncall3 = case_when(\n    oncall %in% c(1,2, 3) ~ 3, # several times a month \n    oncall %in% c(4)      ~ 2, # rarely\n    TRUE ~ 1                   # none\n  )) %>%\n  mutate(lwh = ifelse(Q22_1 > 45, 2, 1)) %>%\n  mutate(lwhf = factor(lwh, levels = c(1,2), \n                         labels = c(\"Working hours less than 45\", \n                                    \"Working hours more than 45\")))\n\noncall별 우울감이 얼마나 있는지 계산해 보겠습니다. 우선 전체 4만여명 중에 1006명이 우울감을 호소하고 있습니다.\n\na2 %>%\n  group_by(depression) %>%\n  count() \n\n# A tibble: 2 × 2\n# Groups:   depression [2]\n  depression         n\n  <fct>          <int>\n1 Depression      1006\n2 Non depression 40583\n\n\n그렇다면 group_by()에 oncall3를 넣고 count에 depression을 넣어 온콜에 따른 우울감 분율을 계산해 보겠습니다.\n\na2 %>%\n   group_by(oncall3) %>%\n   count(depression)\n\n# A tibble: 6 × 3\n# Groups:   oncall3 [3]\n  oncall3 depression         n\n    <dbl> <fct>          <int>\n1       1 Depression       773\n2       1 Non depression 33244\n3       2 Depression       164\n4       2 Non depression  5637\n5       3 Depression        69\n6       3 Non depression  1702\n\n\n이때 그래프를 그리기 위해 필요한 것은 Depression은 분율 또는 prevalance입니다. group별 우울감 여부 (n)을 group별 총 명수 (sum(n))으로 나누면 1번 집단은 773/(773+33244)의 크기로 우울감 유병율이 나타나게 됩니다.\n\na2 %>%\n   group_by(oncall3) %>%\n   count(depression) %>%\n   mutate(prob = n/sum(n)*100)\n\n# A tibble: 6 × 4\n# Groups:   oncall3 [3]\n  oncall3 depression         n  prob\n    <dbl> <fct>          <int> <dbl>\n1       1 Depression       773  2.27\n2       1 Non depression 33244 97.7 \n3       2 Depression       164  2.83\n4       2 Non depression  5637 97.2 \n5       3 Depression        69  3.90\n6       3 Non depression  1702 96.1 \n\n\n다만 그래프를 그릴때 사용되는 것은 우울감의 유병율이므로 filter 를 통해 depression 변수의 변수값이 Depression일 때만을 사용하도록 하겠습니다.\n\na2 %>%\n   group_by(oncall3) %>%\n   count(depression) %>%\n   mutate(prob = n/sum(n)*100) %>%\n   filter(depression == 'Depression')\n\n# A tibble: 3 × 4\n# Groups:   oncall3 [3]\n  oncall3 depression     n  prob\n    <dbl> <fct>      <int> <dbl>\n1       1 Depression   773  2.27\n2       2 Depression   164  2.83\n3       3 Depression    69  3.90\n\n\n이제 oncall3의 1, 2, 3을 x-축에, prob(우울감 유병률)을 y-축에 위치시키고 그림을 그리겠습니다. 그림은 ggplot2 라이브러리를 이용하겠습니다. 이것이 가장 기본적인 그래프 그리기입니다. aes안에는 x, y축을 담당하고 데이터의 기본 가정에 해당하는 것을 적어 놓습니다. geom_* 뒤에는 표현 방식에 대한 부분을 넣습니다. geom_line은 선차트, geom_point 산점차트를 그린다는 것입니다.\n\na2 %>%\n   group_by(oncall3) %>%\n   count(depression) %>%\n   mutate(prob = n/sum(n)*100) %>%\n   filter(depression == 'Depression') %>%\n   ggplot(aes(x = oncall3, y = prob)) +\n   geom_point()+\n   geom_line()\n\n\n\n\n우리는 이제 데이터에 내포된 의미를 찾기 위해 고용형태에 따른 oncall과 우울간의 관계를 그려보고자 합니다. 따라서 group_by( )에 고용형태 변수를 넣어 같은 그림을 그려봅니다. 상기 그래프에서 color, shape, linetype을 고용형태에 따라 다르게 해달라고 표신한 부분을 확인해 주세요.\n\na2 %>%\n   group_by(oncall3, Statusw) %>%\n   count(depression) %>%\n   mutate(prob = n/sum(n)*100) %>%\n   filter(depression == 'Depression') %>%\n   ggplot(aes(x = oncall3, y = prob)) +\n   geom_point(aes(color = Statusw, shape = Statusw), size =3) +\n   geom_line(aes(color = Statusw, linetype = Statusw)) \n\n\n\n\n이번에는 그래프를 둘로 나눌 것입니다. 하나는 45시간 이하 근무자, 다른 하나는 45시간 초과 근무자로 나눌 것입니다. 그래프는 facet_wrap()를 이용하고 그 안에 lwhf~.는 근무시간에 따라 나누어 그리라는 뜻입니다. color부분은 중복되니 맨 처음에 group = Statusw 를 color = Statusw로 바꾸겠습니다.\n\na2 %>% group_by(oncall3, Statusw, lwhf) %>%\n  count(depression) %>%\n  mutate(prob = n/sum(n)*100) %>%\n  filter(depression == 'Depression') %>%\n  ggplot(aes(x= oncall3, y = prob, color = Statusw)) +\n  geom_point(aes(shape = Statusw), size =3) +\n  geom_line(aes(linetype = Statusw)) +\n  theme_minimal() +\n  facet_wrap(lwhf~.)\n\n\n\n\nx 축을 none, rarely, several times로 바꾸고, y축은 % 단위로 바꾸며, 두 그래프 사이를 2칸 띄우는 방식을 도입해 그래프를 마무리 하도록 하겠습니다.\n\na2 %>% group_by(oncall3, Statusw, lwhf) %>%\n  count(depression) %>%\n  mutate(prob = n/sum(n)*100) %>%\n  filter(depression == 'Depression') %>%\n  ggplot(aes(x= oncall3, y = prob, color = Statusw)) +\n  geom_point(aes(shape = Statusw), size =3) +\n  geom_line(aes(linetype = Statusw)) +\n  theme_minimal() +\n  facet_wrap(lwhf~.) +\n  labs(color = 'Employment status',\n       shape = 'Employment status',\n       linetype = 'Employment status') +\n  xlab(\"on call works\") + ylab(\"Prevalance of Depressive symptoms\") +\n  scale_x_continuous(breaks = c(1, 2, 3), \n                     labels = c('1' = 'none', \n                                '2' = 'rarely',\n                                '3' = 'several \\ntimes')) +\n  scale_y_continuous(labels = function(x) paste0(x, \"%\"))+\n  theme(panel.spacing = unit(2, \"lines\"))"
  },
  {
    "objectID": "320_kwcs_tutor_plot.html#시각화-과제-1",
    "href": "320_kwcs_tutor_plot.html#시각화-과제-1",
    "title": "10  근로환경조사를 이용한 시각화 실습",
    "section": "10.4 시각화 과제 1",
    "text": "10.4 시각화 과제 1\n이번에는 고객 대면 근로자가 화난 고객을 상대함에 있어서, 상사와 동료의 지지가 어떠한 보호 효과를 보이는지 그래프를 그려보도록 하겠습니다. 저번 실습자료에 이번 과제에서 사용될 변수인 감정노동, 동료의지지, 상사의지지를 추가하겠습니다.\n\n\n\n변수\n설명\n\n\n\n\nQ26_7\n서비스직, 대면 근로자\n\n\nQ26_9\n화난 고객 대응 근로자\n\n\nQ49_15\n일하면서 감정숨김\n\n\nQ49_2\n상사의 지지적 문화\n\n\nQ49_1\n동료의 지지적 문화\n\n\n\n\nem <- a %>%\n   select(Q35, AGE, TSEX, TEF1, Q22_1, Q05, Q06, EF11, EF12, Q69,Q62_1_8, Q35, # 기존 실습\n           Q16_1, Q26_7, Q26_8, Q49_15, Q49_2, Q49_1, KQ50) %>%\n   filter(Q16_1 >=1, Q26_7 <7, Q26_8 <=7, Q49_15 <=3,  \n    KQ50<9, Q49_2 <7, Q49_1<7)  %>%               # 새로 추가된 실습 과제 변수\n   mutate(agegp     = ifelse(AGE <30, '<30', \n                             ifelse(AGE <40, '30-49', \n                                    ifelse(AGE <50, '40-49', \n                                           ifelse(AGE < 60, '50-59', '≥60'))))) %>%\n   mutate(Wh=cut(Q22_1, breaks=Wh_breaks, include.lowest=TRUE, right=FALSE, \n                        labels=Wh_labels)) %>% \n   mutate(Wh=structure(Wh, label='Working hours')) %>%\n   mutate(Gender=factor(TSEX, levels=c(1,2), labels=c('Men', 'Women') )) %>%\n   mutate(Education=factor(TEF1, levels=c(1,2,3,4), \n                        labels=(c('Primary', 'Middle', 'High', 'University')))) %>%\n   mutate(Statusw=ifelse(Q05 %in% c(1,2), 'Self employer', \n                         ifelse(Q05 %in% c(3), 'Paid worker', \n                                'Family workers and others'))) %>%\n   mutate(inc1=ifelse(EF11 <10000, EF11, ifelse(EF12<10, EF12*100-50, NA) )) %>%\n   mutate(inc=cut(inc1, breaks=inc_break, labels=inc_label)) %>%\n   mutate(job_st = factor(Q69, levels = c(1:4), \n                  labels = c(\"Very satisfied\", \"Satisfied\", \"Unsatisfied\", \"Very unsatisfied\"))) %>%\n   mutate(job_st = structure(job_st, label = 'Job Satisfaction')) %>%\n   mutate(depression = Q62_1_8)%>%\n   mutate(depression = structure(factor(depression, levels=c(1, 2), \n                                     labels=c(\"Depression\", \"Non depression\")))) %>%\n  mutate(servieworker=Q26_7) %>%\n  \n  mutate(angergp=ifelse(Q26_8 %in% c(1,2,3), 3, \n                        ifelse(Q26_8 %in% c(4, 5), 2, 1))) %>%\n  mutate(angergp2=ifelse(Q26_8 %in% c(1,2,3), 'always', \n                         ifelse(Q26_8 %in% c(4, 5), 'sometimes', 'rarely'))) %>%\n  mutate(angergp2 = factor (angergp2, levels = c('rarely', 'sometimes', 'always'))) %>%\n  mutate(suppressiongp=ifelse(Q49_15 %in% c(1,2,3), 1, 0)) %>%\n  mutate(suppressiongp2=ifelse(Q49_15 %in% c(1,2), 'always',\n                               ifelse(Q49_15 %in% c(3), 'sometimes', 'rarely'))) %>%\n  mutate(support_sup=ifelse(Q49_2 %in% c(1,2), 'always', \n                            ifelse(Q49_2 %in% c(3), 'sometimes', 'rarely')), \n         support_col=ifelse(Q49_1 %in% c(1,2), 'always', \n                            ifelse(Q49_1 %in% c(3),'sometimes', 'rarely')))%>%\n  mutate(support_sup=factor(support_sup, levels=c('always', 'sometimes', 'rarely')), \n         support_col=factor(support_col, levels=c('always', 'sometimes', 'rarely'))) %>%\n  mutate(support_sup2=ifelse(Q49_2 %in% c(1,2, 3), '> sometimes', 'rarely'), \n         support_col2=ifelse(Q49_1 %in% c(1,2, 3), '> sometimes', 'rarely')) %>%\n  mutate(support_sup2=factor(support_sup2, levels=c('> sometimes', 'rarely')), \n         support_col2=factor(support_col2, levels=c('> sometimes', 'rarely'))) %>%\n  filter(!is.na(depression))\n\n상기 자료를 통해 아래의 그래프를 그리시오"
  },
  {
    "objectID": "350_statistical_plot.html#install-package-from-github",
    "href": "350_statistical_plot.html#install-package-from-github",
    "title": "11  T검정, 로지스틱 회귀분석 플랏",
    "section": "11.1 install package from github",
    "text": "11.1 install package from github\ntidyvere and htmlTable, and broom 가 데이터를 변환하고 표시하는데 자주 사용됩니다. devtools 은 “github”에 있는 함수를 불러오는데 사용됩니다.  install_github 을 통해 패키지를 설치하겠습니다.\n\nrm(list=ls())\n#basic requirment\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"htmlTable\")) install.packages(\"htmlTable\")\nif(!require(\"broom\")) install.packages(\"broom\")\nif(!require(\"labelled\")) install.packages(\"labelled\")\n# packages from github\nif(!require(\"devtools\")) install.packages(\"devtools\")\nlibrary(devtools)\nif(!require(\"tabf\")) install_github(\"jinhaslab/tabf\",  quiet = TRUE)\nlibrary(tabf)"
  },
  {
    "objectID": "350_statistical_plot.html#데이터-준비",
    "href": "350_statistical_plot.html#데이터-준비",
    "title": "11  T검정, 로지스틱 회귀분석 플랏",
    "section": "11.2 데이터 준비",
    "text": "11.2 데이터 준비\n\n11.2.1 근로환경조사 기본 변수 생성 (선택)\ndata manipulation에서 실습한 내용을 통해 변수를 실습을 위한 변수를 생성한 데이터인 kwcsData1.rds 를 이용하겠습니다. 이에 대한 자세한 과정은 이전 챔터에 있습니다.  Table 1 변수 생성과정 챕터\n\nurl <- \"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/kwcsData1.rds\"\ndownload.file(url, \"data/kwcsData1.rds\")\ndat1 = readRDS(\"data/kwcsData1.rds\")"
  },
  {
    "objectID": "350_statistical_plot.html#overview-of-data-and-table-1",
    "href": "350_statistical_plot.html#overview-of-data-and-table-1",
    "title": "11  T검정, 로지스틱 회귀분석 플랏",
    "section": "11.3 Overview of data and Table 1",
    "text": "11.3 Overview of data and Table 1\nVariables can be divided into dependent and independent variables, and independent variables can be further categorized based on their nature into continuous or nominal variables. Let’s distinguish them as such.\n\nstratas  = c(\"sleepgp\")\ncatVars = c(\"wwa1gp\", \"shortReturn\",\"shiftWork\" , \"njob\", \"sexgp\",  \"edugp\", \"empgp\")\nconVars = c(\"AGE\",\"satisfaction\")\n\n\ntab1 = tabf(dat1=dat1, stratas = stratas, catVars = catVars, conVars = conVars)\ntab1\n\n# A tibble: 22 × 5\n   variables      values       `0.non distrubance` `1.sleep disturbance` p.value\n   <chr>          <chr>        <chr>               <chr>                 <chr>  \n 1 \"AGE\"          \"\"           46.8±12.4           49.7±11.9             \"<0.00…\n 2 \"wwa1gp\"       \"Never\"      12222 (94.7%)       684 (5.3%)            \"<0.00…\n 3 \"\"             \"Rarely\"     12316 (94.5%)       714 (5.5%)            \"\"     \n 4 \"\"             \"Sometimes\"  9112 (90.3%)        981 (9.7%)            \"\"     \n 5 \"\"             \"Often\"      3456 (82.8%)        717 (17.2%)           \"\"     \n 6 \"\"             \"Always\"     634 (70.0%)         272 (30.0%)           \"\"     \n 7 \"satisfaction\" \"\"           2.1±0.5             2.4±0.6               \"<0.00…\n 8 \"shortReturn\"  \"non short … 36183 (92.5%)       2927 (7.5%)           \"<0.00…\n 9 \"\"             \"short retu… 1557 (77.9%)        441 (22.1%)           \"\"     \n10 \"shiftWork\"    \"non shift … 35056 (91.9%)       3073 (8.1%)           \"<0.00…\n# ℹ 12 more rows\n\n\n\ntab1 %>% \n  setNames(c(\"\", \"\", \"None\", \"Disturbance\", \"P value\")) %>%\n  htmlTable(\n    cgroup = c(\"\",  \"Sleep disturbance\", \"\"), \n    n.cgroup = c(2, 2, 1), \n    tfoot = \"P value calculated by Chisq-Test and T-Test\", \n    rnames = FALSE, \n    caption = \"Basic Characteristics according to Sleep disturbance\", \n    css.table = \"font-family: 'Times New Roman';\"\n  ) \n\n\n\n\n\nBasic Characteristics according to Sleep disturbance\n\n \nSleep disturbance \n\n\n\n\n \n \nNone\nDisturbance \n \nP value\n\n\n\n\nAGE\n \n \n46.8±12.4\n49.7±11.9 \n \n<0.001\n\n\nwwa1gp\nNever \n \n12222 (94.7%)\n684 (5.3%) \n \n<0.001\n\n\n\nRarely \n \n12316 (94.5%)\n714 (5.5%) \n \n\n\n\n\nSometimes \n \n9112 (90.3%)\n981 (9.7%) \n \n\n\n\n\nOften \n \n3456 (82.8%)\n717 (17.2%) \n \n\n\n\n\nAlways \n \n634 (70.0%)\n272 (30.0%) \n \n\n\n\nsatisfaction\n \n \n2.1±0.5\n2.4±0.6 \n \n<0.001\n\n\nshortReturn\nnon short return \n \n36183 (92.5%)\n2927 (7.5%) \n \n<0.001\n\n\n\nshort return \n \n1557 (77.9%)\n441 (22.1%) \n \n\n\n\nshiftWork\nnon shift work \n \n35056 (91.9%)\n3073 (8.1%) \n \n<0.001\n\n\n\nshift work \n \n2684 (90.1%)\n295 (9.9%) \n \n\n\n\nnjob\none-job \n \n37471 (91.9%)\n3317 (8.1%) \n \n<0.001\n\n\n\nnjob \n \n269 (84.1%)\n51 (15.9%) \n \n\n\n\nsexgp\nMen \n \n17892 (93.1%)\n1327 (6.9%) \n \n<0.001\n\n\n\nWomen \n \n19848 (90.7%)\n2041 (9.3%) \n \n\n\n\nedugp\nuniversity or more \n \n19597 (92.9%)\n1502 (7.1%) \n \n<0.001\n\n\n\nhigh school \n \n14943 (91.9%)\n1318 (8.1%) \n \n\n\n\n\nmiddle school or below \n \n3200 (85.4%)\n548 (14.6%) \n \n\n\n\nempgp\npaid-worker \n \n25786 (92.4%)\n2122 (7.6%) \n \n<0.001\n\n\n\nemployer/self-employer \n \n2539 (91.7%)\n229 (8.3%) \n \n\n\n\n\nown-account worker \n \n8359 (90.5%)\n880 (9.5%) \n \n\n\n\n\nunpaind family work \n \n1056 (88.5%)\n137 (11.5%) \n \n\n\n\n\nP value calculated by Chisq-Test and T-Test\n\n\n\n\nT-test\n\nThe mean level of age are differ according to sleep disturbance. Box plot show mean and distribution of values. There are two method of t.test in R. One is using two vector, and the other is using of relationship.\n\n#stratas = \"sleepgp\"\nage1 = dat1$AGE[dat1$sleepgp ==\"0.non distrubance\"]\nage2 = dat1$AGE[dat1$sleepgp ==\"1.sleep disturbance\"] \nt.test(age1, age2)\n\n\n    Welch Two Sample t-test\n\ndata:  age1 and age2\nt = -13.409, df = 4040.8, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.306275 -2.462776\nsample estimates:\nmean of x mean of y \n 46.77878  49.66330 \n\nt.test(dat1$AGE ~ dat1$sleepgp)\n\n\n    Welch Two Sample t-test\n\ndata:  dat1$AGE by dat1$sleepgp\nt = -13.409, df = 4040.8, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group 0.non distrubance and group 1.sleep disturbance is not equal to 0\n95 percent confidence interval:\n -3.306275 -2.462776\nsample estimates:\n  mean in group 0.non distrubance mean in group 1.sleep disturbance \n                         46.77878                          49.66330 \n\n\nT test’s p-value can be generated by following pipe code.\n\nttestPvalue <- dat1 %>%\n  select(sleepgp, AGE) %>%\n  pivot_longer(-sleepgp) %>%\n  nest(dat=-name) %>%\n  mutate(fit =map(dat, ~t.test(.$value ~ .$sleepgp)), \n         tidied =map(fit, tidy)) %>%\n  unnest(tidied) %>%\n  select(name, p.value) %>%\n  mutate(pvalue = ifelse(p.value < 0.001, \"<0.001\", sprintf(\"%.3f\", p.value))) %>%\n  pull(pvalue)\n\n# 위에는 여러개의 p value를 구할때 사용하는 것이고 하나만 사용할 때는\nmyttest <- t.test(dat1$AGE ~ dat1$sexgp)\nmyttest$p.value #를 사용한다. \n\n[1] 3.287249e-28\n\n\n\ndat1 %>%\n  select(sleepgp, AGE) %>%\n  ggplot(aes(x=sleepgp, y= AGE)) +\n  geom_boxplot() \n\n\n\n\n\ndat1 %>%\n  select(sleepgp, AGE) %>%\n  ggplot(aes(x=sleepgp, y= AGE)) +\n  geom_boxplot() +\n  ggtitle(\"Age according to sleep disturbance\")+\n  xlab(\"Sleep Disturbance\") + \n  theme(text=element_text(family=\"Times\"))\n\n\n\n\n\ndat1 %>%\n  select(sleepgp, AGE) %>%\n  ggplot(aes(x=sleepgp, y= AGE)) +\n  geom_boxplot() +\n  ggtitle(\"Age according to sleep disturbance\")+\n  xlab(\"Sleep Disturbance\") + \n  annotate(\n    geom =\"text\", -Inf, Inf, \n    hjust=-3, vjust=5,\n    label =sprintf(\"P value:  %s\", ttestPvalue), \n    family = \"Times\"\n  ) #+\n\n\n\n  #theme(text=element_text(family=\"Times\")) \n\n\nChisq Test\n\nwwa1gp is “A. kept worrying about work when you were not working”, and the response is 0.Never, 1.Rarely, 2.Sometimes, 3.Often and 4.Always. In the previous table, the distribution of sleep distrubance are differ according to wwa1gp.  The bar chart is one of the best option for distribution visualization.\n\ndat1 %>%\n  group_by(wwa1gp) %>%\n  count(sleepgp) %>%\n  mutate(prob = n/sum(n)*100, \n         prob = round(prob, 1)) %>%\n  filter(sleepgp ==\"1.sleep disturbance\") %>%\n  ggplot(aes(x=wwa1gp, y = prob)) +\n  geom_bar(stat=\"identity\")\n\n\n\n\nSome modification are needed to communicat to other researcher. Percent scale are easy to follow.\n\ndat1 %>%\n  group_by(wwa1gp) %>%\n  count(sleepgp) %>%\n  mutate(prob = n/sum(n)*100, \n         prob = round(prob, 1)) %>%\n  filter(sleepgp ==\"1.sleep disturbance\") %>%\n  ggplot(aes(x=wwa1gp, y = prob)) +\n  #theme(text=element_text(family=\"Times New Roman\", face=\"bold\", size=12)) +\n  xlab(\"kept worrying about work when you were not working\") +\n  ylab(\"Sleep Disturbance (proportion)\") +\n  ylim(0, 0.35)+\n  geom_bar(stat=\"identity\") +\n  geom_text(aes(label = sprintf(\"%s%%\", prob)),  vjust = -0.5) +\n  scale_y_continuous(labels = function(x) sprintf(\"%s%%\", round(x)))\n\n\n\n\nNow, we added the p value of chisq.test().\n\nchisqp = dat1 %>%\n  select(wwa1gp, sleepgp) %>%\n  table() %>%\n  chisq.test()\nif (chisqp$p.value <0.001){\n  pChisqV = \"<0.001\"\n  } else{\n    pChisqV = chisqp$p.value %>% sprintf(\"%.3f\", .)\n  }\npChisqV\n\n[1] \"<0.001\"\n\n\n\n#library(extrafont)\nlibrary(ggthemes)\ndat1 %>%\n  group_by(wwa1gp) %>%\n  count(sleepgp) %>%\n  mutate(prob = n/sum(n)*100, \n         prob = round(prob, 1)) %>%\n  filter(sleepgp ==\"1.sleep disturbance\") %>%\n  ggplot(aes(x=wwa1gp, y = prob)) +\n  xlab(\"kept worrying about work when you were not working\") +\n  ylab(\"Sleep Disturbance (proportion)\") +\n  geom_bar(stat=\"identity\") +\n  geom_text(aes(label = sprintf(\"%s%%\", prob)),  vjust = -0.5) +\n  scale_y_continuous(labels = function(x) sprintf(\"%s%%\", round(x))) +\n  annotate(\n    geom =\"text\", -Inf, Inf, \n    hjust=-0.5, vjust=8,\n    label =sprintf(\"P value:  %s\", pChisqV), \n    #family = \"Times New Roman\", \n    fontface = \"bold\"\n  ) +\n  theme_minimal() #+\n\n\n\n  #theme(text=element_text(family=\"Times New Roman\", face=\"bold\", size=12))"
  },
  {
    "objectID": "350_statistical_plot.html#odds-ratio-and-95-confidence-interval",
    "href": "350_statistical_plot.html#odds-ratio-and-95-confidence-interval",
    "title": "11  T검정, 로지스틱 회귀분석 플랏",
    "section": "11.4 Odds ratio and 95% confidence interval",
    "text": "11.4 Odds ratio and 95% confidence interval\n\nLogistic regression model\n\nThere are 3 kind of models, model II and III are include more confounding variables compare to model I.\n\nmod1 = dat1 %>%\n  glm(data=.,family=\"binomial\",formula = sleepgp == \"1.sleep disturbance\"  \n      ~ wwa1gp)\nmod2 = dat1 %>%\n  glm(data=.,family=\"binomial\",formula = sleepgp == \"1.sleep disturbance\"\n      ~ wwa1gp + AGE + sexgp +satisfaction)\nmod3 = dat1 %>%\n  glm(data=.,family=\"binomial\",formula = sleepgp == \"1.sleep disturbance\"\n      ~ wwa1gp + AGE + sexgp +satisfaction + shiftWork + njob)\n\noddf0() function gives us odds ratio, lower limt and upper limit (95% confidence interval). We can draw OR (95% CI) plot, using errobar() function. we can also change the color using RGB triplet (ref: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)).\n\noddf0(mod1) %>%\n  ggplot(aes(y=values, x = or)) +\n  theme_classic() +\n  geom_errorbarh(aes(xmin = ll, xmax = ul), height = 0.09, color = \"#666666\", size=0.5) +\n  geom_point(shape = 15, size = 2, color = \"#444444\")  +\n  geom_vline(xintercept = 1, color = \"#FF0000\", linetype = \"dashed\", cex =0.5, alpha = 0.5) +\n  ylab(\"kept worrying about work when you were not working\") +\n  xlab(\"Odds Ratio and 95% Confidence Interval of Sleep Dsturbance\") #+\n\n\n\n  #theme(text = element_text(family =\"Times\"))\n\nVertical error bar also suitable to plot the OR (95% CI).\n\noddf0(mod1) %>%\n  ggplot(aes(x=values, y = or)) +\n  theme_classic() +\n  geom_errorbar(aes(ymin = ll, ymax = ul), width = 0.07, color = \"#666666\", size=0.5) +\n  geom_point(shape = 15, size = 2.5, color = \"#444444\") +\n  geom_hline(yintercept = 1, color = \"#FF0000\", linetype = \"dashed\", cex =0.5, alpha = 0.5) +\n  xlab(\"Kept worrying about work when you were not working\") +\n  ylab(\"Odds Ratio and 95% Confidence Interval of Sleep Dsturbance\")# +\n\n\n\n  #theme(text = element_text(family =\"Times\"))"
  },
  {
    "objectID": "370_labelled_plot.html#introduction",
    "href": "370_labelled_plot.html#introduction",
    "title": "12  다차원 라벨자료",
    "section": "12.1 introduction",
    "text": "12.1 introduction\nIn current data, We will used covid19 data from 16 countries. The group is country level and the interesting relationship is regarded on individual economic activity and psychological symptoms. In this chapter, we will discuss about how can visualization multi-level data (The original data are own to Laura, PH.D.)\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"ggthemes\")) install.packages(\"ggthemes\")\nif(!require(\"ggrepel\")) install.packages(\"ggrepel\")\nif(!require(\"gridExtra\")) install.packages(\"gridExtra\")\n\n\nurl1 <- \"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/CovidJobLoss.rds\"\ndownload.file(url1, \"data/CovidJobLoss.rds\")\nurl2 <- \"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/hdi.csv\"\ndownload.file(url2, \"data/hdi.csv\")\nurl3 <- \"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/covid_agu1.csv\"\ndownload.file(url3, \"data/covid_agu1.csv\")\n\n\nmm3 = readRDS(\"data/CovidJobLoss.rds\")\nhdi = read_csv(\"data/hdi.csv\")%>% \n  setNames(c('rank', 'country', 'y2019', 'country_1'))\nagu1 <- read_csv('data/covid_agu1.csv')\n\nData manipulation for creating and reshaping variables. We will exam the assoicaton between human development index and covid related outcomes.\n\nmm4 <- mm3 %>%\n    mutate(agegp3 = ifelse(age <=40, '≤40', '>40')) %>%\n    mutate(agegp3 = factor(agegp3, levels=c('≤40', '>40'))) %>%\n    filter(Education %in% c(1:6)) %>%\n    mutate(edugp = ifelse(Education %in% c(1, 2), 1, # high school or less\n                        ifelse(Education %in% c(3),2, # college\n                               ifelse(Education %in% c(4), 3, 4)))) %>% # university (5, 6) Graduate school\n    mutate(edugp2 = ifelse(Education %in% c(1, 2, 3), 1, 2)) %>%\n mutate(EcLossAllgp = ifelse(EcLossAll ==0, 0, 1)) #%>%\n\nmm4.0 = mm4 %>% # hid: human development index for each countries\n  left_join(hdi %>% filter(!is.na(country_1)) %>%\n  rename(country_source = country,\n         country=country_1   ), by = c('country')) %>%\n  mutate(y2019 = ifelse(country_c =='South Korea', 0.916, y2019))\n\n\n12.1.1 Data analysis start\n\n12.1.1.1 job loss due to COVID19, psychological aggravation according to Gender\nExplore data using basic static for job loss prevalence across country and genders. the pyschological aggravation due to covid-19 are also plotted according to same logic.\n\nfig1 <- mm4 %>%\n  group_by(country_c, gender) %>%\n  count(EcLossAllgp) %>%\n  mutate(prob = n/sum(n)) %>%\n  filter(EcLossAllgp == 1 ) %>%\n  ggplot(aes(x = gender, y = prob, fill = gender)) +\n  geom_bar(stat = \"identity\")+\n  xlab(\"\")+\n  ylab(\"Job loss due to COVID19\")+\n  scale_y_continuous(labels = function(x) paste0(x*100, \"%\"))+\n  facet_wrap(country_c ~., nrow = 3) +\n  theme_minimal()+\n  theme(text = element_text(size=17))+\n  theme(legend.position = c(.92, .1))\n\nfig2 <- mm4 %>%\n  group_by(country_c, gender) %>%\n  count(TotalDepAnx) %>%\n  mutate(prob = n/sum(n)) %>%\n  filter(TotalDepAnx == 1 ) %>%\n  ggplot(aes(x = gender, y = prob, fill = gender)) +\n  geom_bar(stat = \"identity\")+\n  ylab(\"Psychological aggravation due to COVID19\")+\n  xlab('Gender')+\n  scale_y_continuous(labels = function(x) paste0(x*100, \"%\"))+\n  facet_wrap(country_c ~., nrow = 3) +\n  theme_minimal()+\n  theme(text = element_text(size=17))+\n  theme(legend.position = c(.92, .1))\n\nSave total figure of fig1 and fig2. arrageGrob make two figures into the one figure.\n\nggsave(arrangeGrob(fig1, fig2, ncol = 1), file ='results/covid/figtotal.png', dpi = 300, width = 10, height =14)\nggsave(arrangeGrob(fig1, fig2, ncol = 1), file ='results/covid/tiff/figtotal.tiff', dpi = 300, width = 10, height =14)\n\n\n\n\n\n\n\n\n12.1.1.2 job loss, psychological aggravation according to age group\n\nfig3 <- mm4.0 %>%\n  #filter(!country %in% c(16, 173)) %>%\n  group_by(country_c, gender, agegp2) %>%\n  count(EcLossAllgp) %>%\n  mutate(prob = n/sum(n)) %>%\n  filter(EcLossAllgp == 1) %>%\n  ggplot(aes(x = agegp2, y = prob, color = gender)) +\n  geom_point(aes(size = n), alpha = 0.2, show.legend = FALSE) +\n  geom_smooth(method = 'loess', span =0.9,  se=FALSE) + \n   ylab(\"Job loss due to COVID19\")+\n    scale_y_continuous(labels = function(x) paste0(x*100, \"%\"))+\n  theme_minimal() +\n  xlab(\"\")+\n  labs(color = \"Gender\") +\n  facet_wrap(country_c~., nrow =3)+\n  theme(legend.position = c(.92, .1))+\n  theme(text = element_text(size=17))+\n  scale_x_continuous(breaks = c(2,4,6,8), labels = c(30,40,50,60))\n\nfig4 <- mm4.0 %>%\n  #filter(!country %in% c(16, 173)) %>%\n  group_by(country_c, gender, agegp2) %>%\n  count(TotalDepAnx) %>%\n  mutate(prob = n/sum(n)) %>%\n  filter(TotalDepAnx == 1) %>%\n  ggplot(aes(x = agegp2, y = prob, color = gender)) +\n  geom_point(aes(size = n), alpha = 0.2, show.legend = FALSE) +\n  geom_smooth(method = 'loess', span =0.9,  se=FALSE) + \n   ylab(\"Psychological aggravation due to COVID19\")+\n    scale_y_continuous(labels = function(x) paste0(x*100, \"%\"))+\n  theme_minimal() +\n  xlab(\"Age (size = number of respondents)\") +\n  labs(color = \"Gender\") +\n  facet_wrap(country_c~., nrow =3)+\n  theme(legend.position = c(.92, .1))+\n  theme(text = element_text(size=17))+\n  scale_x_continuous(breaks = c (2,4,6,8), labels = c(30,40,50,60))\n\nsave figures\n\n#figtotal2 <- grid.arrange(fig3, fig4, ncol = 1)\nggsave(arrangeGrob(fig3, fig4, ncol = 1), file ='results/covid/figtotal2.png', dpi = 300, width = 10, height =14)\nggsave(arrangeGrob(fig3, fig4, ncol = 1), file ='results/covid/tiff/figtotal2.tiff', dpi = 300, width = 10, height =14)\n\n\n\n\n\n\n\n\n\n12.1.2 job loss, psychological aggravation according to Education level\n\nQuzi 1\n\nLet’s fill the { num } to draw following figure.\n\nfig5 <- mm4.0 %>%\n  group_by(edugp, country_c, gender) %>%\n  count(EcLossAllgp ) %>%\n  mutate(prob = n/sum(n)) %>%\n  filter(EcLossAllgp  == 1) %>%\n  ggplot(aes(x = {1}     , y ={2}    , color = {2})) +\n  geom_smooth(method = {3}, span =0.9, se = FALSE) +\n  scale_x_continuous(labels=c(\"1\" = \"≤H\", \"2\" = \"C\",\n                            \"3\" = \"U\", \"4\" = \"G\")) +\n  theme_minimal() + \n  labs(color = {4}) +\n  ylab(\"Job Loss due to COVID19\") +\n  xlab(\"\")+\n  facet_wrap({5})+\n  theme(text = element_text(size=17))+\n  theme(legend.position = c(.92, .1))\n\n# mm4 %>%\n#   count(TotalDepAnx)\nfig6 <- mm4.0 %>%\n  group_by(edugp, country_c, gender) %>%\n  count(TotalDepAnx ) %>%\n  mutate(prob = n/sum(n)) %>%\n  filter(TotalDepAnx  == 1  ) %>%\n  ggplot(aes({6})) +\n  geom_smooth({7}) +\n  scale_x_continuous({8}) +\n  theme_minimal() + \n  ylab(\"Psychological aggravation due to COVID19\") +\n  scale_y_continuous(labels = {9})+\n  xlab(\"H = high school, C = college, U = university, G = graduate \") +\n  labs(color = \"Gender\") +\n  facet_wrap({10})+\n  theme(text = element_text(size=17))+\n  theme(legend.position = c(.92, .1))\n\nsave figure\n\n#figtotal3 <- grid.arrange(fig5, fig6, ncol = 1)\nggsave(arrangeGrob(fig5, fig6, ncol = 1), file ='results/covid/figtotal3.png', dpi = 300, width = 10, height =14)\nggsave(arrangeGrob(fig5, fig6, ncol = 1), file ='results/covid/tiff/figtotal3.tiff', dpi = 300, width = 10, height =14)\n\n\nknitr::include_graphics('results/covid/figtotal3.png')\n\n\n\n\n\n\n12.1.3 Visulization with human develop index stratification\nLabelled data visualization need the code of `geom_label_repel’\n\nagu1 <- read_csv('data/covid_agu1.csv')\n\nlab_mm <- mm4.0 %>%\n  left_join(agu1, by = 'country_c') %>%\n  mutate(inc_aug = c_case_agu1 / population *100000, \n         dth_aug = c_death_agu1/ population *100000) %>%\n  group_by(country_c, gender) %>%\n  summarize(hdi = mean(y2019), \n            Psycho = mean(TotalDepAnx ==1), \n            ecl = mean(EcLossAll !=0),\n            inc = mean(inc_aug)) \n\nlab_mm%>%\n  ggplot(aes(x = hdi, y = ecl, color = inc)) +\n  geom_point() +\n  theme_classic()+\n  xlab(\"Human Devlopment Index\") +\n  #ylab(\"prevalance of Psychological Symptoms\") +\n  ylab(\"prevalance of Economic Loss due to COVID19\") + \n  #ylim(c(-0.1, 1)) + #xlim(c(2, 4)) + \n  geom_label_repel(aes(label = country_c), \n                   fill = NA, # 투명하게 해서 겹쳐도 보이게\n                   alpha =1, size = 3, # 작게\n                   box.padding = 0.4,  # 분별해서\n                   segment.size =0.1,  # 선 \n                   force = 2)      +    # 이것은 무엇일까요?\n   theme_minimal() +\n  geom_smooth(method = 'lm', formula = y ~ poly(x,2), se=FALSE) +\n  #geom_smooth( se=FALSE) +\n  facet_wrap(gender~.) +\n  guides(color = \"none\")\n\n\n\n\n\nQuiz 2,\n\nThe Education level may affect the relationship between hdi and economic loss status. please draw following plot, and find the most vulnerable population."
  },
  {
    "objectID": "380_wave_plot.html",
    "href": "380_wave_plot.html",
    "title": "13  웨이브 형테의 시각화",
    "section": "",
    "text": "13.0.1 Data Visualization example (EEG data)\nIn public health, another most common used data is bio-signal data. Bio-signal data usually used in medical research, but recently bio-log signals are widely used in public health beyond medical setting. Now I present some example of data visualization using bio-signal data from EEG.\n\n\n13.0.2 Introduction\nEEG refers to the signal of the brain’s electrical activty. Electrodes are placed on the scalp, and each electrodes recorded brain’s activity. EEG is one of the most common methods to support diagnosis of brain diseases such as epilepsy, sleep disorder and even brain death. Furthermore, we can get usufull understand how brain activity correlated to various neurological activity. So, I tried to anaysis EEG signal and hope to predict eye opening status.\nThe data include information of row_n, AF3, F7, F3, FC5, T7,P7, O1, O2, P8, T8, FC6, F4, F8, AF4, and eyeDetection. Eye Detection is outcome variable data Open or Closed. Others numeric variable about activity of each electrodes. Each electro-nodes place on scalp, and represent area of particular location on brain, as below.\n\nrm(list=ls())\nurl <- \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/70/21_electrodes_of_International_10-20_system_for_EEG.svg/1024px-21_electrodes_of_International_10-20_system_for_EEG.svg.png\"\ndownload.file(url, 'img/eegpng.png')\n\n\n\n\neeg electrodes from wiki\n\n\n\n\n13.0.3 Dataset and Data step\n\n13.0.3.1 Data download and handling\nThe data set locate here, https://archive.ics.uci.edu/ml/machine-learning-databases/00264/EEG%20Eye%20State.arff. The data was stored into my computer, as name of ‘dl_eeg.txt’ in `data’ folder.\n\nurl ='https://archive.ics.uci.edu/ml/machine-learning-databases/00264/EEG%20Eye%20State.arff'\ndownload.file(url, 'data/dl_eeg.txt')\n\nTo start the data step, some packages should be loaded.\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"htmlTable\")) install.packages(\"htmlTable\")\nif(!require(\"broom\")) install.packages(\"broom\")\nif(!require(\"ggthemes\")) install.packages(\"ggthemes\")\n# packages from github\nif(!require(\"devtools\")) install.packages(\"devtools\")\nlibrary(devtools)\n#install_github(\"jinhaslab/tabf\", force = TRUE)\nlibrary(tabf)\n#library(caret)\nlibrary(knitr)\nlibrary(kableExtra)\n#library(doMC)\n\nscan the data and create DB for analysis\n\nmm <- scan('data/dl_eeg.txt', what=\"\", sep=\"\")\nwhich(mm == '@DATA')\n\n[1] 48\n\ndat<-mm[-c(1:48)]\nbook <- mm[1:48]\nbook[-c(1:2, 48)] %>% \n  matrix(., ncol=3, byrow=TRUE) %>% \n  .[,2] -> col_names\ncol_names\n\n [1] \"AF3\"          \"F7\"           \"F3\"           \"FC5\"          \"T7\"          \n [6] \"P7\"           \"O1\"           \"O2\"           \"P8\"           \"T8\"          \n[11] \"FC6\"          \"F4\"           \"F8\"           \"AF4\"          \"eyeDetection\"\n\n\nthe tibble form is easy to hand or transforming. So, I change the data form to tibble style.\n\ntibble(wave =dat) %>%\n  mutate (val = strsplit(wave, \",\"), row_n=row_number()) %>% \n  unnest (cols=c(val)) %>% \n  select (-wave) %>%\n  mutate (val = as.numeric(val)) %>%\n  group_by(row_n)  %>%\n  mutate (wave_colname = col_names) %>% \n  ungroup() %>%\n  pivot_wider(names_from = wave_colname, values_from = val)%>%\n  select (row_n, all_of(col_names)) %>%\n  arrange (row_n) -> \n  eeg\n\nI check the class of all variable.\n\nnames(eeg)\n\n [1] \"row_n\"        \"AF3\"          \"F7\"           \"F3\"           \"FC5\"         \n [6] \"T7\"           \"P7\"           \"O1\"           \"O2\"           \"P8\"          \n[11] \"T8\"           \"FC6\"          \"F4\"           \"F8\"           \"AF4\"         \n[16] \"eyeDetection\"\n\nsapply(eeg, mode)\n\n       row_n          AF3           F7           F3          FC5           T7 \n   \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\" \n          P7           O1           O2           P8           T8          FC6 \n   \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\" \n          F4           F8          AF4 eyeDetection \n   \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\" \n\n\nThis data were measure for 117 seconds, I create sec variable to represent measurement time. I create facotor variable for eye opening status `Eye’, as below.\n\neeg <- eeg %>% \n  mutate(Eye = ifelse(eyeDetection ==1, 'open', 'closed')) %>%\n  mutate(sec = seq(0, 117, length.out = nrow(.)))\n\nFor data step, the long form data were created via pivot_longer function. the key represent each electrode, and the activity were stored into activity variable. EyeDetection, row_n, sec, Eye will be indentical variable for each electrode activity.\n\neeg %>% \n  pivot_longer(-c(eyeDetection, row_n, sec, Eye),\n    names_to = 'electrode', \n               values_to= 'activity') ->\n  eegl\neegl %>% head()\n\n# A tibble: 6 × 6\n  row_n eyeDetection Eye      sec electrode activity\n  <int>        <dbl> <chr>  <dbl> <chr>        <dbl>\n1     1            0 closed     0 AF3          4329.\n2     1            0 closed     0 F7           4009.\n3     1            0 closed     0 F3           4289.\n4     1            0 closed     0 FC5          4148.\n5     1            0 closed     0 T7           4350.\n6     1            0 closed     0 P7           4586.\n\n\n\n\n13.0.3.2 data explorer\n\n13.0.3.2.1 heatmap\nThe heatmap can show overview the time course about eye opening status. Dark blue indicate closed status, and light blue indicate eye open status.\n\neegl %>%\n  ggplot(aes(x = sec, y = 0.5, fill = eyeDetection)) +\n  geom_tile() +\n  scale_fill_continuous(name = \"Eye status\", \n                        limits = c(0, 1), \n                        labels = c('closed','open'), \n                        breaks = c(0, 1)) +\n  theme_classic() +\n  scale_y_continuous(breaks = c(0,  1), 'Eye status') +\n  scale_x_continuous(breaks = c(0, 30, 60, 90, 117)) +\n  theme(aspect.ratio=1/7)\n\n\n\n\n\n\n13.0.3.2.2 waveform\nThe activity waves are plotting for data explorer.\n\neegl %>% \n  ggplot(aes(x = row_n, y = activity, color = Eye)) +\n  geom_line(size = c(1)) +\n  geom_point(alpha = 1)+\n  facet_grid(electrode ~. ) +\n  theme_classic() \n\n\n\n\nThere are much of outlier, so the fluctuation of each node’s activity atteunted. So We needed to exclude outlier. To ensure the outlier status, I plot the boxplot, as below. The boxplot also suggested that there were much of outlier effect.\n\neegl %>% \n  ggplot(aes(x = electrode, y = activity, color = Eye)) +\n  geom_boxplot() +\n  scale_y_continuous(trans='log2') +\n  theme_classic()\n\n\n\n\nAlthoug there are outlier effect on data, but I just want to check the t-test results.\n\nttest<- eegl %>% select(-eyeDetection, -row_n) %>%\n  group_by(electrode)  %>%\n  nest() %>% \n  mutate(stats     = map(data, ~t.test(activity ~ Eye, data=.x)), \n         summarise = map(stats, broom::glance))  %>%\n  select(electrode, summarise) %>%\n  unnest(summarise) \nttest %>% \n  mutate_if(is.numeric, round, 2) %>% \n  mutate(p.values = ifelse(p.value <0.01, '<0.01', as.character(p.value))) %>%\n  select(electrode,`difference` = estimate, 'open' = estimate1, 'closed' =  estimate2, p.values) %>%\n  htmlTable(\n    cgroup=c(\"\", \"Eye status\", \"\"), \n    n.cgroup=c(2, 2, 1), \n    caption = \"Table1. t-test result without outlier cleaning\", \n    align = 'llrc'\n  )\n\n\n\n\n\nTable1. t-test result without outlier cleaning\n\n\n \nEye status \n\n\n\nelectrode\ndifference \n \nopen\nclosed \n \np.values\n\n\n\n\n1\nAF3\n-52.4 \n \n4298.4\n4350.8 \n \n0.25\n\n\n2\nF7\n7.39 \n \n4013.08\n4005.69 \n \n<0.01\n\n\n3\nF3\n-3.47 \n \n4262.46\n4265.94 \n \n<0.01\n\n\n4\nFC5\n78.98 \n \n4200.39\n4121.41 \n \n0.31\n\n\n5\nT7\n0.03 \n \n4341.75\n4341.73 \n \n0.96\n\n\n6\nP7\n46.13 \n \n4664.73\n4618.59 \n \n0.29\n\n\n7\nO1\n66.82 \n \n4140.39\n4073.57 \n \n0.33\n\n\n8\nO2\n-1.48 \n \n4615.39\n4616.87 \n \n<0.01\n\n\n9\nP8\n-41.13 \n \n4200.37\n4241.5 \n \n0.29\n\n\n10\nT8\n-3.61 \n \n4229.7\n4233.31 \n \n<0.01\n\n\n11\nFC6\n-4.88 \n \n4200.26\n4205.15 \n \n<0.01\n\n\n12\nF4\n-4.01 \n \n4277.43\n4281.44 \n \n<0.01\n\n\n13\nF8\n-31.88 \n \n4600.9\n4632.77 \n \n0.15\n\n\n14\nAF4\n89.42 \n \n4456.57\n4367.15 \n \n0.31\n\n\n\n\n\n\n\n\n13.0.3.3 data cleaning for outlier\nThe visualization of interquartile range is useful for detecting the presenting of outliers. Outliers are individual values that fall outside of the overall pattern of a data set. So, the outlier can be define when they are far way as several times of interquarile range from median. I made scale_median function to detect outlier. there are several step to make function. first, calculate the interquartile range. The differece between median and value divided by inter quartile range, and the absolute value will be Z-score. That z-score represent that how many times of interquartile range far from median. I used score of 2.5 times par form median for detect outlier.\n\nscale_median <- function(x){\n    Q1  <- quantile(x, 0.25)\n    Q3  <- quantile(x, 0.75)\n    iqr <- c(Q3 - Q1)\n  ifelse(x < median(x),(x - Q1)/iqr*-1,(x - Q3)/iqr*1 )\n}\neegl1<- \n  eegl %>% \n  group_by(electrode) %>%\n  nest() %>%\n  mutate(Zscores = map (data, ~scale_median(.$activity))) %>% \n  unnest(cols = c(data, Zscores))  %>%\n  filter(Zscores < 2.5) #Outlier is Zscores > 2.5 or <-2.5\n\nI check box-plot without outlier. That plot sounds good for me. So, I can generate wave form plot.\n\neegl1 %>%\n  ggplot(aes(x = electrode, y = activity, color = Eye)) +\n  geom_boxplot() \n\n\n\n\nHere, I plot wave form of each electrodes’ activity. Before generate the wave plot, I want increase the working cores.\n\nlibrary('doMC')\nlibrary(parallel)\ngetDoParWorkers()\n\n[1] 1\n\nnumber_of_cores <- detectCores()\n\nMy computer use only 1 core, so I increase number of cores working up-to 46 cores.\n\nregisterDoMC(cores = number_of_cores - 2)\ngetDoParWorkers()\n\n[1] 46\n\n\nThen, I want plot wave form.\n\neegl1 %>% \n  ggplot(aes(x = sec, y = activity, fill = eyeDetection)) +\n  geom_point(size = 0.1, alpha =0.5) +\n  geom_line(aes(color = eyeDetection), show.legend = F) +\n  facet_grid(electrode ~., scales = \"free\" ) +\n  theme_classic() +\n  scale_fill_continuous(name = \"Eye status\", limits = c(0, 1), \n                        labels = c('close', 'open'), breaks = c(0, 1)) +\n  xlab('Time series') +\n  scale_x_continuous(breaks = c(0, 30, 60, 90, 117))"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "14  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "815_rmarkdown02.html#설치",
    "href": "815_rmarkdown02.html#설치",
    "title": "14  R markdown",
    "section": "14.1 설치",
    "text": "14.1 설치\n설치는 다른 페키지 설치와 같습니다. 아래의 install.packages()명령을 통해 설치해 봅니다 .\n\ninstall.packages(\"rmarkdown\")\n\n그리고 Cheatsheet을 이용하여 공부하면 됩니다. 다시 말하지만, 외우기 보다는 필요한 내용만 그때 그때 사용하시는 것을 권장합니다.\n * R Markdown Cheat Sheet \n * R Markdown Reference Guide"
  },
  {
    "objectID": "815_rmarkdown02.html#components",
    "href": "815_rmarkdown02.html#components",
    "title": "14  R markdown",
    "section": "14.2 3 components",
    "text": "14.2 3 components\nR마크다운은 3가지 요소로 구성됩니다. 하나는 YAML, Text, 그리고 Code 입니다.\n\n\n\n구성\n내용\n\n\n\n\nYAML\n모양, 기본 구조, 출력 폼 등\n\n\nText\n기본 문장, 워드 등과 동일\n\n\nCode\nR code, Python code 등"
  },
  {
    "objectID": "815_rmarkdown02.html#simple-tutor",
    "href": "815_rmarkdown02.html#simple-tutor",
    "title": "14  R markdown",
    "section": "14.3 simple tutor",
    "text": "14.3 simple tutor\n기본 세팅을 하게 됩니다. 제목과 html_document 방식 출력을 해보겠습니다. 문장, 그리고 plot을 해보겠습니다. 지도 및 위치정보 시각화는 Dspub-Project 에서 다루게 됩니다. 여기서는 그냥 leaflet 페키지를 이용해서 가능하다 정도 알아두시면 되겠습니다. leaflet 페키지가 설치가 않되시는 분은 그림을 출력하는 png, 지형출력 raster 등이 설치 되지 않아 그럴 수 있습니다. 이는 지도 쳅터 부분에서 leaflet 페키지 설치 부분을 참조해 주세요. leaflet\n\n\n\n\n\n\n아래와 같은 코딩으로 상기 화면이 출력됩니다.\n\nlibrary(leaflet)\npopup = c(\"Korea\")\nleaflet() %>%\n  addProviderTiles(\"NASAGIBS.ViirsEarthAtNight2012\") %>%\n  addMarkers(lng = c(124.3636,125.1057, 131.52104),\n             lat = c(37.5810, 32.5620,37.14268))\n\n\n\n\nmarkdown tutor 1\n\n\n즉 글을 쓰고, chunck를 만들고 그 안에 R code를 적으면 됩니다. chunck 는 crtrl + alt + i를 사용하면 생성됩니다."
  },
  {
    "objectID": "815_rmarkdown02.html#chunck-option",
    "href": "815_rmarkdown02.html#chunck-option",
    "title": "14  R markdown",
    "section": "14.4 chunck option",
    "text": "14.4 chunck option\nchunck 에 몇가지 옵션을 설정할 수 있습니다. 이는 r{ } 의 괄호 안에 넣으면 됩니다.\n\n\n\nchunck option\n내용\n\n\n\n\ninclude = FALSE\n마크다운 파일에 넣지 않기\n\n\necho = FALSE\n코드는 보여 주지 않고 결과만 보이기\n\n\nmessage = FALSE\n메세지 보이지 않기\n\n\nwarning = FALSE\n경고 메세지 보이지 않기\n\n\nfig.cap = ” xx ”\nxx 라고 figure caption 넣기"
  },
  {
    "objectID": "815_rmarkdown02.html#table-만들기",
    "href": "815_rmarkdown02.html#table-만들기",
    "title": "14  R markdown",
    "section": "14.5 Table 만들기",
    "text": "14.5 Table 만들기\n표를 만들기 위해서는 code를 이용하는 방법과 Text를 이용하는 방법이 있습니다. code는 data.fram을 출력하는 방식입니다.\n\nfruit<-data.frame(\"fruit\" = c(\"apple\", \"orange\", \"mango\"), \n                  \"taste\" = c(\"delicious\", \"lovely\", \"Wow!\"))\nDT::datatable(fruit)\n\n\n\n\n\n\nText방식은 |를 이용하는 것입니다.\n\n\n\nfruit\ntaste\n\n\n\n\napple\ndelicious\n\n\norange\nlovely\n\n\nmango\nWow!\n\n\n\n\n\n\ntable rmarkdown"
  },
  {
    "objectID": "815_rmarkdown02.html#필독",
    "href": "815_rmarkdown02.html#필독",
    "title": "14  R markdown",
    "section": "14.6 필독!",
    "text": "14.6 필독!\n마크다운은 설명하기가 쉽지 않네요! 반드시 아래 홈페이지를 들어가서 보시기 바랍니다.\n\n\n\n\n\n\n\n마크다운 안내\n내용\n\n\n\n\nrstudio.com\nhttps://rmarkdown.rstudio.com\n\n\nlecture 1"
  },
  {
    "objectID": "815_rmarkdown02.html#동영상",
    "href": "815_rmarkdown02.html#동영상",
    "title": "14  R markdown",
    "section": "14.7 동영상",
    "text": "14.7 동영상\n글로 설명하기 어렵네요, 아래의 동영상을 보고 해보세요!"
  },
  {
    "objectID": "815_rmarkdown02.html#과제",
    "href": "815_rmarkdown02.html#과제",
    "title": "14  R markdown",
    "section": "14.8 과제",
    "text": "14.8 과제\n아래 과제 내용을 읽고 해보세요!\n\n\n\n과제\n내용\n\n\n\n\n데이터 테이블\n기존 강의에서 table 1 마크 다운에 표시시\n\n\n그림(plot)\n아무 plot이나 그리기\n\n\n우리나라지도\n우리나라 지도 보여주기\n\n\nhtml파일 저장하기\n저장한 파일 업로드"
  },
  {
    "objectID": "410_wrangling_text_01.html",
    "href": "410_wrangling_text_01.html",
    "title": "14  문자값 다루기",
    "section": "",
    "text": "데이터 시각화 및 데이터 표준화를 위해서 반드시 수행되는 과정입니다. 데이터를 어떻게 사용할 것인가와 최종 사용 양상을 고려해서 이에 맞도로 데이터를 수정하는 과정입니다. 이에 자주 사용하는 방법은 gather, spread, seprate, unite, unique등이 이 있습니다 이에 대한 실습을 수행하겠습니다. 제가 어떠한 일을 한때 50%는 위의 pivot_longer, pivot_wider, seprate, unique와 str_()를 사용하게 됩니다. 추가적 내용은 Garrett Grolemund, Hadley Wickham의 R for Data Science예제와 실습과제를 이용하여 살펴 보시기 바랍니다 . stringr을 이용해서 문자 변수를 핸들링하게 되는데 이때 알야될 부분은 Regular Expressions이고 이는 아래 cheat sheet을 살펴보면서 응용하시면 됩니다. 참고로 Rstudio에서 여러 cheat sheet을 제공하는데 (https://rstudio.com/resources/cheatsheets/) 필요에 따라 사용하시기 바랍니다. 우선 Stringr Cheat Sheet는 https://github.com/rstudio/cheatsheets/raw/master/strings.pdf 여기서 다운로드받으세요."
  },
  {
    "objectID": "410_wrangling_text_01.html#동영상",
    "href": "410_wrangling_text_01.html#동영상",
    "title": "14  문자값 다루기",
    "section": "14.1 동영상",
    "text": "14.1 동영상"
  },
  {
    "objectID": "410_wrangling_text_01.html#문제-제기",
    "href": "410_wrangling_text_01.html#문제-제기",
    "title": "14  문자값 다루기",
    "section": "14.1 문제 제기",
    "text": "14.1 문제 제기\n아래와 같은 파일을 불러와 봅시다. 자료는 url에 있습니다.\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(readxl)\nlibrary(DT)\n\n\nurl = \"https://raw.githubusercontent.com/jinhaslab/opendata/main/data/exp00.xlsx\"\ndownload.file(url, \"data/exp00.xlsx\")\nexp1 &lt;- read_xlsx('data/exp00.xlsx')\n\n아래 데이터를 살펴보면 exposure 변수에 값들이 있고, 이 값들은 ,를 이용하여 구분되고 있습니다. 우리가 자주 사용하는 데이터를 입력하는데 있어서 여러 에러를 없애기 위해 상기 방법을 사용하고는 합니다. 그런데 막상 분석을 수행하여 망간에 노출된 사람이 몇명인가를 어떻게 알수 있을까요? 가장 쉬운 방법은 str_detect()를 사용하는 것입니다. 그러나 소음은, 톨루엔은, 분진은, 염화비닐은 이렇게 늘어나는 경우에는 어떻게 하는 것이 좋을 까요 그것은 long form 또는 wide form으로 데이터를 만드는 것입니다.\n\nhead(exp1$exposure)\n\n[1] \"망간및그무기화합물함유제제,산화아연(흄)함유제제,크롬과그무기화합물(수용성 6가크롬),소음,용접흄\"                                \n[2] \"망간및그무기화합물함유제제,산화아연(흄)함유제제,크롬과그무기화합물(수용성 6가크롬),소음,용접흄\"                                \n[3] \"일반검진,망간및그무기화합물함유제제,산화아연(흄)함유제제,산화철분진과흄함유제제,크롬과그무기화합물(수용성 6가크롬),소음,용접흄\"\n[4] \"일반검진,망간및그무기화합물함유제제,산화아연(흄)함유제제,산화철분진과흄함유제제,크롬과그무기화합물(수용성 6가크롬),소음,용접흄\"\n[5] \"일반검진\"                                                                                                                      \n[6] \"일반검진,메틸알코올,포름알데히드,방사선\"                                                                                       \n\n\n우선 한 변수 안에 여러 변수값(value)가 ,로 구분되어 있는 것을 여러 변수 또는 여러 독립된 변수값으로 변형해 보겠습니다."
  },
  {
    "objectID": "410_wrangling_text_01.html#split",
    "href": "410_wrangling_text_01.html#split",
    "title": "14  문자값 다루기",
    "section": "14.2 split",
    "text": "14.2 split\nsplit를 사용하여 ,로 이루어진 변수를 나누어 보겠습니다. split는 문자를 다룰 때 가장 많이 사용하는 명령문 중 하나입니다. 이번 시간에는 tidyverse 패키지와의 통일을 위해 str_split을 사용하겠습니다. fruits에 사과와 딸기와 오렌지라는 문장이 있고, 이때 와를 기준으로 단어를 나누어 보겠습니다.\n\nfruits &lt;- c(\n  \"사과와 딸기와 오렌지 \"\n)\nsplit_sentence &lt;- str_split(fruits, \"와\" )\nsplit_sentence\n\n[[1]]\n[1] \"사과\"     \" 딸기\"    \" 오렌지 \"\n\n\n같은 방법으로 위 exposure자료를 ,를 기준으로 나누어 보겠습니다. 아래에서 , 를 기준으로 나누었을때 첫번째 항이 일반검진과 망간(분진및 그화합물)로 나누어 진것을 볼 수 있습니다.\n\nstr_split(exp1$exposure, \",\", simplify = TRUE) %&gt;%\n  head()\n\n     [,1]                         [,2]                        \n[1,] \"망간및그무기화합물함유제제\" \"산화아연(흄)함유제제\"      \n[2,] \"망간및그무기화합물함유제제\" \"산화아연(흄)함유제제\"      \n[3,] \"일반검진\"                   \"망간및그무기화합물함유제제\"\n[4,] \"일반검진\"                   \"망간및그무기화합물함유제제\"\n[5,] \"일반검진\"                   \"\"                          \n[6,] \"일반검진\"                   \"메틸알코올\"                \n     [,3]                                 [,4]                    \n[1,] \"크롬과그무기화합물(수용성 6가크롬)\" \"소음\"                  \n[2,] \"크롬과그무기화합물(수용성 6가크롬)\" \"소음\"                  \n[3,] \"산화아연(흄)함유제제\"               \"산화철분진과흄함유제제\"\n[4,] \"산화아연(흄)함유제제\"               \"산화철분진과흄함유제제\"\n[5,] \"\"                                   \"\"                      \n[6,] \"포름알데히드\"                       \"방사선\"                \n     [,5]                                 [,6]   [,7]     [,8] [,9] [,10] [,11]\n[1,] \"용접흄\"                             \"\"     \"\"       \"\"   \"\"   \"\"    \"\"   \n[2,] \"용접흄\"                             \"\"     \"\"       \"\"   \"\"   \"\"    \"\"   \n[3,] \"크롬과그무기화합물(수용성 6가크롬)\" \"소음\" \"용접흄\" \"\"   \"\"   \"\"    \"\"   \n[4,] \"크롬과그무기화합물(수용성 6가크롬)\" \"소음\" \"용접흄\" \"\"   \"\"   \"\"    \"\"   \n[5,] \"\"                                   \"\"     \"\"       \"\"   \"\"   \"\"    \"\"   \n[6,] \"\"                                   \"\"     \"\"       \"\"   \"\"   \"\"    \"\"   \n     [,12] [,13] [,14]\n[1,] \"\"    \"\"    \"\"   \n[2,] \"\"    \"\"    \"\"   \n[3,] \"\"    \"\"    \"\"   \n[4,] \"\"    \"\"    \"\"   \n[5,] \"\"    \"\"    \"\"   \n[6,] \"\"    \"\"    \"\"   \n\n\n다만 몇가지 NA가 보이고 있습니다. NA는 문자이므로 ““를 통해 빈칸으로 만들어 주겠습니다.\n\nexp.m &lt;-str_split(exp1$exposure, \",\", simplify = TRUE) %&gt;% \n  as.data.frame() %&gt;%\n  #  na_if(., \"\") na_if 가 error 가 있어 아래의 코드로 바꿨습니다 .\n  mutate_all(~ifelse(. == \"\", NA, .))\ndatatable(head(exp.m[, 1:5]))\n\n\n\n\n\n\n이렇게 만들어 진 파을 처음 파일과 합치도록 하겠습니다.\n\nexp2 &lt;- cbind(exp1, exp.m) \nexp3 &lt;- exp2 %&gt;%\n  select(-exposure)\n\ndatatable(head(exp3[, 1:6]) )"
  },
  {
    "objectID": "410_wrangling_text_01.html#gather-pivot_longer",
    "href": "410_wrangling_text_01.html#gather-pivot_longer",
    "title": "14  문자값 다루기",
    "section": "14.3 gather (pivot_longer)",
    "text": "14.3 gather (pivot_longer)\nvisiting occurrence,data, id에 대해서 long form 데이터를 만들어 분석하기 쉽게 하도록 하겠습니다. 무언가 좀더 자주 보는 모양의 데이터가 되었습니다.\n\nexp4 &lt;- exp3 %&gt;% \n  pivot_longer(!c(vo, sdate, id), \n         names_to = 'Vs', values_to = 'exposure') \nhead(exp4)\n\n# A tibble: 6 × 5\n     vo sdate         id Vs    exposure                          \n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                             \n1 48064 2008-03-21     1 V1    망간및그무기화합물함유제제        \n2 48064 2008-03-21     1 V2    산화아연(흄)함유제제              \n3 48064 2008-03-21     1 V3    크롬과그무기화합물(수용성 6가크롬)\n4 48064 2008-03-21     1 V4    소음                              \n5 48064 2008-03-21     1 V5    용접흄                            \n6 48064 2008-03-21     1 V6    &lt;NA&gt;                              \n\n\n이때 발생된 NA는 빈칸을 억지로 가져오면서 생긴 것이기 때문에 필요가 없고, Vs 또한 필요한 부분이 아닙니다. 따라서 na.omit()을 이용하여 지우도록 하겠습니다.\n\nexp4 &lt;- exp3 %&gt;% \n  pivot_longer(!c(vo,sdate,id), \n         names_to = 'Vs', values_to = 'exposure') %&gt;%\n  select(-Vs) %&gt;%\n  na.omit()\nhead(exp4)\n\n# A tibble: 6 × 4\n     vo sdate         id exposure                          \n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                             \n1 48064 2008-03-21     1 망간및그무기화합물함유제제        \n2 48064 2008-03-21     1 산화아연(흄)함유제제              \n3 48064 2008-03-21     1 크롬과그무기화합물(수용성 6가크롬)\n4 48064 2008-03-21     1 소음                              \n5 48064 2008-03-21     1 용접흄                            \n6 48065 2008-03-21     1 망간및그무기화합물함유제제        \n\n\n자 이제 ID가 1인 사람을 관찰해 보겠습니다. 2008년 3월과 2008년 7월에 용접흄에 노출되었네요. 좀 보기 편해졌습니다.\n\nexp4 %&gt;%\n  filter(id ==1)\n\n# A tibble: 24 × 4\n      vo sdate         id exposure                          \n   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                             \n 1 48064 2008-03-21     1 망간및그무기화합물함유제제        \n 2 48064 2008-03-21     1 산화아연(흄)함유제제              \n 3 48064 2008-03-21     1 크롬과그무기화합물(수용성 6가크롬)\n 4 48064 2008-03-21     1 소음                              \n 5 48064 2008-03-21     1 용접흄                            \n 6 48065 2008-03-21     1 망간및그무기화합물함유제제        \n 7 48065 2008-03-21     1 산화아연(흄)함유제제              \n 8 48065 2008-03-21     1 크롬과그무기화합물(수용성 6가크롬)\n 9 48065 2008-03-21     1 소음                              \n10 48065 2008-03-21     1 용접흄                            \n# ℹ 14 more rows"
  },
  {
    "objectID": "410_wrangling_text_01.html#spread-pivot_wider",
    "href": "410_wrangling_text_01.html#spread-pivot_wider",
    "title": "14  문자값 다루기",
    "section": "14.4 spread (pivot_wider)",
    "text": "14.4 spread (pivot_wider)\n이를 wide form으로 만들어 보겠습니다 .\n\nexp5&lt;-exp4 %&gt;%\n  mutate(values = 1) %&gt;%\n  pivot_wider(names_from = exposure, \n              values_from = values, \n              values_fill = 0)\nexp5 %&gt;% head()\n\n# A tibble: 6 × 76\n      vo sdate         id 망간및그무기화합물함유제제 `산화아연(흄)함유제제`\n   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;                      &lt;dbl&gt;                  &lt;dbl&gt;\n1  48064 2008-03-21     1                          1                      1\n2  48065 2008-03-21     1                          1                      1\n3  62526 2008-07-02     1                          1                      1\n4  62527 2008-07-02     1                          1                      1\n5 174043 2011-12-22     2                          0                      0\n6 306072 2014-12-09     3                          0                      0\n# ℹ 71 more variables: `크롬과그무기화합물(수용성 6가크롬)` &lt;dbl&gt;, 소음 &lt;dbl&gt;,\n#   용접흄 &lt;dbl&gt;, 일반검진 &lt;dbl&gt;, 산화철분진과흄함유제제 &lt;dbl&gt;,\n#   메틸알코올 &lt;dbl&gt;, 포름알데히드 &lt;dbl&gt;, 방사선 &lt;dbl&gt;, 기타광물성분진 &lt;dbl&gt;,\n#   `니켈(불용성무기화합물)` &lt;dbl&gt;, `주석(금속)` &lt;dbl&gt;, 염화수소 &lt;dbl&gt;,\n#   질산 &lt;dbl&gt;, 황산 &lt;dbl&gt;, 자외선 &lt;dbl&gt;, 기타금속가공유 &lt;dbl&gt;, 생애검진 &lt;dbl&gt;,\n#   아크릴아미드 &lt;dbl&gt;, 엑스선 &lt;dbl&gt;, IPA &lt;dbl&gt;, `염화수소(염산)` &lt;dbl&gt;,\n#   망간및그무기화합물 &lt;dbl&gt;, `메틸에틸케톤(MEK)` &lt;dbl&gt;, 메틸에틸케톤 &lt;dbl&gt;, …\n\n\n이제 id 1인 근로자의 산화철분진 노출을 관찰해 보겠습니다. id 1인 근로자는 언제 부터 산화철 분진에 노출되었나요? 2008년 7월부터 입니다.\n\nexp5 %&gt;%\n  select(sdate, id, contains(\"산화철분진\")) %&gt;%\n  filter(id ==1)\n\n# A tibble: 4 × 3\n  sdate         id 산화철분진과흄함유제제\n  &lt;chr&gt;      &lt;dbl&gt;                  &lt;dbl&gt;\n1 2008-03-21     1                      0\n2 2008-03-21     1                      0\n3 2008-07-02     1                      1\n4 2008-07-02     1                      1\n\n\n저는 데이터 클리닝과 visualization에서는 주로 long form을 데이터 분석에서는 wide form을 사용하고 있습니다."
  },
  {
    "objectID": "410_wrangling_text_01.html#stringr",
    "href": "410_wrangling_text_01.html#stringr",
    "title": "14  문자값 다루기",
    "section": "14.5 stringr",
    "text": "14.5 stringr\nStringr부분은 강의한다기 보다는 필요한 시기에 cheat를 찾아서 실행해 본다가 맞는 것 같습니다. 다만 여기서 몇가지 실습을 통해 익히고, 앞서 이야기한 cheat를 사용해서 필요한 경우 사용하면 되겠습니다.\n\n14.5.1 분진 노출자 찾기\n여러 분진에 노출되는 근로자를 찾아보고 싶습니다. 어떤 데이터에서 찾으면 좋을 까요? 이때 사용할 코드는 str_detect입니다. 구별을 위해 long 과 wide 파일을 만들겠습니다.\n\nlong &lt;-exp4\nwide &lt;-exp5\n\nstr_detect는 어떤 변수에 특정 문장이 있으면 TRUE, 아니면 FALSE를 돌려주는 함수 입니다.\n\nfruit &lt;- c('apple', 'banna', 'apple-banna')\nstr_detect(fruit, 'apple')\n\n[1]  TRUE FALSE  TRUE\n\nstr_detect(fruit, '-')\n\n[1] FALSE FALSE  TRUE\n\n\n이를 이용해서 long form에서 찾아 보겠습니다. 10명이 노출되고 있네요.\n\nlong %&gt;%\n  filter(str_detect(exposure, \"분진\")) %&gt;%\n  pull(id) %&gt;%\n  unique() %&gt;%\n  length()\n\n[1] 10\n\n\n그럼 어떤 분진인지 보겠습니다. 산화철분진, 광물성분진이었네요.\n\nlong %&gt;%\n  filter(str_detect(exposure, \"분진\")) %&gt;%\n  pull(exposure) %&gt;%\n  unique() \n\n[1] \"산화철분진과흄함유제제\" \"기타광물성분진\"        \n\n\n\n\n14.5.2 특정 문자 변환\n찾아보니 기타라는 단어가 있어 혼란이 있네요, 그리고 함유제제와 화합물이라는 것은 필요 없는 단어라고 판단됩니다. 이를 찾아서 지워보겠습니다. str_replace_all을 이용합니다.\n\nlong %&gt;%\n  mutate(exposure2=str_replace_all(exposure, \"기타|함유제제|화합물|및그무기\", \"\")) %&gt;%\n  filter(str_detect(exposure, '납|분진')) %&gt;%\n  select(exposure, exposure2) %&gt;%\n  unique()\n\n# A tibble: 5 × 2\n  exposure                     exposure2     \n  &lt;chr&gt;                        &lt;chr&gt;         \n1 산화철분진과흄함유제제       산화철분진과흄\n2 기타광물성분진               광물성분진    \n3 납(연)및그무기화합물함유제제 납(연)        \n4 납(연)및그무기화합물         납(연)        \n5 납.납화합물                  납.납         \n\n\n\n\n14.5.3 과제\nR 코드를 작성해 보세요\n\n“납”의 노출 종류는 몇가지 인가요? 힌트 long %&gt;% filter(str_detect(??, ??)) %&gt;% pull(??) %&gt;% unique()\n\n\n\n[1] \"납(연)및그무기화합물함유제제\" \"납(연)및그무기화합물\"        \n[3] \"납.납화합물\"                 \n\n\n\n납 노출이 일어난 날짜(sdate)를 찾으세요\n\n\n\n[1] \"2007-07-19\" \"2009-07-13\" \"2010-07-12\" \"2011-04-14\" \"2012-05-07\"\n[6] \"2013-05-27\" \"2014-05-26\" \"2015-04-27\"\n\n\n\nid 별 소음에 노출된 횟수를 구하고 이를 ggplot(aes(x=n)) + geom_bar()를 이용해 그리세요.\n\n\n\n\n\n\n\n숫자가 포함된 인자를 찾아보세요, 숫자는 \\\\d로 표기합니다. d 는 digit이란 뜻이고 \\\\는 그 뒤에 오는 것이 문자가 아니라 약속된 기호라는 뜻입니다. 예를 들면 아래와 같습니다. ^가 의미하는 것은 무엇일까요?. $가 뜻하는 것은 무엇일까요. 아래와 같습니다.\n\n\nfruits &lt;- c('apple2', 'apple', 'a4ple', '4apple', 'b4ple')\nstr_detect(fruits, '\\\\d')\n\n[1]  TRUE FALSE  TRUE  TRUE  TRUE\n\nstr_detect(fruits, '^\\\\d')\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\nstr_detect(fruits, '\\\\d$')\n\n[1]  TRUE FALSE FALSE FALSE FALSE\n\nstr_detect(fruits, 'a\\\\d')\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\n\n자 그럼 숫자가 포함된 유해 물질을 찾아 봅시다. 이렇게 결과가 나오게 해보세요!. 아직 1이 있는 사람이 있나요? 숫자만 혼자 있는 경우도 지워보세요.\n\n\n\n# A tibble: 6 × 1\n  exposure                          \n  &lt;chr&gt;                             \n1 크롬과그무기화합물(수용성 6가크롬)\n2 야간작업(월평균4회)               \n3 크롬과그무기화합물(불용성 6가크롬)\n4 2-부톡시에탄올                    \n5 3-부타디엔                        \n6 크롬(6가)화합물(불용성무기화합물) \n\n\n\n숫자로 시작되는 유해물질을 찾아보세요\n\n\n\n# A tibble: 2 × 1\n  exposure      \n  &lt;chr&gt;         \n1 2-부톡시에탄올\n2 3-부타디엔"
  },
  {
    "objectID": "510_webscraping_01.html#issue-of-covid19-in-korea",
    "href": "510_webscraping_01.html#issue-of-covid19-in-korea",
    "title": "15  웹 스크래핑 I",
    "section": "15.1 issue of COVID19 in Korea",
    "text": "15.1 issue of COVID19 in Korea\n아래의 URL에서 데이터를 다운로드 받아 보려고 합니다 . https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory\n우선 위 주소로 가보겠습니다.\n\n\n\nwiki covid\n\n\n\nurl &lt;-\"https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory\"\n\n“read_html()은 URL과 그 내용을 읽을 수 있게 해줍니다.”\n\nh &lt;-read_html(url)\nclass(h)\n\n[1] \"xml_document\" \"xml_node\"    \n\nh\n\n{html_document}\n&lt;html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-typography-survey-disabled vector-toc-available\" lang=\"en\" dir=\"ltr\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body class=\"skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr ...\n\n\n\n#html_text(h) # just overview of HTML structure\n\n크롬을 통해 웹사이트에 접속하고 F12 버튼을 클릭하세요. 아래와 같이 오른쪽 창이 나타납니다. 이제 저는 Covid-19 표를 찾고 싶습니다. ctrl + F를 사용하여 검색 탭을 엽니다. 그리고 india를 입력하여 해당 표를 찾습니다.\n\n\n\nFind source and nodes\n\n\nURL에서 table 노드를 찾으려고 합니다.\n\ntab &lt;- h %&gt;% html_nodes(\"table\")\n\n아래에서 보는 것 처럼 tab[[2]] 일때도 있고, tab[[13]]일 때도 있습니다. 매번 page가 바뀌면 우리도 바꿔줘야 합니다.\n\ntab2 &lt;- tab[[13]] %&gt;% html_table\n\n\n#openxlsx::write.xlsx(tab2, 'data/websc/tab2.xlsx')\n#tab2 &lt;- read_xlsx('data/websc/tab2.xlsx')\ntab2\n\n# A tibble: 240 × 5\n   ``    Country                `Deaths / million` Deaths    Cases      \n   &lt;chr&gt; &lt;chr&gt;                  &lt;chr&gt;              &lt;chr&gt;     &lt;chr&gt;      \n 1 \"\"    World[a]               874                6,978,162 771,820,173\n 2 \"\"    Peru                   6,511              221,727   4,522,474  \n 3 \"\"    Bulgaria               5,670              38,456    1,307,688  \n 4 \"\"    Bosnia and Herzegovina 5,060              16,364    403,293    \n 5 \"\"    Hungary                4,898              48,828    2,211,136  \n 6 \"\"    North Macedonia        4,752              9,949     349,618    \n 7 \"\"    Georgia                4,575              17,132    1,855,289  \n 8 \"\"    Croatia                4,574              18,438    1,276,497  \n 9 \"\"    Slovenia               4,465              9,467     1,346,628  \n10 \"\"    Montenegro             4,232              2,654     251,280    \n# ℹ 230 more rows\n\n\nCases, Death, 그리고 Recover에서 ’,’를 제거하고 숫자 변수로 만듭니다.\n\ntab3= tab2 %&gt;% select(2, 3, 4, 5) %&gt;%\n  mutate(across(-Country, ~str_replace_all(., \",\", \"\") %&gt;% as.numeric())) %&gt;%\n  mutate(Country = str_replace_all(Country, '\\\\[[:alpha:]]', \"\")) %&gt;%\n  na.omit() %&gt;%\n  rename(Mortality = `Deaths / million`)\n\nI used \\\\[[:alpha:]], \\\\[ means “[” and [:aplpah:] means any alphabet, and last ] means “]”. So, I try to remove the all character within “[ ]”. Now, Table is.\n\ntab3 %&gt;% datatable()\n\n\n\n\n\n\n\nstd = sd(tab3$Cases)\nmean = mean(tab3$Cases)\nfigs &lt;-tab3 %&gt;%\n  filter(Cases &lt; mean + 2*std, \n         Cases &gt; 2000) %&gt;%\n  ggplot(aes(x=Cases, y = Mortality, size = Deaths)) +\n  geom_point() +\n  scale_x_continuous(trans=\"log\") +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), se=F)\n\nfigs"
  },
  {
    "objectID": "510_webscraping_01.html#homework",
    "href": "510_webscraping_01.html#homework",
    "title": "15  웹 스크래핑 I",
    "section": "15.2 homework",
    "text": "15.2 homework\n\n15.2.1 download Cumulative covid19 death\nDownload data table from url. You can use tab[[ i ]] code to find cumulative covid19 death. The taret Table in web looks like that.\nhint\n\ntab4&lt;-tab[[?]] %&gt;% html_table(fill = TRUE) \n\n and the table file is\n\n\n\n\n\n\n\n\n\n15.2.2 UK, Italy, France, Spain, USA, Canada\nselect countris of “UK, Italy, France, Spain, USA, Canada” and plot the trends. and upload the final plot in dspubs.org tutor\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nstep1:\ncreate Month_mortatlity data filter countries names of above\n\n\nstep2:\nchage character data to numeric data\n\n\nstep3:\npivot data to long form\n\n\nstep4:\nplot the graph!\n\n\n\nStep 1 and 2\n\nMonth_mortality %&gt;% datatable()\n\n\n\n\n\n\nstep 3\n\nlong_death %&gt;% datatable()\n\n\n\n\n\n\nstep 4\n\n\n[1] \"LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=ko_KR.UTF-8;LC_PAPER=ko_KR.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=ko_KR.UTF-8;LC_IDENTIFICATION=C\""
  },
  {
    "objectID": "510_webscraping_01.html#review-of-title-from-google-scholar",
    "href": "510_webscraping_01.html#review-of-title-from-google-scholar",
    "title": "15  웹 스크래핑 I",
    "section": "15.3 Review of title from google scholar",
    "text": "15.3 Review of title from google scholar\n\n15.3.1 googl scholar\nSearch the My name of “Jin-Ha Yoon” in google scholar. The url is https://scholar.google.com/citations?hl=en&user=FzE_ZWAAAAAJ&view_op=list_works&sortby=pubdate\n\nurl &lt;- \"https://scholar.google.com/citations?hl=en&user=FzE_ZWAAAAAJ&view_op=list_works&sortby=pubdate\"\n\nstep1 read the html using url address\n\nlibrary(rvest)\ngs &lt;- read_html(url)\n\nstep2 filter title using nodes and text, and make data.frame\n\ndat&lt;-gs %&gt;% html_nodes(\"tbody\") %&gt;%\n  html_nodes(\"td\") %&gt;%\n  html_nodes(\"a\") %&gt;%\n  html_text() %&gt;%\n  data.frame()\n\n\nif(!require(\"tm\")) install.packages(\"tm\")\nif(!require(\"SnowballC\")) install.packages(\"SnowballC\")\n#if(!require(\"wordcloud\")) install.packages(\"wordcloud\")\nif(!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nif(!require(\"tidytext\")) install.packages(\"tidytext\")\nif(!require(\"stringr\")) install.packages(\"stringr\")\nif(!require(\"knitr\")) install.packages(\"knitr\")\nif(!require(\"DT\")) install.packages(\"DT\")\n#library(wordcloud)\n\nstep3 split the words (tokenizing) using packages or user own methods.\n\ndat &lt;- dat %&gt;%\n  setNames(c(\"titles\"))\ntokens &lt;-dat %&gt;%\n  unnest_tokens(word, titles) %&gt;%\n  count(word, sort = TRUE)%&gt;%\n  ungroup()\n\ntokens2 &lt;- str_split(dat$titles, \" \", simplify = TRUE) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(!c(id), names_to = 'Vs', values_to = 'word') %&gt;%\n  select(-Vs) %&gt;%\n  filter(!word==\"\") %&gt;%\n  count(word, sort = TRUE)%&gt;%\n  ungroup()\n\nstep4 import lookup data for removing words\n\ndata(\"stop_words\") # we should add user own words.\nstop_words %&gt;% datatable()\n\n\n\n\n\n\nstep5 remove stop words and numbers\n\ntokens_clean &lt;- tokens %&gt;%\n  anti_join(stop_words, by = c(\"word\")) %&gt;%\n  filter(!str_detect(word, \"^[[:digit:]]\")) %&gt;%\n  filter(!str_detect(word, \"study|korea\"))\n\nstep6 create word cloud\n\nset.seed(1)\npal &lt;- brewer.pal(12, \"Paired\")\ntokens_clean %&gt;% \n  with(wordcloud(word, n, random.order = FALSE, colors=pal))\n\n\n\n\nwordcloud"
  },
  {
    "objectID": "510_webscraping_01.html#home-work-2",
    "href": "510_webscraping_01.html#home-work-2",
    "title": "15  웹 스크래핑 I",
    "section": "15.4 home work 2",
    "text": "15.4 home work 2\nSearch you own word in google scholar. for example, You can search “Suicide” or “Hypertension” in google scholar. And, upload your word cloud to google classroom."
  },
  {
    "objectID": "510_webscraping_01.html#black-report-2",
    "href": "510_webscraping_01.html#black-report-2",
    "title": "15  웹 스크래핑 I",
    "section": "15.5 Black Report 2",
    "text": "15.5 Black Report 2\n\n\nplease visit “https://www.sochealth.co.uk/national-health-service/public-health-and-wellbeing/poverty-and-inequality/the-black-report-1980/the-black-report-2-the-evidence-of-inequalities-in-health/”. That is black report 2, and I need some visualization to present health inequality. Let’s start!.\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(DT)\n\nGet url, save html from url and find tag of “table”. Review the table 5 using html_table(), and datatable().\n\nurl &lt;- \"https://www.sochealth.co.uk/national-health-service/public-health-and-wellbeing/poverty-and-inequality/the-black-report-1980/the-black-report-2-the-evidence-of-inequalities-in-health/\"\nh &lt;-read_html(url)\ntab &lt;- h %&gt;% html_nodes(\"table\")\ntab[[5]] %&gt;% html_table() %&gt;% DT::datatable()\n\n\n\n\n\n\nThe source and gender share same column, hence, I want divided that into two columns. the col names are changed by setNames. The gender variable was reshaped when that have any word of males or female. code of fill fill the missing row as very next values, in other word, fill code make html table to data frame table.\n\ntab[[5]] %&gt;% html_table() %&gt;%\n  setNames(c('source',  paste0('class', 1:6), 'all', 'ratio')) %&gt;%\n  mutate(gender = case_when(\n    source == 'Males' ~ 'Males', \n    str_detect(source, 'Females') ~ 'Females', \n    TRUE ~ \"\"\n  )) %&gt;%\n  select(source, gender, class1:class6, all, ratio) %&gt;%\n  mutate(source = case_when(\n    str_detect(source, 'Males|Females') ~ \"\",\n    TRUE ~ source\n  )) %&gt;%\n  mutate(source = ifelse(source ==\"\", NA, source)) %&gt;%\n  fill(source, .direction = \"down\") %&gt;%\n  filter(!gender ==\"\") -&gt; tab5\ntab5 %&gt;% DT::datatable()\n\n\n\n\n\n\nPlot the bar plot\n\ntab5 %&gt;%\n  pivot_longer(-c(source, gender), names_to = 'variables', values_to = 'value') %&gt;% \n  filter(!variables %in% c('all', 'ratio') ) %&gt;%\n  mutate(variables = factor(variables, \n                        level = c(paste0('class', 1:6)))) %&gt;%\n  mutate(value = as.numeric(value)) %&gt;%\n  mutate(source = str_replace(source, \"per\", \"\\n per\")) %&gt;%\n  ggplot(aes(x=variables, y=value, color=gender, fill=gender, group=gender)) +\n  geom_bar(stat='identity', aes(color = gender, fill=gender)) +\n  facet_grid(source~gender, scale = 'free') +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_blank())+\n  theme(strip.text.y.right = element_text(angle = 0, hjust = 0), \n        axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 1)) +\n  guides(color = \"none\", fill = \"none\") \n\n\n\ntab5.1 &lt;- tab5 %&gt;% filter(str_detect(source, \"Stillbirths\")) %&gt;% select(-all, -ratio)\n\nplot the table 6 using same methods of table 5\n\ntab[[6]] %&gt;% html_table() %&gt;%\n  tibble() %&gt;%\n  setNames(c('source', paste0('class', 1:6), 'all', 'ratio')) %&gt;%\n  filter(source == 'SMR') %&gt;%\n  mutate(gender = c('Males', 'Females')) %&gt;% \n  select(-all, -ratio) %&gt;%\n  pivot_longer(-c(source, gender), names_to = 'variables', values_to = 'value') %&gt;%\n  mutate(value= as.numeric(value)) %&gt;%\n  ggplot(aes(x=variables, y = value)) +\n  geom_bar(stat='identity', aes(fill=gender, color=gender)) +\n  facet_grid(~gender) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_blank()) +\n  guides(fill =\"none\", color =\"none\") +\n  ggtitle(\"Child\")\n\n\n\ntab[[6]] %&gt;% html_table() %&gt;%\n  tibble() %&gt;%\n  setNames(c('source', paste0('class', 1:6), 'all', 'ratio')) %&gt;%\n  filter(source == 'SMR') %&gt;%\n  mutate(gender = c('Males', 'Females')) %&gt;%\n  mutate(source = \"Childhood Mortality (SMR)\") %&gt;%\n  select(names(tab5.1)) -&gt; tab6\ntab6\n\n# A tibble: 2 × 8\n  source                    gender  class1 class2 class3 class4 class5 class6\n  &lt;chr&gt;                     &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n1 Childhood Mortality (SMR) Males   74     79     95     98     112    162   \n2 Childhood Mortality (SMR) Females 89     84     93     93     120    156   \n\n\nRepeat reshaping for Table 7.\n\ntab[[7]] %&gt;% html_table() %&gt;% tibble() %&gt;%\n  setNames(c('source', paste0('class', 1:6),  'ratio')) %&gt;%\n  filter(str_detect(source, 'Men|men')) %&gt;%\n  mutate(gender = source) %&gt;%\n  mutate(source = \"Adult (16-64) SMR\") %&gt;%\n  select(names(tab5.1)) %&gt;%\n  slice(-3) -&gt; tab7\n\nThe final graph for black report 2 presentation as below.\n\nrbind(tab5.1, tab6, tab7) %&gt;%\n  tibble() %&gt;%\n  mutate(source = str_replace(source, \"per\", \"(mortality) \\n per\")) %&gt;%\n  mutate(gender = ifelse(str_detect(gender, 'women'), 'Females',\n                         ifelse(str_detect(gender, 'Men'), 'Males', gender))) %&gt;%\n  pivot_longer(-c(source, gender), names_to = 'class', values_to = 'SMR') %&gt;%\n  mutate(SMR = as.numeric(SMR)) %&gt;%\n  ggplot(aes(x=class, y=SMR)) +\n  geom_bar(stat='identity', aes(fill=gender, color=gender)) +\n  facet_grid(source ~gender, scale='free')+\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_blank()) +\n  theme_minimal()+\n  theme(strip.text.y.right = element_text(angle = 0, hjust = 0)) +\n \n  guides(fill =\"none\", color =\"none\")"
  },
  {
    "objectID": "510_webscraping_01.html#home-work-2-1",
    "href": "510_webscraping_01.html#home-work-2-1",
    "title": "15  웹 스크래핑 I",
    "section": "15.6 home work 2",
    "text": "15.6 home work 2\nBlack report 중에 관심있는 표를 visualization 해 주세요"
  },
  {
    "objectID": "610_webscraping_02.html#pubmed-검색-방법",
    "href": "610_webscraping_02.html#pubmed-검색-방법",
    "title": "16  PubMed 에서 R 과 EndNote",
    "section": "16.1 PubMed 검색 방법",
    "text": "16.1 PubMed 검색 방법\nPubMed 검색 방법의 참고 문헌은 질병관리청의 미세먼지 근거보고서의 검색전략 관련 보고서입니다.\n보고서 중에 심혈관질환의 검색 전략을 PDF는 아래와 같습니다.\n\n\n\n\n\n각 Query 를 PubMed 에서 고급검색을 통해 지속합니다. 여기서 [MESH] 는 keyword 검색 [TIAB]는 제목(TItle)과 초록(ABstract)에서 검색한다는 의미입니다. 연세대학교 도서관의 pubmed 검색 문서를 꼭 읽어 보세요. PubMed 검색 연세대\n\n\n\n\n\n질병관리 본문 검색전략을 따라 모두 검색을 수행한 후 검색 History에 있는 Query 문을 가져오는 것이 첫번째 단계입니다. 모두 글어 복사하고 이제 Rstudio를 열어 진행해 보겠습니다."
  },
  {
    "objectID": "610_webscraping_02.html#r-에서-pubmed-query-사용",
    "href": "610_webscraping_02.html#r-에서-pubmed-query-사용",
    "title": "16  PubMed 에서 R 과 EndNote",
    "section": "16.2 R 에서 PubMed Query 사용",
    "text": "16.2 R 에서 PubMed Query 사용\n필요한 라이브러리를 가져오겠습니다.\n\nlibrary(tidyverse)\nlibrary(XML)\nlibrary(xml2)\nlibrary(methods)\nlibrary(easyPubMed)\nlibrary(parallel)\nlibrary(tictoc)\n\n여기서 easyPubMED로 검색을 수행하고, xml 형식의 데이터를 우리가 관리하기 쉬운 엑셀형식으 데이터로 tidyverse를 이용해서 분석합니다. parallel와 tictoc은 병렬연산과 시간계산을 위해 하는 것이고, 이번 실습에는 꼭 필요한 것은 아닙니다.\n\nQuery 불러오기\n\n\n# get query create start -------------- \nmy_query = '(((\"Air Pollution\"[MeSH Terms] OR (\"Air Pollution\"[Title/Abstract] OR \"Air Pollutions\"[Title/Abstract] OR \"air contamination\"[Title/Abstract] OR \"atmosphere pollution\"[Title/Abstract] OR \"atmospheric pollution\"[Title/Abstract] OR \"polluted air\"[Title/Abstract] OR \"polluted atmosphere\"[Title/Abstract] OR \"pollution air\"[Title/Abstract] OR \"air pollutant\"[Title/Abstract] OR \"air pollutants\"[Title/Abstract]) OR \"Particulate Matter\"[MeSH Terms] OR \"particulate matter*\"[Title/Abstract]) AND (\"Cardiovascular Diseases\"[MeSH Terms] OR (\"cardiovascular\"[Title/Abstract] OR \"Coronary Artery\"[Title/Abstract] OR \"atheroscleros*\"[Title/Abstract] OR \"heart attack*\"[Title/Abstract] OR \"arrhythmia*\"[Title/Abstract] OR \"blood pressure\"[Title/Abstract] OR \"blood pressures\"[Title/Abstract] OR \"hypertension*\"[Title/Abstract]) OR ((\"myocardial\"[Title/Abstract] OR \"myocardiac\"[Title/Abstract] OR \"cardiac\"[Title/Abstract]) AND (\"ischemic\"[Title/Abstract] OR \"ischemia*\"[Title/Abstract] OR \"infact*\"[Title/Abstract])) OR (\"Electrocardiography\"[MeSH Terms] OR \"electrocardiogr*\"[Title/Abstract] OR \"ECG\"[Title/Abstract] OR \"EKG\"[Title/Abstract]) OR (\"stroke\"[MeSH Terms] OR \"stroke*\"[Title/Abstract] OR \"cerebral infarction*\"[Title/Abstract] OR \"brain infarction*\"[Title/Abstract] OR \"Cerebral Hemorrhage\"[MeSH Terms] OR \"cerebral hemorrhag*\"[Title/Abstract] OR \"brain hemorrhag*\"[Title/Abstract]))) NOT (((\"case reports\"[Publication Type] OR \"hascommenton\"[All Fields] OR \"Editorial\"[Publication Type] OR \"Letter\"[Publication Type] OR \"case report*\"[Title]) AND \"case series*\"[Title]) OR \"comment*\"[Title] OR \"letter*\"[Title])) AND 2010/01/01:3000/12/31[Date - Publication] AND \"English\"[Language] AND (\"Systematic\"[Filter] OR \"meta-analysis\"[Publication Type] OR \"Systematic\"[Title] OR \"literature review*\"[Title] OR \"meta analys*\"[Title] OR \"Guideline\"[Publication Type] OR \"Practice Guideline\"[Publication Type] OR \"guideline*\"[Title] OR \"recommendation*\"[Title] OR \"Health Planning Guidelines\"[MeSH Terms] OR \"Consensus\"[MeSH Terms] OR \"consensus development conference, nih\"[Publication Type] OR \"Consensus Development Conference\"[Publication Type] OR \"Consensus\"[Title] OR \"Evidence-Based Medicine\"[MeSH Terms] OR \"Evidence-Based Medicine\"[Title/Abstract] OR \"cohort studies\"[MeSH Terms] OR \"Cohort\"[Title/Abstract] OR \"randomized controlled trial\"[Publication Type] OR \"random*\"[Text Word]) AND '\n\n커리가 “AND”로 끝난 것을 볼수 있습니다. 미완성입니다. 이렇게 한 이유는 이후 시기를 정해서 불러오려고 합니다. 예를 들어 2010년 1월 1일 부터 2010년 12월 31일까지 불러오겠습니다. 우선 반복문으로 2010년부터 2020년까지 모두 불러올 수 있는 항목을 만들고 2010년에 대해 설명해 보겠습니다.\n\ndate1 = list()\nmy_query_date = list() \nmy_entrez_id = list()\ndate1 = mclapply(2010:2020, \n                 function(i){\n                   paste0(sprintf('%s/01/01:', i), sprintf('%s/12/31[Date - Publication]', i))\n                 })\n\nfor (i in 1:11) { \n  my_query_date[[i]] &lt;- paste0(my_query, date1[[i]] %&gt;% as.character())\n}\n\n여기서 my_query_date[[1]]은 처음 가녀온 my_query에 date1[[1]]을 붙인(paste) 코드입니다. 살펴 보겠습니다.\n\nmy_query_date[[1]]\n\n[1] \"(((\\\"Air Pollution\\\"[MeSH Terms] OR (\\\"Air Pollution\\\"[Title/Abstract] OR \\\"Air Pollutions\\\"[Title/Abstract] OR \\\"air contamination\\\"[Title/Abstract] OR \\\"atmosphere pollution\\\"[Title/Abstract] OR \\\"atmospheric pollution\\\"[Title/Abstract] OR \\\"polluted air\\\"[Title/Abstract] OR \\\"polluted atmosphere\\\"[Title/Abstract] OR \\\"pollution air\\\"[Title/Abstract] OR \\\"air pollutant\\\"[Title/Abstract] OR \\\"air pollutants\\\"[Title/Abstract]) OR \\\"Particulate Matter\\\"[MeSH Terms] OR \\\"particulate matter*\\\"[Title/Abstract]) AND (\\\"Cardiovascular Diseases\\\"[MeSH Terms] OR (\\\"cardiovascular\\\"[Title/Abstract] OR \\\"Coronary Artery\\\"[Title/Abstract] OR \\\"atheroscleros*\\\"[Title/Abstract] OR \\\"heart attack*\\\"[Title/Abstract] OR \\\"arrhythmia*\\\"[Title/Abstract] OR \\\"blood pressure\\\"[Title/Abstract] OR \\\"blood pressures\\\"[Title/Abstract] OR \\\"hypertension*\\\"[Title/Abstract]) OR ((\\\"myocardial\\\"[Title/Abstract] OR \\\"myocardiac\\\"[Title/Abstract] OR \\\"cardiac\\\"[Title/Abstract]) AND (\\\"ischemic\\\"[Title/Abstract] OR \\\"ischemia*\\\"[Title/Abstract] OR \\\"infact*\\\"[Title/Abstract])) OR (\\\"Electrocardiography\\\"[MeSH Terms] OR \\\"electrocardiogr*\\\"[Title/Abstract] OR \\\"ECG\\\"[Title/Abstract] OR \\\"EKG\\\"[Title/Abstract]) OR (\\\"stroke\\\"[MeSH Terms] OR \\\"stroke*\\\"[Title/Abstract] OR \\\"cerebral infarction*\\\"[Title/Abstract] OR \\\"brain infarction*\\\"[Title/Abstract] OR \\\"Cerebral Hemorrhage\\\"[MeSH Terms] OR \\\"cerebral hemorrhag*\\\"[Title/Abstract] OR \\\"brain hemorrhag*\\\"[Title/Abstract]))) NOT (((\\\"case reports\\\"[Publication Type] OR \\\"hascommenton\\\"[All Fields] OR \\\"Editorial\\\"[Publication Type] OR \\\"Letter\\\"[Publication Type] OR \\\"case report*\\\"[Title]) AND \\\"case series*\\\"[Title]) OR \\\"comment*\\\"[Title] OR \\\"letter*\\\"[Title])) AND 2010/01/01:3000/12/31[Date - Publication] AND \\\"English\\\"[Language] AND (\\\"Systematic\\\"[Filter] OR \\\"meta-analysis\\\"[Publication Type] OR \\\"Systematic\\\"[Title] OR \\\"literature review*\\\"[Title] OR \\\"meta analys*\\\"[Title] OR \\\"Guideline\\\"[Publication Type] OR \\\"Practice Guideline\\\"[Publication Type] OR \\\"guideline*\\\"[Title] OR \\\"recommendation*\\\"[Title] OR \\\"Health Planning Guidelines\\\"[MeSH Terms] OR \\\"Consensus\\\"[MeSH Terms] OR \\\"consensus development conference, nih\\\"[Publication Type] OR \\\"Consensus Development Conference\\\"[Publication Type] OR \\\"Consensus\\\"[Title] OR \\\"Evidence-Based Medicine\\\"[MeSH Terms] OR \\\"Evidence-Based Medicine\\\"[Title/Abstract] OR \\\"cohort studies\\\"[MeSH Terms] OR \\\"Cohort\\\"[Title/Abstract] OR \\\"randomized controlled trial\\\"[Publication Type] OR \\\"random*\\\"[Text Word]) AND 2010/01/01:2010/12/31[Date - Publication]\"\n\n\n여기서 AND 2010/01/01:2010/12/31[Date - Publication]가 추가된 것을 볼 수 있습니다. 이렇게 1년씩 11번을 검색해서 데이터로 가져오는 것입니다. 이렇게 하는 이유는 시간을 절약하기 위해서 입니다.\n1번 커리(처음 1년치)만 가져와 보겠습니다. my_entrez_id는 커리이고, mu_abstracts_xml은 검색된 결과는 xml 데이터 형식으로 가져욘 것입니다. 이것을 list로 변경하면 all_xml로 바끼오이를 data frame 형식으로 반복해서 가져옵니디ㅏ. fdf_bas가 됩니다. 이제 이를 횡 병합해서 fdf_bas2로 만듭니다.\n\nmy_entrez_id &lt;- get_pubmed_ids(my_query_date[[1]])\nmy_abstracts_xml &lt;- fetch_pubmed_data(pubmed_id_list = my_entrez_id)\nall_xml &lt;- articles_to_list(my_abstracts_xml)\nfdf_bas = lapply(all_xml, article_to_df, max_chars = -1, getAuthors = FALSE)\nfdf_bas2 = do.call(rbind, fdf_bas) %&gt;% select(-keywords, -lastname, -firstname, -address, -email)\n\nfdf_bas2를 살펴보겠습니다. 보기 편하게 하기 위해 초록은 제외하고 보겠습니다. 잘 가져와 졌습니다.\n\nDT::datatable(fdf_bas2 %&gt;% select(-abstract))\n\n\n\n\n\n\n이제 가져온 1번 커리의 xml데이터, 그 중에서 초록과 id만 우리가 보기 쉽게 정리하겠습니다. Abstract의 경우 Background, method, result, conclusion 등의 label로 테그가 걸려 있는 경우가 있고 각각의 텍스트를 가지고 있으므로 다음 방식으로 정리할 수 있습니다.\n\ndoc = list();gg = list();ab_num = list();\ngg1 = list(); gg2 = list(); gg3 = list();\ntitle=list();author=list();journal=list();\nabst = list();\n\n일단 리스트의 자료구조를 만들어주고\n\nloop1 = function(i){\n  doc[[i]] = read_xml(all_xml[i]) %&gt;% xml_children()\n  # splite all nodes to list(step)\n  gg[[i]]= xml_find_all(doc[[i]], './*')\n  ab_num[[i]] = c(1:length(gg[[i]])) [gg[[i]] %&gt;% xml_name() == \"Article\"]\n  gg1[[i]] = xml_children(gg[[i]][ab_num[[i]]])\n  # 초록만 가져오기\n  abst[[i]] = gg1[[i]] [which(gg1[[i]] %&gt;% xml_name() == \"Abstract\")]\n  abstnodes = abst[[i]] %&gt;% xml_children(.)\n  abst_fun = function(z) {tibble(\"labels\"= xml_attr(abstnodes[z], \"Label\"),\n                               \"text\"  = abstnodes[z] %&gt;% xml_text()\n                               )}\n  fdf = list()\n  fdf = lapply(1: c(abst[[i]] %&gt;% xml_children() %&gt;% length(.)), abst_fun)\n  fdf_df = do.call(rbind, fdf) \n  #id붙이기\n  fdf_df2 = fdf_df %&gt;% tibble() %&gt;%\n    mutate(ID = fdf_bas2$pmid[i])\n  return(fdf_df2)\n}\n\nloop1은 all_xml의 데이터를 리스트의 형태로 자르고 lapply, do.call(rbind,.)를 통하여 tibble의 형태로 고치는 함수입니다. 예를 들어 loop1(1)은 1번 커리의 첫번째 xml데이터를 저장합니다.\n\nloop1(1)\n\n# A tibble: 5 × 3\n  labels       text                                                        ID   \n  &lt;chr&gt;        &lt;chr&gt;                                                       &lt;chr&gt;\n1 INTRODUCTION \"A cross sectional study was conducted in Khartoum State. … 2206…\n2 OBJECTIVES   \"The study was conducted to determine lead concentrations … 2206…\n3 METHODS      \"The level of lead in ambient air was determined in 14 loc… 2206…\n4 RESULTS      \"The degree of environmental lead pollution in traffic amb… 2206…\n5 CONCLUSION   \"When we compared the results of age groups and work durat… 2206…\n\n\n이제 1번 커리의 모든 xml데이터를 반복문을 통하여 합쳐봅시다.\n\nloop_list = mclapply(1:c(all_xml %&gt;% length()), loop1)\n\nloop_df = do.call(rbind, loop_list)\n\n이제 1번 커리 뿐만 아니라 나머니 10년 동안의 데이터를 모두 가져와서 합쳐보겠습니다. 위의 모든 코드를 all_loop함수로 만들겠습니다.\n\nall_loop = function(pp){\n  #위의 모든 코드\n  my_entrez_id &lt;- get_pubmed_ids(my_query_date[[pp]])\n  my_abstracts_xml &lt;- fetch_pubmed_data(pubmed_id_list = my_entrez_id)\n  all_xml &lt;- articles_to_list(my_abstracts_xml)\n  fdf_bas = lapply(all_xml, article_to_df, max_chars = -1, getAuthors = FALSE)\n  fdf_bas2 = do.call(rbind, fdf_bas) %&gt;% select(-keywords, -lastname, -firstname, -address, -email)\n  \n  doc = list();gg = list();ab_num = list();\n  gg1 = list(); gg2 = list(); gg3 = list();\n  title=list();author=list();journal=list();\n  abst = list();\n\n  loop1 = function(i){\n    doc[[i]] = read_xml(all_xml[i]) %&gt;% xml_children()\n    gg[[i]]= xml_find_all(doc[[i]], './*')\n    ab_num[[i]] = c(1:length(gg[[i]])) [gg[[i]] %&gt;% xml_name() == \"Article\"]\n    gg1[[i]] = xml_children(gg[[i]][ab_num[[i]]])\n\n    abst[[i]] = gg1[[i]] [which(gg1[[i]] %&gt;% xml_name() == \"Abstract\")]\n    abstnodes = abst[[i]] %&gt;% xml_children(.)\n    abst_fun = function(z) {tibble(\"labels\"= xml_attr(abstnodes[z], \"Label\"),\n                               \"text\"  = abstnodes[z] %&gt;% xml_text()\n                               )}\n    fdf = list()\n    fdf = lapply(1: c(abst[[i]] %&gt;% xml_children() %&gt;% length(.)), abst_fun)\n    fdf_df = do.call(rbind, fdf) \n    fdf_df2 = fdf_df %&gt;% tibble() %&gt;%\n      mutate(ID = fdf_bas2$pmid[i])\n    \n    return(fdf_df2)\n  }\n  \n  loop_list = mclapply(1:c(all_xml %&gt;% length()), loop1)\n  loop_df = do.call(rbind, loop_list)\n  \n  #그 후 return\n  return(loop_df)\n}\n\n11년치를 함수와 반복문으로 가져옵니다.\n\ntic()\nall_loop_list = mclapply(1:11, all_loop)\ntoc() #43 sec(시간 체크하면 기다리는 시간을 미리 알수있어 좋습니다.)\n\nall_loop_df = do.call(rbind, all_loop_list)\n\n파일로 저장해 주겠습니다.\n\nsaveRDS(all_loop_df, \"data/all_loop_df.rds\")\n\n이제 초록을 제외한 논문의 index들을 가져와 보도록 하겠습니다. 초록과 비슷한 방식으로 함수와 반복문으로 가져오겠습니다. 다음은 fdf_loop함수입니다.\n\nfdf_loop = function(pp){\n  my_entrez_id &lt;- get_pubmed_ids(my_query_date[[pp]])\n  my_abstracts_xml &lt;- fetch_pubmed_data(pubmed_id_list = my_entrez_id)\n  all_xml &lt;- articles_to_list(my_abstracts_xml)\n  fdf_bas = lapply(all_xml, article_to_df, max_chars = -1, getAuthors = TRUE)\n  fdf_bas2 = do.call(rbind, fdf_bas)\n  # get summay data frame -------------\n  return(fdf_bas2)\n}\n\nfdf_loop(1)에는 1번 커리의 모든 index가 들어있습니다. 앞에서 했던 것처럼 abstract만 제외하고 보기 쉽게 나타내겠습니다.\n\nDT::datatable(fdf_loop(1)%&gt;% select(-abstract))\n\n\n\n\n\n\n이제 반복문으로 11년치 데이터를 모두 합쳐보겠습니다.\n\ntic()\ndfd_list = mclapply(1:11, fdf_loop)\ntoc() #43 sec(시간 체크하면 기다리는 시간을 미리 알수있어 좋습니다.)\ndfd_df = do.call(rbind, dfd_list)\ndfd_df1 = dfd_df %&gt;%\n  filter(doi !=\"\") %&gt;%\n  select(-keywords, -address, -email) %&gt;%\n  na.omit()\nauthors = dfd_df1 %&gt;%\n  mutate(author1 = paste(lastname, firstname)) %&gt;%\n  select(pmid, author1) %&gt;%\n  group_by(pmid) %&gt;%\n  summarize(authors = paste(author1, collapse=\", \"))\n\ndfd_dfd2 = dfd_df1 %&gt;%\n  select(-lastname, -firstname) %&gt;%\n  left_join(authors, by = \"pmid\") %&gt;%\n  unique() %&gt;%\n  mutate(date = paste(year, month, day)) %&gt;%\n  select(-year, -month, -day) %&gt;%\n  select(pmid, title, doi, jabbrv, journal, authors, date, abstract )\nsaveRDS(dfd_dfd2, \"data/summary1.rds\")"
  },
  {
    "objectID": "610_webscraping_02.html#pdf-자동-보고서",
    "href": "610_webscraping_02.html#pdf-자동-보고서",
    "title": "16  PubMed 에서 R 과 EndNote",
    "section": "16.3 PDF 자동 보고서",
    "text": "16.3 PDF 자동 보고서\n\n\n마크다운에서 아래의 항목을 구성합니다. 새로운 마크다운을 사용하는 것입니다. 전체 파일을 참고하시고 이미지를 보면서 따라하세요. 전체파일 \n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(htmlTable)\nlibrary(kableExtra)\nlibrary(knitr)\nlibrary(xtable)\nlibrary(data.table)\n\n앞서 작성한 데이터를 불러오겠습니다.\n\nall_loop_df = readRDS(\"data/all_loop_df.rds\")\ndfd_df = readRDS(\"data/summary1.rds\")\n#tab_num &lt;- run_autonum(seq_id = \"표\", pre_label = \"표. \", bkm = \"\")\n#fig_num &lt;- run_autonum(seq_id = \"그림\", pre_label = \"그림. \", bkm = \"\")\n\n데이터를 통해 필요한 표 형식을 만들어 보겠습니다. 표 형식 만들기는 Data Wrangling 부분을 공부해 주세요.\n\ndtab = list()\nids1 = dfd_df %&gt;% select(pmid) %&gt;% mutate(ids1=1) %&gt;% unique()\nids2 = all_loop_df %&gt;% select(ID) %&gt;% mutate(ids2=1) %&gt;% unique()\nidsm = ids1 %&gt;%\n  left_join(ids2, by = c(\"pmid\"=\"ID\")) %&gt;%\n  na.omit() %&gt;% pull(pmid)\n\ndfs = all_loop_df %&gt;% \n  filter(!is.na(labels)) %&gt;%\n  unique() %&gt;%\n  select(ID, \"Labels\" = labels, \"Values\" =text)\ndfd_df2 = dfd_df %&gt;% filter(pmid %in% idsm) %&gt;%\n  mutate(Journal = paste0(journal, \": \", doi), ID= pmid) %&gt;%\n  select(ID, \"PubMed ID\" = pmid, \n             \"TITLE\" = title,\n             \"JOURNAL\" =Journal, \n             \"AUTHORS\"=authors) %&gt;%\n  pivot_longer(-ID, names_to = \"Labels\", values_to = \"Values\") %&gt;%\n  rbind(., dfs) %&gt;%\n  arrange(ID) %&gt;%\n  group_by(ID) %&gt;%\n  mutate(n=n()) %&gt;%\n  filter(n&gt;4) %&gt;% ungroup() %&gt;% select(-n)\nidsm2 = dfd_df2 %&gt;% select(ID) %&gt;% unique() %&gt;% pull(ID) \n\nPubMEd ID와 초록등 모든 것이 잘 갖추어진 논문만 idsm2 에 넣어 이것 만으로 표를 만들어 보겠습니다. 모든 표를 반복작업해서(lapply)만들어 보겠습니다.\n\ntabs_f =  function(i){dfd_df2 %&gt;% filter(ID == idsm2[i]) %&gt;% select(-ID)}\ntabs_l = lapply(1:c(idsm2%&gt;%length()), tabs_f)\n\n구분/내용 으로 표의 항목을 만들고, linesep로 줄 간격 조절, 이후 자동 표 제목을 만드는 방법으로 표를 만듭니다.\nplotTab = function(i){\nkbl(tabs_l[[i]] %&gt;% setNames(c(\"구분\", \"내용\")), \"latex\", booktabs = T, linesep = \"\\\\addlinespace\\\\addlinespace\",\n    caption = paste0(\"표-\", i, \".\", \" PubMed 논문번호 \", tabs_l[[i]]$Values[1], \"의 내용 요약\"  )) %&gt;%\n    column_spec(2, width = \"12cm\") %&gt;%\n    kable_styling(latex_options = \"striped\")\n}\n첫번째 표를 생성해 보겠습니다.\nplotTab(1)\n\n\n\npubmed png\n\n\n반복적으로 수행해 보니다.\nfor (i in 1:889){\n   cat(\"\\\\clearpage\")\n cat(\"\\n\\n\\\\pagebreak\\n\")\n cat(plotTab(i))\n cat(\"\\\\clearpage\")\n}\n을 수행하면 PDF가 생성되고 이로인해, 생성된 자동보고서는 다음과 같습니다. 자동보고서\n\n\n\n\n\n이것을 액셀로도 저장하게 됩니다.\n\nexcels = dfd_df %&gt;%\n  filter(pmid %in% idsm2)\nwritexl::write_xlsx(excels, \"output/excels.xlsx\")\n\n액셀저장 파일 파일 의 형식입니다."
  },
  {
    "objectID": "610_webscraping_02.html#원문-pdf-찾기",
    "href": "610_webscraping_02.html#원문-pdf-찾기",
    "title": "16  PubMed 에서 R 과 EndNote",
    "section": "16.4 원문 PDF 찾기",
    "text": "16.4 원문 PDF 찾기"
  },
  {
    "objectID": "620_TimeData_01.html#date-create",
    "href": "620_TimeData_01.html#date-create",
    "title": "17  시간관련 자료 다루기",
    "section": "17.1 Date create",
    "text": "17.1 Date create\n오늘 날짜와 지금 시간, 초를 생성해 보겠습니다. 어떻게 나오나요?\n\ntoday()\nnow()\nSys.time()\n\n주로 날짜는 엑셀 등으로 저잘할 때 문자형식으로 저장합니다. 숫자로 저장하는 경우 변환할때 한번더 수작업을 해야 하기 때문입니다. 또한 월이 있는데, 일이 없거나 하는 경우 변환때 missing 값을로 자동변환됨에도 연구자가 모르고 지나 갈수 있기 때문입니다. 문자로 저장된 날짜를 날짜 변수로 만들어 보겠습니다.\n\n\nymd, mdy, dmy를 사용하겠습니다.\n\ntibble(ymd(\"2020-11-10\"), \nymd(\"20201110\"),\nymd(\"2020-Nov-10\"),\nmdy(\"November 10th, 2020\"),\ndmy(\"10-Nov-2020\")) %&gt;% t() \n\n                           [,1]        \nymd(\"2020-11-10\")          \"2020-11-10\"\nymd(\"20201110\")            \"2020-11-10\"\nymd(\"2020-Nov-10\")         \"2020-11-10\"\nmdy(\"November 10th, 2020\") \"2020-11-10\"\ndmy(\"10-Nov-2020\")         \"2020-11-10\"\n\n\n모든 날짜는 숫자로 저장되고 있습니다. 2020년 11월 10일은 숫자로는 18576입니다. 기준 날짜인 1970년 1월 1일부터 몇일이 지났는지를 적어 놓은 것입니다. 그래서 아래의 함수를 통해 숫자를 날짜로 변환 할 수 있습니다.\n\nymd(\"2020-11-10\")%&gt;% as.numeric()\n\n[1] 18576\n\nas.Date(18576, origin = \"1970-01-01\", tz = \"UTC\")\n\n[1] \"2020-11-10\"\n\n\n2020년 11월 10일부터 100일이 지난 날의 날짜는 무엇일까요? 아래와 같이 해볼 수 있습니다.\n\nn100days &lt;- ymd(\"2020-11-10\")%&gt;% as.numeric() +100\nas.Date(n100days, origin = \"1970-01-01\")\n\n[1] \"2021-02-18\"\n\n\n여기에 hour, minute, second를 추가할 수 있습니다 .\n\ntibble(year  = 2020, \n              month = 11, \n              day   = 10, \n              hour  = 11, \n              minute= 15, \n              sec   = 30) %&gt;%\n        mutate(now = make_datetime(\n                year, month, day, hour, minute, sec\n        )) \n\n# A tibble: 1 × 7\n   year month   day  hour minute   sec now                \n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;             \n1  2020    11    10    11     15    30 2020-11-10 11:15:30\n\n\n그럼 그냥 문자로 사용하면 되지 왜 date 형식으로 변환을 하는 걸까요? Date-time components 가 필요하기 때문이기도 합니다. 그리고 연산이 가능해 집니다.\n\ndatetime &lt;- ymd_hms(\"2020-11-10 11:30:55\")\nyear(datetime)\n\n[1] 2020\n\nmonth(datetime)\n\n[1] 11\n\nmday(datetime) # 몇 일인지\n\n[1] 10\n\nyday(datetime) # 1월 1일 부터 몇일이 지난는지\n\n[1] 315\n\nwday(datetime) # 월요일 1, 일요일 7\n\n[1] 3\n\n\n몇월인지 무슨 요일인지 생성해 보겠습니다.\n\nlubridate::month(datetime, label = TRUE)\n\n[1] 11월\n12 Levels:  1월 &lt;  2월 &lt;  3월 &lt;  4월 &lt;  5월 &lt;  6월 &lt;  7월 &lt;  8월 &lt; ... &lt; 12월\n\nlubridate::wday(datetime, label = TRUE)\n\n[1] 화\nLevels: 일 &lt; 월 &lt; 화 &lt; 수 &lt; 목 &lt; 금 &lt; 토\n\n\n몇가지 연산을 위해 필요한 days, hours, minutes, seconds, weeks, months, years가 있습니다. 상식적 수순에서 이해할 수 있는데요,\n\n시간간격\n\ndays: 일수를 나타냅니다. days(100)는 100일의 시간 간격을 나타냅니다.\nhours: 시간을 나타냅니다. hours(24)는 24시간의 시간 간격을 나타냅니다.\nminutes: 분을 나타냅니다. minutes(60)은 60분의 시간 간격을 나타냅니다.\nseconds: 초를 나타냅니다. seconds(60)은 60초의 시간 간격을 나타냅니다.\nweeks: 주를 나타냅니다. weeks(2)는 2주의 시간 간격을 나타냅니다.\nmonths: 월을 나타냅니다. 주어진 월 수만큼의 평균 일 수를 계산합니다. months(1)은 한 달의 시간 간격을 나타냅니다.\nyears: 연도를 나타냅니다. years(1)은 1년의 시간 간격을 나타냅니다.\n\n\n우리는 날짜 데이터를 생성하고, 다양한 형식으로 날짜를 변환하는 방법을 배웠습니다. 이번에는 특정 날짜로부터 10일 전의 날짜를 계산하는 방법에 대해 알아보겠습니다. R에서 날짜 계산을 용이하게 해주는 lubridate 패키지를 사용하여 이러한 계산을 수행할 수 있습니다. 예를 들어, ’2023년 11월 28일’로부터 10일 전의 날짜를 찾고 싶다면, 다음과 같이 할 수 있습니다:"
  },
  {
    "objectID": "620_TimeData_01.html#tutor-heat-wave-and-death",
    "href": "620_TimeData_01.html#tutor-heat-wave-and-death",
    "title": "17  시간관련 자료 다루기",
    "section": "17.2 Tutor: heat wave and death",
    "text": "17.2 Tutor: heat wave and death\nhm0 자료를 가지고 몇가지 실습을 해보도록 하겠습니다.\n\nhead(hm0) %&gt;%\n        data.table()\n\n         date   temp_mean temp_max cloud_mean  sun_day wind_mean rhum_mean temp\n1: 2002-01-01  -4.4222222      4.6  1.6333333 7.577778  2.966667  55.62222 -1.5\n2: 2002-01-02 -11.6222222     -4.7  0.1777778 8.033333  3.566667  40.97778 -8.7\n3: 2002-01-03  -8.2222222      1.9  1.3111111 8.011111  3.266667  45.83333 -3.6\n4: 2002-01-04   0.3777778      7.6  5.9666667 2.811111  3.800000  66.02222  2.7\n5: 2002-01-05  -2.6777778      4.0  0.5333333 7.855556  2.833333  45.53333  0.5\n6: 2002-01-06  -3.6222222      8.4  1.2333333 8.055556  1.477778  54.55556  1.2\n   tempf humidity heat_index event period region_id inclusion year circulation\n1:  29.3     44.5      -1.51     0      0        42         1 2002           0\n2:  16.4     27.1      -8.69     0      0        42         1 2002          10\n3:  25.6     24.1      -3.58     0      0        42         1 2002          10\n4:  36.8     47.9       2.68     0      0        42         1 2002          16\n5:  32.8     29.4       0.45     0      0        42         1 2002           8\n6:  34.2     32.4       1.23     0      0        42         1 2002           0\n   cvd endo gi heat infection injury mental nmd respiratory skin uro tmr occp1\n1:   0    0  0    0         0      0      0   0           0    0   0  27     0\n2:   0    1 23    0         5      6      6  26          76   10  17  27     0\n3:   1    7 10    0         8      8      2  12          63   10  12  25     0\n4:   1    7 12    0         5      4      2  17          57   16  12  29     0\n5:   0    1 13    0         1      7      1  16          36    8   5  25     0\n6:   0    0  0    0         0      0      0   0           0    0   0  23     1\n   occp2 occp3 occp4 occp5 occp6 occp7 occp8 occp9 occp13 occp99 inside outside\n1:     0     0     0     2     3     0     0     0     22      0     27       0\n2:     0     1     0     3     3     1     0     1     18      0     27       0\n3:     0     0     1     0     6     0     0     0     18      0     25       0\n4:     0     0     1     4     3     2     0     1     18      0     29       0\n5:     1     0     1     1     4     0     0     0     18      0     25       0\n6:     0     0     0     0     5     1     0     0     16      0     23       0\n   tomr\n1:    5\n2:    9\n3:    7\n4:   11\n5:    7\n6:    7\n\n\n\n17.2.1 date create\n날짜를 생성해 보겠습니다.\n\nclass(hm0$date)\n\n[1] \"Date\"\n\nclass(hm0$year)\n\n[1] \"numeric\"\n\n\n2002년부터 2015년까지의 자료 입니다.\n\ns1 &lt;- hm0 %&gt;%\n  group_by(year) %&gt;%\n  count() \ns1\n\n# A tibble: 14 × 2\n# Groups:   year [14]\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  2002 24820\n 2  2003 24820\n 3  2004 24892\n 4  2005 24824\n 5  2006 24824\n 6  2007 24835\n 7  2008 24894\n 8  2009 24822\n 9  2010 24856\n10  2011 25200\n11  2012 25267\n12  2013 25220\n13  2014 25213\n14  2015 25212\n\n\n월, 일, 숫자형 월을 생성해 보겠습니다.\n\ns2 &lt;- hm0 %&gt;% mutate(months   = lubridate::month(date, label = TRUE, abbr = TRUE)) %&gt;%\n  mutate(weekday = lubridate::wday(date, label = TRUE, abbr = TRUE)) %&gt;%\n  mutate(month = substr(date, 6, 7)) %&gt;%\n  filter(year(date) &lt;= 2015) \n\n월별 햇볕이 있었던 날의 수를 표로 나타내 보겠습니다.\n\ns2 %&gt;% group_by(months) %&gt;%\n  summarize(`number of sunny days` = sum(sun_day, na.rm=TRUE)) %&gt;%\n  htmlTable()\n\n\n\n\n\nmonths\nnumber of sunny days\n\n\n\n\n1\n1월\n163634.1\n\n\n2\n2월\n160058\n\n\n3\n3월\n196690\n\n\n4\n4월\n196703.6\n\n\n5\n5월\n216350.7\n\n\n6\n6월\n173973.8\n\n\n7\n7월\n132143\n\n\n8\n8월\n154961.4\n\n\n9\n9월\n160004.7\n\n\n10\n10월\n197543.7\n\n\n11\n11월\n150447.1\n\n\n12\n12월\n153073.2\n\n\n\n\n\n월별 분포를 그림으로 그려보겠습니다.\n\nlibrary(ggplot2)\ns2 %&gt;% filter(year ==2015) %&gt;%\n        group_by(months) %&gt;%\n        ggplot(aes(x= months, y = sun_day, color = months)) +\n        geom_point()+\n        ggtitle('sunny day counts by months') +\n        ylab ('sunny day counts') +\n        guides(color = FALSE) \n\n\n\n\n이번에는 최고 기온가 일병 사망 숫자와의 관계를 살폐보겠습니다. 고온과 사망이므로 filter(month %in% c(\"06\", \"07\", \"08\", \"09\"))을 이용하겠습니다. 월을 만들어 놓았으니 filter를 이용해서 필요한 월만 가져오면 되겠습니다. tomr 은 total occupation mortality 입니다. count 입니다. 35도가 넘으면 사망 숫자가 상승하고 있습니다.\n\ns2 %&gt;% filter(month %in% c(\"06\", \"07\", \"08\", \"09\"), \n              temp_max &gt;15) %&gt;%\n  ggplot(aes(x = lag(temp_max), y = tomr)) +\n  geom_point(aes(color = months), alpha = 0.5) +\n  stat_smooth(method = \"lm\", formula = y ~ poly(x, 3), size = 1) +\n  theme_minimal() +\n  xlab('max temperature') + ylab('total death counts') +\n  labs(caption =\"*row assocation, no lag with linear\",\n       title=\" Association between max temperature \\n and daily total death counts \"#, subtitle=\"Beta = -0.014, p = 0.158  before 2009\\n  Beta =  0.002,  p = 0.011     from 2009\"\n  )"
  },
  {
    "objectID": "620_TimeData_01.html#home-work",
    "href": "620_TimeData_01.html#home-work",
    "title": "17  시간관련 자료 다루기",
    "section": "17.3 home work",
    "text": "17.3 home work\n\n17.3.1 home0\n\n2020-11-10로부터 100일전 날짜는?\n2020-11-10로 부터 100일전 날짜의 요일은?\n1978-01-21 에 태어난 사람이 2020-12-31까지 몇일을 살았을까?\n1919-03-01 은 무슨 요일일까?\n\n\n\n17.3.2 home work1\n6, 7, 8, 9월에 고온과 농업인의 사망과의 관련성을 그려보세요. 농업인의 사망은 occp6 입니다.\n\n\n\n\n\n\n\n17.3.3 home work2\n12, 1, 2월에 기온과 농업인의 사망과의 관련성을 그려보세요. 기온은 temp_mean을 농업인의 사망은 occp6 입니다.\n\n\n\n\n\n\n\n17.3.4 home work3\n12, 1, 2월에 기온과 관리직의 사망과의 관련성을 그려보세요. 기온은 temp_mean을 관리직의 사망은 occp1 입니다."
  },
  {
    "objectID": "710_GenAI_01.html",
    "href": "710_GenAI_01.html",
    "title": "18  생성형 AI 사용",
    "section": "",
    "text": "기계는 어떻게 학습할까?\n\n기계가 학습을 할 때,\n\n개념적으로 문제가 있고 이에 맞는 답이 있는 경우,\n문제는 나열되어 있고 비슷한 문제끼리 묶어야 하는 경우,\n해보고 뭐가 문제인지 뭐가 답인지 경험해보고 보상받는 경우로\n\n\n구분해 볼 수 있습니다. 각각 지도학습, 비지도학습, 강화학습이라고 합니다."
  },
  {
    "objectID": "710_GenAI_01.html#기계는-어떻게-학습할까",
    "href": "710_GenAI_01.html#기계는-어떻게-학습할까",
    "title": "18  생성형 AI 사용",
    "section": "18.1 기계는 어떻게 학습할까?",
    "text": "18.1 기계는 어떻게 학습할까?\n기계는 어떻게 학습할까?\n\n기계가 학습을 할 때,\n\n개념적으로 문제가 있고 이에 맞는 답이 있는 경우,\n문제는 나열되어 있고 비슷한 문제끼리 묶어야 하는 경우,\n해보고 뭐가 문제인지 뭐가 답인지 경험해보고 보상받는 경우로\n\n\n구분해 볼 수 있습니다. 각각 지도학습, 비지도학습, 강화학습이라고 합니다."
  },
  {
    "objectID": "710_GenAI_01.html#chatgpt-의-간단-소개",
    "href": "710_GenAI_01.html#chatgpt-의-간단-소개",
    "title": "18  생성형 AI 사용",
    "section": "18.2 ChatGPT 의 간단 소개",
    "text": "18.2 ChatGPT 의 간단 소개\n\n대형언어모델이다. (GPT는 비지도학습, 미세조정)\n\n문장에서 다음에 오는 단어를 예측하는 방식\n많이 읽어서 확률적 분포를 예측한다. → 구조 이용\n\n인간피드백을 이용한 강화 학습이다. (강화학습)\n\n인간의 피드백을 바탕으로 보상이나 벌점을 주고, 높은 점수를 받는 쪽으로 유도하는 방법\n이후 반복적 미세 조정을 통해 성능을 향상시킴"
  },
  {
    "objectID": "710_GenAI_01.html#생성된-방식대로-사용하기",
    "href": "710_GenAI_01.html#생성된-방식대로-사용하기",
    "title": "18  생성형 AI 사용",
    "section": "18.3 생성된 방식대로 사용하기",
    "text": "18.3 생성된 방식대로 사용하기\n\n\n\nladdering and N-shot, asking\n\n\nChatGPT에게 질문할 때 사용할 수 있는 ‘Laddering (N-shot)’과 ’Asking’ 접근 방식에 대한 순서도가 나타나 있습니다. 그림을 통해 정보를 얻거나 대화를 진행할 때 단계별로 어떻게 접근하는 것이 효과적인지 생각해 보세요.\n위 그림은 다음과 같은 단계를 포함하고 있습니다:\n\n질문 정의 및 분명한 질문 설정: 대화의 시작점에서, 명확하고 구체적인 질문을 통해 대화를 시작합니다.\n분명한 1단계 질문: 첫 번째 단계에서는 주제에 대한 기본적인 정보를 얻기 위해 직접적인 질문을 합니다.\n질문 군 1단계 제공: 초기 질문에 대한 답변을 바탕으로, 보다 구체적인 세부 사항을 파악하기 위한 추가 질문을 합니다.\n통합적 2단계 질문: 이 단계에서는 앞선 답변들을 통합하여 깊이 있는 이해를 위한 질문을 합니다.\n보다 깊은 논리성 요청: 답변에서 더 깊이 있는 논리나 이유를 요청함으로써, 사물의 본질이나 근본적인 원인을 탐구합니다.\nN shot: 위의 단계를 여러 번 반복하여 주제에 대한 포괄적인 이해를 구축합니다.\n이 과정은 깊이 있는 대화를 위한 좋은 가이드라인을 제공하며, 사용자가 ChatGPT와의 상호작용을 최대화하기 위한 전략적인 질문 방식을 개발하는 데 도움을 줄 수 있습니다.\n\n강의 자료를 통해 이야기 하겠습니다."
  },
  {
    "objectID": "710_GenAI_01.html#생성형-ai와-이야기-하기",
    "href": "710_GenAI_01.html#생성형-ai와-이야기-하기",
    "title": "18  생성형 AI 사용",
    "section": "18.4 생성형 AI와 이야기 하기",
    "text": "18.4 생성형 AI와 이야기 하기\n\n\n아래 코드를 통해 기존에 논의했던 logistic regression 표를 생성해 봅니다.\n\npkgs = c(\"tidyverse\",  \"htmlTable\", \"broom\", \"labelled\", \"haven\", \"DT\",\n         \"devtools\", \"lmtest\", \"ggplot2\", \"shiny\", \"shinyWidgets\", \n         \"plotly\", \"httr\", \"rvest\", \"jsonlite\")\nfor (pkg in pkgs){\n  if(!require(pkg, character.only = T)) install.packages(pkg)\n  library(pkg, character.only = T)\n}\nif(!require(\"tabf\")) install_github(\"jinhaslab/tabf\")\nlibrary(tabf)\n\n\nurl1 &lt;- \"https://raw.githubusercontent.com/jinhaslab/opendata/main/kwcs/myoutput.rds\"\n\ndownload.file(url1, \"data/myoutput.rds\")\n#myoutput = readRDS(\"results/myoutput.rds\")\nmyoutput = readRDS(\"data/myoutput.rds\")\nmytab1 = myoutput\nmytab1\n\n\n\n\nTable. OR(95%CI) for sleepgp of 1.sleep disturbance\n\n\nVariables\nValues\nModel.I\n\n\n\n\nint\n0.Rarely with 0.non short return\n1.00 (reference)\n\n\n\n1.≥Sometimes with 0.non short return\n1.53 (1.36-1.73)\n\n\n\n0.Rarely with 1.short return\n1.97 (1.31-2.97)\n\n\n\n1.≥Sometimes with 1.short return\n6.64 (5.46-8.06)\n\n\nedugp\n0.university or more\n1.00 (reference)\n\n\n\n1.high school\n1.02 (0.91-1.14)\n\n\n\n2.middle school or below\n1.68 (1.39-2.04)\n\n\nAGE\n\n1.01 (1.01-1.02)\n\n\n\n\n\n이 표를 어떻게 chatGPT에게 설명해 달라고 할 수 있을까요?\n우선 api_key가 있어야 합니다. 이걸 통해서 chatGPT와 소통할 것입니다.\n\napi_key_report = \"your key\"\n\n우선 보내고 싶은 table을 R에서 사용할 수 있는 표로 만듭니다.\n\nmytab1 %&gt;% read_html() %&gt;% html_nodes(\"table\") %&gt;% html_table\n\n[[1]]\n# A tibble: 10 × 3\n   X1                                                    X2                X3   \n   &lt;chr&gt;                                                 &lt;chr&gt;             &lt;chr&gt;\n 1 \"Table. OR(95%CI) for sleepgp of 1.sleep disturbance\" \"Table. OR(95%CI… Tabl…\n 2 \"Variables\"                                           \"Values\"          Mode…\n 3 \"int\"                                                 \"0.Rarely with 0… 1.00…\n 4 \"\"                                                    \"1.≥Sometimes w…  1.53…\n 5 \"\"                                                    \"0.Rarely with 1… 1.97…\n 6 \"\"                                                    \"1.≥Sometimes w…  6.64…\n 7 \"edugp\"                                               \"0.university or… 1.00…\n 8 \"\"                                                    \"1.high school\"   1.02…\n 9 \"\"                                                    \"2.middle school… 1.68…\n10 \"AGE\"                                                 \"\"                1.01…\n\n\nmytab1 %&gt;% read_html() %&gt;% html_nodes(“table”) %&gt;% html_table: mytab1 변수에 저장된 데이터를 HTML 테이블로 읽고 변환합니다. 이 코드는 mytab1이 HTML 형식의 데이터를 포함하고 있다고 가정합니다. read_html, html_nodes, html_table 함수들은 각각 HTML 데이터를 읽고, HTML 내의 테이블 요소를 찾고, 이를 R에서 사용할 수 있는 테이블 형식으로 변환하는 데 사용됩니다.\n\nai_models=\"gpt-3.5-turbo\"\ntm=0.1\ncm_tmp=c(\"Please summarize the my result provided below,write it like a report document,\n         and express it in sentences as public health specilist.\")\naddinfor = c(\"wwa1gp is worry about work after finished work\")\n\nai_models=“gpt-3.5-turbo”: ai_models 변수에 “gpt-3.5-turbo”라는 문자열을 할당합니다. 이는 특정 인공지능 모델을 지칭하는 것으로 보입니다. tm=0.1: tm 변수에 0.1이라는 값을 할당합니다. 이 값의 정확한 용도는 코드에서 명시되어 있지 않지만, 일반적으로 시간이나 임계값 등을 나타내는 데 사용됩니다. cm_tmp: 사용자가 요청한 내용을 포함하는 문자열 배열을 정의합니다. 여기에는 결과를 공중보건 전문가처럼 보고서 형식으로 요약하라는 지시가 포함되어 있습니다. addinfor: 추가 정보를 제공하는 addinfor 변수가 정의되어 있으며, 이는 “wwa1gp is worry about work after finished work”라는 문자열을 포함합니다.\n\nmytable1 = mytab1 %&gt;%\n    read_html() %&gt;%\n    html_nodes(\"tr\") %&gt;%\n    html_text(trim=TRUE) \nmytable1\n\n [1] \"Table. OR(95%CI) for sleepgp of 1.sleep disturbance\"    \n [2] \"Variables\\nValues\\nModel.I\"                             \n [3] \"int\\n0.Rarely with 0.non short return\\n1.00 (reference)\"\n [4] \"1.≥Sometimes with 0.non short return\\n1.53 (1.36-1.73)\"\n [5] \"0.Rarely with 1.short return\\n1.97 (1.31-2.97)\"         \n [6] \"1.≥Sometimes with 1.short return\\n6.64 (5.46-8.06)\"    \n [7] \"edugp\\n0.university or more\\n1.00 (reference)\"          \n [8] \"1.high school\\n1.02 (0.91-1.14)\"                        \n [9] \"2.middle school or below\\n1.68 (1.39-2.04)\"             \n[10] \"AGE\\n\\n1.01 (1.01-1.02)\"                                \n\n\n\nHTML 데이터 처리:\n\nread_html(): mytab1 변수에서 HTML 데이터를 읽습니다. read_html 함수는 rvest 패키지의 일부로, HTML 문서를 R의 세션으로 가져오는 데 사용됩니다.\nhtml_nodes(“tr”): 읽어들인 HTML 문서에서 모든 행(\n\n태그)을 찾습니다. html_nodes 함수는 지정된 CSS 선택자에 해당하는 HTML 노드들을 추출하는 데 사용됩니다.\nhtml_text(trim=TRUE): 각 행의 텍스트 내용을 추출합니다. trim=TRUE 옵션은 텍스트의 앞뒤 공백을 제거합니다.\n\n결과 저장 및 출력:\n\nmytable1: 위의 파이프라인을 통해 처리된 데이터는 mytable1이라는 새로운 변수에 저장됩니다.\n\n\n\nmyquestion = sprintf(\"%s, consider my special request of %s. {result} is {%s}\", \n                     cm_tmp, \n                     addinfor, \n                     paste(mytable1, collapse=\" \"))\nmyquestion %&gt;% htmlTable()\n\n\n\n\nPlease summarize the my result provided below,write it like a report document, and express it in sentences as public health specilist., consider my special request of wwa1gp is worry about work after finished work. {result} is {Table. OR(95%CI) for sleepgp of 1.sleep disturbance Variables Values Model.I int 0.Rarely with 0.non short return 1.00 (reference) 1.≥Sometimes with 0.non short return 1.53 (1.36-1.73) 0.Rarely with 1.short return 1.97 (1.31-2.97) 1.≥Sometimes with 1.short return 6.64 (5.46-8.06) edugp 0.university or more 1.00 (reference) 1.high school 1.02 (0.91-1.14) 2.middle school or below 1.68 (1.39-2.04) AGE 1.01 (1.01-1.02)}\n\n\n\n\n\n\nresponse &lt;- POST(\n    url = \"https://api.openai.com/v1/chat/completions\", \n    add_headers(Authorization = paste(\"Bearer\", api_key_report)),\n    content_type_json(),\n    encode = \"json\", \n    body = list(\n      model = ai_models,\n      temperature = 0.1, \n      messages = list(list(role = \"user\", content = myquestion))\n    )\n)\n\ncontent &lt;- content(response, as = \"text\")\nparsed_content &lt;- fromJSON(content)\nparsed_content$choices[2]$message$content\n\n\nAPI 요청 설정 (POST 함수 사용):\n\nurl = “https://api.openai.com/v1/chat/completions”: OpenAI의 채팅 API 엔드포인트로 요청을 보냅니다.\nadd_headers(Authorization = paste(“Bearer”, api_key_report)): API 사용을 인증하기 위해 ‘Bearer’ 토큰을 포함한 헤더를 추가합니다. 여기서 api_key_report는 API 키를 가리킵니다.\ncontent_type_json(): 요청의 내용이 JSON 형식임을 명시합니다.\nencode = “json”: 요청 본문을 JSON으로 인코딩합니다.\n\n요청 본문 설정 (body):\n\nmodel = ai_models: 사용할 모델을 지정합니다. 여기서 ai_models 변수는 앞서 설정된 모델 이름(예: “gpt-3.5-turbo”)을 포함합니다.\ntemperature = 0.1: 응답의 창의성을 결정하는 온도 값을 설정합니다. 낮은 값은 더 일관되고 예측 가능한 응답을 생성합니다.\nmessages = list(list(role = “user”, content = myquestion)): 사용자의 질문을 포함하는 메시지 리스트를 설정합니다. 여기서 myquestion은 사용자의 질문을 담고 있습니다.\n\nAPI 요청 실행 및 응답 처리:\n\nresponse: API 요청의 결과를 response 변수에 저장합니다.\ncontent &lt;- content(response, as = “text”): 응답의 내용을 텍스트 형식으로 추출합니다.\nparsed_content &lt;- fromJSON(content): 텍스트 형식의 응답을 JSON으로 파싱합니다.\n\n결과 추출:\n\nparsed_content\\(choices[2]\\)message$content: 파싱된 JSON 객체에서 특정 응답 내용을 추출합니다. 이 경로는 JSON 객체 내의 두 번째 ’choice’에 해당하는 메시지의 내용을 가리킵니다.\n\n\n\nparsed_content$choices[2]$message$content\n\n\n\n\n\n\n[1] The results of the analysis indicate that there is a significant association between sleep disturbance and work-related worries after finishing work. Individuals who reported experiencing sleep disturbance sometimes or more frequently were 1.53 times more likely to worry about work after finishing compared to those who rarely experienced sleep disturbance. Additionally, individuals who reported short returns from work and sometimes or more frequently experienced sleep disturbance were 6.64 times more likely to worry about work after finishing compared to those who rarely experienced sleep disturbance and had non-short returns from work. Furthermore, the results suggest that education level is also associated with sleep disturbance and work-related worries. Individuals with a middle school education or below were 1.68 times more likely to experience sleep disturbance compared to those with a university education or higher. However, there was no significant difference in sleep disturbance and work-related worries between individuals with a high school education and those with a university education or higher. Lastly, the analysis found a small but significant association between age and sleep disturbance. For every one-year increase in age, there was a 1.01 times increase in the likelihood of experiencing sleep disturbance. Overall, these findings highlight the importance of addressing sleep disturbance and work-related worries, particularly among individuals with lower education levels and those who frequently experience short returns from work. Public health interventions should focus on promoting healthy sleep habits and providing support for individuals to manage work-related stress and worries."
  },
  {
    "objectID": "710_GenAI_01.html#실용화-예시",
    "href": "710_GenAI_01.html#실용화-예시",
    "title": "18  생성형 AI 사용",
    "section": "18.5 실용화 예시",
    "text": "18.5 실용화 예시\nshiny와 interractive visualization은 DS project 수업에 있으니 생략하겠습니다."
  },
  {
    "objectID": "620_TimeData_01.html#time-series-data-analysis",
    "href": "620_TimeData_01.html#time-series-data-analysis",
    "title": "17  시간관련 자료 다루기",
    "section": "17.4 Time series data analysis",
    "text": "17.4 Time series data analysis\n기온등의 시계열적 변수가 질병에 어떻게 영향을 미치는지 알고자 할 때 어떻게 해야 할 까요? 우선 전체 직업군의 사망수 ’tomr’을 타겟 변수로 지정하겠습니다. 그리고 월별 사망률을 그려보겠습니다.\n\ns2 &lt;- s2 %&gt;%\n  mutate(dz = tomr)\n\nlibrary(plotly)\ns2 %&gt;% group_by(months) %&gt;%\n  plot_ly( x= ~ months, y = ~dz,\n           type = \"box\", \n           color = ~ months) %&gt;%\n  layout(title = 'daily total mortality counts by months', \n         yaxis = list(title ='daily death counts'))"
  },
  {
    "objectID": "720_Supervised_01.html",
    "href": "720_Supervised_01.html",
    "title": "19  지도학습",
    "section": "",
    "text": "#\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"dslabs\")) install.packages(\"dslabs\")\nif(!require(\"caret\")) install.packages(\"caret\")\nif(!require(\"purrr\")) install.packages(\"purrr\")\nif(!require(\"randomForest\")) install.packages(\"randomForest\")\nif(!require(\"doParallel\")) install.packages(\"doParallel\")\nif(!require(\"foreach\")) install.packages(\"foreach\")\nif(!require(\"tictoc\")) install.packages(\"tictoc\")"
  },
  {
    "objectID": "720_Supervised_01.html#simulation-data",
    "href": "720_Supervised_01.html#simulation-data",
    "title": "19  지도학습",
    "section": "19.1 simulation data",
    "text": "19.1 simulation data\nCreating a simulated dataset in R for machine learning practice is an excellent approach to grasp foundational concepts in data science and analytics. The provided example generates a dataset with variables such as gender, education, age, income, working hours, and responses to the PHQ-9 questionnaire. This exercise is particularly beneficial for understanding how to manipulate and prepare data, a critical step in machine learning workflows\n\nset.seed(2023) # For reproducible results\nn &lt;- 10000 # Sample size\n\nset.seed(2023):\n\nThis function sets the seed of R’s random number generator, which is essential for ensuring reproducible results.\nThe number 2023 is arbitrary; any integer can be used. The important aspect is that using the same seed number will produce the same sequence of random numbers, which is crucial for replicability in scientific studies or when sharing code with others.\nIn machine learning, reproducibility is key for debugging, comparing model performance, and collaborative projects.\n\nn &lt;- 10000:\n\nThis line of code initializes a variable n with the value 10000, which represents the sample size for the dataset you are creating or analyzing.\nThe choice of sample size can significantly impact the outcomes of data analysis and machine learning models. A larger sample size can lead to more reliable and stable results, but it also requires more computational resources.\nIn the context of your simulated dataset, this means you will be generating data for 10,000 observations or entries.\n\n\nmm &lt;- tibble(\n  Gender    = sample(c(\"Male\", \"Female\"), n, replace = TRUE), \n  Education = sample(c(\"2.Middle\",  \"1.High\", \"0.Univ\"), n, replace = TRUE), \n  Age       = sample(c(30:70), n, replace=TRUE), \n  Income    = sample(c(1:10)*100, n, replace = TRUE ), \n  working_h = sample(c(35:65), n, replace = TRUE),\n) %&gt;% \n  mutate(agegp = case_when(\n    Age &lt;30 ~ \"&lt;30\", \n    Age &lt;40 ~ \"&lt;40\", \n    Age &lt;50 ~ \"&lt;50\", \n    Age &lt;60 ~ \"&lt;60\", \n    TRUE ~ \"\\u226560\" \n  )) %&gt;%\n  mutate(incgp = case_when(\n    Income &lt; 300 ~ \"&lt;300\", \n    Income &lt; 500 ~ \"&lt;500\", \n    Income &lt; 700 ~ \"&lt;700\", \n    TRUE ~ \"\\u2265700\"\n  )) %&gt;%\n  mutate(whgp = case_when(\n    working_h &lt; 45 ~ \"&lt;45\", \n    working_h &lt; 55 ~ \"&lt;55\", \n    TRUE ~ \"\\u226555\"\n  )) %&gt;%\n  cbind(.,\n        replicate(9, integer(n)) %&gt;%\n          data.frame(.) %&gt;% setNames(paste0(\"Q\", 1:9))\n)\n\nFollowing code performs a series of conditional transformations on the PHQ-9 questionnaire responses, based on demographic and personal characteristics like gender, age, education, and working hours. Gender-Based Transformation: Modifies PHQ-9 responses (Q columns) with gender-specific probability distributions. Age-Based Adjustment: Adjusts Q responses according to age groups using varied probabilities. Education-Based Adjustment: Alters Q responses based on education levels with different probabilities. Working Hours Adjustment: Changes Q responses according to working hours, using distinct probability distributions. Final Adjustment of Q Responses: Maintains zero responses; others are reduced based on probability.\n\nmm= mm %&gt;% tibble() %&gt;%\n  mutate(across(\n    .cols = starts_with(\"Q\"), \n    .fns  = ~ case_when(\n      Gender == \"Male\" ~ sample(0:3, length(.), replace = TRUE, prob = c(0.8,  0.1, 0.05, 0.05)), \n      TRUE             ~ sample(0:3, length(.), replace = TRUE, prob = c(0.7,  0.15, 0.1, 0.05))\n    )\n  )) %&gt;%\n  mutate(across(\n    .cols = starts_with(\"Q\"), \n    .fns  = ~ case_when(\n      Age &lt;30 ~ . + sample(0:1, length(.), replace = TRUE, prob = c(0.90,  0.1)),\n      Age &lt;40 ~ . + sample(0:1, length(.), replace = TRUE, prob = c(0.85,  0.15)), \n      Age &lt;50 ~ . + sample(0:1, length(.), replace = TRUE, prob = c(0.80,  0.2)), \n      Age &lt;60 ~ . + sample(0:1, length(.), replace = TRUE, prob = c(0.60,  0.4)), \n      TRUE    ~ . + sample(0:1, length(.), replace = TRUE, prob = c(0.40,  0.6))\n  ))) %&gt;%\n  mutate(across(\n    .cols = starts_with(\"Q\"), \n    .fns  = ~ case_when(\n      Education == \"0.Univ\"   ~ . + sample(0:1, length(.), replace = TRUE, prob = c(0.95,  0.05)),\n      Education == \"1.High\"   ~ . + sample(0:1, length(.), replace = TRUE, prob = c(0.90,  0.1)),\n      Education == \"2.Middle\" ~ . + sample(0:1, length(.), replace = TRUE, prob = c(0.85,  0.15)),\n    )\n  )) %&gt;%\n  mutate(across(\n    .cols = starts_with(\"Q\"), \n    .fns  = ~ case_when(\n      working_h &lt;45 ~ . + sample(0:1, length(.), replace = TRUE, prob = c(0.95, 0.05)), \n      working_h &lt;55 ~ . + sample(0:1, length(.), replace = TRUE, prob = c(0.90, 0.1)), \n      TRUE          ~ . + sample(0:1, length(.), replace = TRUE, prob = c(0.70, 0.3))\n    )\n  )) %&gt;%\n  mutate(across(\n    .cols = starts_with(\"Q\"), \n    .fns  = ~ case_when(\n      . == 0 ~ 0, \n      TRUE   ~ . - sample(0:1, length(.), replace = TRUE, prob = c(0.8, 0.2))\n    )\n  )) %&gt;%\n  mutate(phqsum = rowSums(across(starts_with(\"Q\")))) %&gt;%\n  mutate(phq_level =case_when(\n    phqsum &lt;= 4  ~ \"0.None\",\n    phqsum &lt;= 9  ~ \"1.Mild\", \n    phqsum &lt;= 14 ~ \"2.Moderate\", \n    phqsum &lt;= 19 ~ \"3.Moderate Severe\", \n    TRUE         ~ \"4.Severe\", \n  )) %&gt;%\n  mutate(depressive = ifelse(phqsum &gt;=10, \"Depressive\", \"None\"))"
  },
  {
    "objectID": "720_Supervised_01.html#supervised-machine-learning",
    "href": "720_Supervised_01.html#supervised-machine-learning",
    "title": "19  지도학습",
    "section": "19.2 supervised machine learning",
    "text": "19.2 supervised machine learning\n\n19.2.1 train vs test\nTraining Set\n\nPurpose: The training set is used to train or fit the machine learning model. This means the model learns the patterns, relationships, or features from this subset of the dataset.\nSize: Typically, a larger portion of the dataset is allocated to training (commonly 70-80%) because the model’s performance largely depends on the amount and quality of data it learns from.\nUsage: During training, the model makes predictions or classifications based on the input features and adjusts its internal parameters (in case of algorithms like neural networks) or criteria (like in decision trees) to reduce errors, based on a pre-defined loss function.\n\nTesting Set\n\nPurpose: The testing set is used to evaluate the performance and generalizability of the trained model. It acts as new, unseen data for the model.\nSize: A smaller portion of the dataset (usually 20-30%) is reserved for testing. The key is to have enough data to confidently assess the model’s performance but not so much that it compromises the training set size.\nUsage: The model, after being trained, is used to make predictions on the testing set. These predictions are then compared against the actual outcomes (labels) in the testing set to evaluate metrics like accuracy, precision, recall, F1-score, etc., depending on the problem type (classification or regression).\n\nImportance of Splitting\n\nOverfitting Prevention: By having a separate testing set, you can ensure that your model hasn’t just memorized the training data but can generalize well to new data. Overfitting occurs when a model performs exceptionally well on the training data but poorly on new, unseen data.\nModel Evaluation: The testing set provides a realistic assessment of the model’s performance in real-world scenarios.\nBias Reduction: Separating data into training and testing sets helps in reducing bias. It ensures that the model is evaluated on a different set of data than it was trained on.\n\nTechniques\n\nRandom Splitting: As seen in your code, datasets are often randomly split into training and testing sets.\nStratified Splitting: In scenarios where the dataset is imbalanced (e.g., one class is significantly underrepresented), stratified sampling ensures that the train and test sets have approximately the same percentage of samples of each target class as the original dataset.\nCross-Validation: Beyond simple train-test splits, cross-validation techniques like k-fold cross-validation are used for a more robust evaluation, where the dataset is divided into ‘k’ subsets, and the model is trained and evaluated ‘k’ times, each time with a different subset as the testing set.\n\n\n\nstep 1\n\nNow, following code focusing on the crucial step of splitting the dataset into training and testing subsets.\n\nlibrary(caret)\nset.seed(2023)\nmm1 = mm %&gt;% \n  select(phqsum, Gender, Education, Age, Income, working_h)\ntrainingIndex &lt;- createDataPartition(mm1$phqsum, p = .8, list = FALSE)\ntrain_data &lt;- mm1[+trainingIndex, ]\ntest_data  &lt;- mm1[-trainingIndex, ]\n\nImporting the caret Package:\n\nThe caret package, a comprehensive framework for building machine learning models in R, is loaded.\n\nSetting a Seed for Reproducibility:\n\nset.seed(2023) ensures that the random processes can be replicated.\n\nData Preparation:\n\nmm1 is created by selecting specific columns from the mm dataset. These columns include the PHQ-9 sum (phqsum) and other demographic variables.\n\nSplitting the Dataset:\n\ncreateDataPartition: This function from the caret package is used to create indices for splitting the dataset. It divides the data into training (80%) and testing (20%) sets based on the phqsum column.\ntrainingIndex: Stores the indices of rows for the training set. train_data and test_data: The original dataset mm1 is split into training and testing subsets using these indices.\n\n\n\nstep 2\n\nModel Creation with lm():\n\nlm(data=., formula = phqsum ~ .): This line of code specifies the linear regression model. The lm() function is used for fitting linear models.\ndata=.: The dot (.) indicates that the data for the model will be the current dataset in the pipeline, which in this case is train_data.\nformula = phqsum ~ .: This formula specifies that phqsum is the dependent variable (the variable being predicted), and the tilde (~) followed by a dot means that all other columns in train_data are used as independent variables (predictors).\n\n\nmodel &lt;- train_data %&gt;%\n  lm(data=., \n     formula =  phqsum ~ . )\nsummary(model)\n\n\nCall:\nlm(formula = phqsum ~ ., data = .)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.922 -2.243 -0.246  2.024 14.358 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -2.1405458  0.2658471  -8.052 9.34e-16 ***\nGenderMale        -1.2060166  0.0703062 -17.154  &lt; 2e-16 ***\nEducation1.High    0.4247050  0.0860791   4.934 8.22e-07 ***\nEducation2.Middle  0.8875284  0.0864575  10.265  &lt; 2e-16 ***\nAge                0.1144818  0.0029737  38.498  &lt; 2e-16 ***\nIncome             0.0002444  0.0001221   2.002   0.0454 *  \nworking_h          0.0904169  0.0039128  23.108  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.143 on 7995 degrees of freedom\nMultiple R-squared:  0.2317,    Adjusted R-squared:  0.2312 \nF-statistic: 401.9 on 6 and 7995 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nstep 3\n\nAfter fitting a linear regression model to the training data, the next crucial step in supervised machine learning is to evaluate the model’s performance. This is typically done by making predictions on a separate test dataset and then calculating error metrics. Here’s how the provided R code snippet accomplishes this:\n\npredictions &lt;- predict(model, test_data)\n\n\npredict(model, test_data): This function is used to make predictions using the model (linear regression model) that you’ve trained. It applies the model to the test_data to forecast the outcomes.\npredictions: The output of the predict function, which contains the predicted values of the dependent variable (phqsum) for each observation in the test dataset.\n\nCalculating Error Meatrix\n\nMean Squared Error (MSE):\n\nmse[[“linear”]]: MSE is a measure of the average squared difference between the actual and predicted values. It’s calculated by taking the mean of the squared differences (predictions - test_data$phqsum)^2.\nA lower MSE indicates a better fit of the model to the data.\n\nRoot Mean Squared Error (RMSE):\n\nrmse[[“linear”]]: RMSE is the square root of the MSE. It’s a popular metric because it scales the error to be more interpretable, as it’s in the same units as the dependent variable.\nLike MSE, a lower RMSE signifies a better model fit.\n\n\n\nmse = list()\nrmse= list()\nmse[[\"linear\"]] &lt;- mean((predictions - test_data$phqsum)^2)\nrmse[[\"linear\"]] &lt;- sqrt(mse[[\"linear\"]])\n\n\nmse\n\n$linear\n[1] 9.967748\n\nrmse\n\n$linear\n[1] 3.157174\n\n\n\n\n19.2.2 random forest\nRandom Forest is a powerful machine learning method that utilizes multiple decision trees to make predictions. It’s particularly useful for handling large datasets with complex structures. This chapter will guide you through the process of implementing a Random Forest model in R, evaluating its performance, and comparing it with other models.\nWhat is Random Forest?\n\nEnsemble Learning Method: Random Forest is an ensemble learning technique. Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\nBased on Decision Trees: Specifically, Random Forest builds numerous decision trees and merges them together to get a more accurate and stable prediction. Each tree in a Random Forest is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.\nHandling Overfitting: One of the biggest problems in machine learning is overfitting, but most of the time, this won’t happen thanks to the way Random Forest combines the trees. By averaging or combining the results of different trees, it reduces the risk of overfitting.\n\nHow Does Random Forest Work?\n\nRandom Selection of Features: When building each tree, Random Forest randomly selects a subset of the features at each split in the decision tree. This adds an additional layer of randomness to the model, compared to a single decision tree.\nCreating Multiple Trees: It creates a forest of trees where each tree is slightly different from the others. When it’s time to make a prediction, the Random Forest takes an average of all the individual decision tree estimates.\nAdvantages: This process of averaging or combining the results helps to improve accuracy and control overfitting. Random Forest can handle both regression and classification tasks and works well with both categorical and continuous variables.\nHandling Missing Values: Random Forest can also handle missing values by imputing them.\nVariable Importance: It provides a straightforward indicator of the importance of each feature in the prediction.\n\nSetting Up for Parallel Computing\n\n# Register the parallel backend\nnumCores &lt;- parallel::detectCores()\nregisterDoParallel(cores = numCores - 1)\n\n\nParallel Computing: To speed up the computation, especially for complex models like Random Forest, we use parallel processing.\nparallel::detectCores(): Detects the number of CPU cores in your machine.\nregisterDoParallel(cores = numCores - 1): Registers the number of cores to be used for parallel processing. We leave one core free for other tasks.\n\nConfiguring the Training Process\n\ntrain_control &lt;- trainControl(method = \"repeatedcv\", \n                              number = 10, \n                              repeats = 3, \n                              allowParallel = TRUE)\n\nRepeated Cross-Validation: A Closer Look at 10-Fold Cross-Validation In machine learning, cross-validation is a technique used to assess how the results of a statistical analysis will generalize to an independent data set. It is mainly used to predict the fit of a model to a hypothetical validation set when an explicit validation set is not available.\nWhat is 10-Fold Cross-Validation?\n\nDefinition: 10-fold cross-validation is a method where the data set is randomly divided into 10 subsets (or ‘folds’). Each subset contains roughly the same proportion of the sample.\nProcess:\n\nIn each round of validation, one of the 10 subsets is used as the validation set (to test the model), and the other nine subsets are combined to form the training set (to train the model).\nThis process is repeated 10 times, each time with a different subset serving as the validation set.\nThe results from the 10 folds can then be averaged (or otherwise combined) to produce a single estimation.\n\n\nWhy Use 10-Fold Cross-Validation?\n\nBias Reduction: By using different subsets as the validation set at different times, 10-fold cross-validation reduces the bias associated with the random selection of a single train-test split.\nVariance Insight: This method provides insight into how the model’s prediction might vary with respect to the data used for training. It offers a more comprehensive view of the model’s performance.\nGeneralization: The averaged result is a better estimate of how the model will perform on an independent dataset.\n\nRepeated Cross-Validation\n\nIn your specific case, this process is repeated 3 times. This repetition means the entire process of 10-fold cross-validation is conducted three times, each time with a different random division of the original dataset into 10 folds.\nThis repetition helps to further mitigate variability in the estimation of model performance, leading to a more robust understanding of how well your model is likely to perform on unseen data.\n\n10-fold cross-validation is a reliable method for assessing model performance. By using it repeatedly, you substantially increase the robustness and reliability of your model assessment, ensuring that your model not only fits your current data well but also holds up well against new, unseen data.\nTraining the Random Forest Model\n\nModel Formula: phqsum ~ . indicates that phqsum is predicted using all other variables in train_data.\nIn the context of your R code for training a machine learning model, method = “rf” specifies the use of a Random Forest algorithm.\n\n\ntic()\nmodel_rf &lt;- train(phqsum ~ ., data = train_data, \n                  method = \"rf\",\n                  trControl = train_control)  \ntoc()\n\nEvaluating Random Forest Model Performance: Predictions and Error Metrics\nAfter training a Random Forest model (model_rf) on your training data (train_data), it’s essential to evaluate its performance. This evaluation is done by making predictions on a separate test dataset and calculating error metrics like Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).\nMaking Predictions on the Test Set\n\n# Make predictions on the test set\npredictions_rf &lt;- predict(model_rf, test_data)\n\n\npredict(model_rf, test_data): This function uses the trained Random Forest model (model_rf) to predict outcomes based on the test_data.\npredictions_rf: The result of the predict function, which contains the predicted values for the dependent variable (phqsum) for each observation in the test dataset.\n\nCalculating MSE and RMSE\n\n# Calculate MSE and RMSE\nmse[[\"rf\"]] &lt;- mean((predictions_rf - test_data$phqsum)^2)\nrmse[[\"rf\"]] &lt;- sqrt(mse[[\"rf\"]])\nmse\n\n$linear\n[1] 9.967748\n\n$rf\n[1] 9.878868\n\nrmse\n\n$linear\n[1] 3.157174\n\n$rf\n[1] 3.143067\n\n\n\n\n19.2.3 generalized additive model\nGeneralized Additive Models (GAMs) are a flexible class of models used in statistics and machine learning. They extend linear models by allowing non-linear functions of the predictor variables while maintaining interpretability\nSetting Up the Environment\n\nif(!require(\"mgcv\")) install.packages(\"mgcv\")\nlibrary(mgcv)\nnames(train_data)\n\n[1] \"phqsum\"    \"Gender\"    \"Education\" \"Age\"       \"Income\"    \"working_h\"\n\n\nTraining the GAM\n\ngam Function: This function from the mgcv package is used to fit a GAM.\nModel Formula: phqsum ~ s(Age) + Gender + Education + s(Income) + s(working_h) specifies the model’s structure.\ns(): Indicates a smooth term, allowing for a non-linear relationship between the predictor and the outcome.\nOther variables (Gender, Education) are included as linear predictors.\nData: The data=. syntax indicates that the model uses the train_data dataset.\n\n\nmodel_gam = train_data %&gt;%\n  gam(data=., \n      phqsum ~ s(Age) + Gender + Education + s(Income) + s(working_h))\n\nMaking Predictions and Evaluating Performance\n\npredictions_gam &lt;- predict(model_gam, newdata=test_data)\nmse[[\"gam\"]] &lt;- mean((predictions_gam - test_data$phqsum)^2)\nrmse[[\"gam\"]] &lt;- sqrt(mse[[\"gam\"]])\nmse\n\n$linear\n[1] 9.967748\n\n$rf\n[1] 9.878868\n\n$gam\n[1] 9.66441\n\nrmse\n\n$linear\n[1] 3.157174\n\n$rf\n[1] 3.143067\n\n$gam\n[1] 3.108763\n\n\n\n\n19.2.4 Ridge and Lasso Regression\nRidge and Lasso regression are two widely used regularization techniques in machine learning and statistics. They are particularly useful when dealing with multicollinearity or when you have more features than observations. Both methods modify the least squares objective function by adding a penalty term, which helps in controlling overfitting.\nSetting Up the Environment and Preparing Data\n\nif(!require(\"glmnet\")) install.packages(\"glmnet\")\nlibrary(glmnet)\n\n\nglmnet Package: This package is essential for fitting generalized linear models via penalized maximum likelihood. It’s installed and loaded for use.\n\n\n# Convert categorical variables to dummy variables\ntrain_data_matrix &lt;- model.matrix(phqsum ~ ., data = train_data)[, -1]  # Exclude intercept column\ntest_data_matrix &lt;- model.matrix(phqsum ~ ., data = test_data)[, -1]\n\n\nData Preparation: The model.matrix function converts categorical variables into dummy/indicator variables for regression analysis. The intercept column is excluded.\n\n\n# Define the response variable\ny_train &lt;- train_data$phqsum\ny_test &lt;- test_data$phqsum\n\n\nResponse Variable: The dependent variable (phqsum) is extracted from both training and testing datasets.\n\nFitting Ridge and Lasso Regression Models\n\n# Ridge Regression\nmodel_ridge &lt;- glmnet(train_data_matrix, y_train, alpha = 0)\n# Lasso Regression\nmodel_lasso &lt;- glmnet(train_data_matrix, y_train, alpha = 1)\n\n\nRidge Regression (alpha = 0): Adds a penalty equal to the square of the magnitude of coefficients.\nLasso Regression (alpha = 1): Adds a penalty equal to the absolute value of the magnitude of coefficients.\n\nSelecting the Best Lambda (Penalty Parameter)\n\nset.seed(123)  # For reproducibility\ncv_ridge &lt;- cv.glmnet(train_data_matrix, y_train, alpha = 0)\ncv_lasso &lt;- cv.glmnet(train_data_matrix, y_train, alpha = 1)\nbest_lambda_ridge &lt;- cv_ridge$lambda.min\nbest_lambda_lasso &lt;- cv_lasso$lambda.min\n\n\nCross-Validation: Determines the best lambda (penalty parameter) for each model. cv.glmnet: Performs cross-validation for glmnet models.\n\nMaking Predictions and Evaluating Models\n\n# Predictions\npredictions_ridge &lt;- predict(model_ridge, s = best_lambda_ridge, newx = test_data_matrix)\npredictions_lasso &lt;- predict(model_lasso, s = best_lambda_lasso, newx = test_data_matrix)\n\nChoosing Between Ridge and Lasso\n\nPredictive Performance: The choice often depends on the dataset and the problem at hand. It’s common to try both and compare their performance.\nCross-Validation: Using cross-validation to find the optimal λ is crucial in both methods. This helps in balancing the trade-off between bias and variance.\n\n\n# Evaluation\nmse[[\"ridge\"]] &lt;- mean((predictions_ridge - y_test)^2)\nrmse[[\"ridge\"]] &lt;- sqrt(mse[[\"ridge\"]])\nmse[[\"lasso\"]] &lt;- mean((predictions_lasso - y_test)^2)\nrmse[[\"lasso\"]] &lt;- sqrt(mse[[\"lasso\"]])\n\n\ndo.call(cbind, list(mse, rmse)) %&gt;% data.frame() %&gt;%\n  setNames(c(\"mse\", \"rmse\"))\n\n            mse     rmse\nlinear 9.967748 3.157174\nrf     9.878868 3.143067\ngam     9.66441 3.108763\nridge  9.974757 3.158284\nlasso  9.966269  3.15694"
  },
  {
    "objectID": "720_Supervised_01.html#prediction-of-continus-outcome",
    "href": "720_Supervised_01.html#prediction-of-continus-outcome",
    "title": "19  지도학습",
    "section": "19.2 prediction of continus outcome",
    "text": "19.2 prediction of continus outcome\n\n19.2.1 train vs test\nTraining Set\n\nPurpose: The training set is used to train or fit the machine learning model. This means the model learns the patterns, relationships, or features from this subset of the dataset.\nSize: Typically, a larger portion of the dataset is allocated to training (commonly 70-80%) because the model’s performance largely depends on the amount and quality of data it learns from.\nUsage: During training, the model makes predictions or classifications based on the input features and adjusts its internal parameters (in case of algorithms like neural networks) or criteria (like in decision trees) to reduce errors, based on a pre-defined loss function.\n\nTesting Set\n\nPurpose: The testing set is used to evaluate the performance and generalizability of the trained model. It acts as new, unseen data for the model.\nSize: A smaller portion of the dataset (usually 20-30%) is reserved for testing. The key is to have enough data to confidently assess the model’s performance but not so much that it compromises the training set size.\nUsage: The model, after being trained, is used to make predictions on the testing set. These predictions are then compared against the actual outcomes (labels) in the testing set to evaluate metrics like accuracy, precision, recall, F1-score, etc., depending on the problem type (classification or regression).\n\nImportance of Splitting\n\nOverfitting Prevention: By having a separate testing set, you can ensure that your model hasn’t just memorized the training data but can generalize well to new data. Overfitting occurs when a model performs exceptionally well on the training data but poorly on new, unseen data.\nModel Evaluation: The testing set provides a realistic assessment of the model’s performance in real-world scenarios.\nBias Reduction: Separating data into training and testing sets helps in reducing bias. It ensures that the model is evaluated on a different set of data than it was trained on.\n\nTechniques\n\nRandom Splitting: As seen in your code, datasets are often randomly split into training and testing sets.\nStratified Splitting: In scenarios where the dataset is imbalanced (e.g., one class is significantly underrepresented), stratified sampling ensures that the train and test sets have approximately the same percentage of samples of each target class as the original dataset.\nCross-Validation: Beyond simple train-test splits, cross-validation techniques like k-fold cross-validation are used for a more robust evaluation, where the dataset is divided into ‘k’ subsets, and the model is trained and evaluated ‘k’ times, each time with a different subset as the testing set.\n\n\nNow, following code focusing on the crucial step of splitting the dataset into training and testing subsets.\n\nlibrary(caret)\nset.seed(2023)\nmm1 = mm %&gt;% \n  select(phqsum, Gender, Education, Age, Income, working_h)\ntrainingIndex &lt;- createDataPartition(mm1$phqsum, p = .8, list = FALSE)\ntrain_data &lt;- mm1[+trainingIndex, ]\ntest_data  &lt;- mm1[-trainingIndex, ]\n\nImporting the caret Package:\n\nThe caret package, a comprehensive framework for building machine learning models in R, is loaded.\n\nSetting a Seed for Reproducibility:\n\nset.seed(2023) ensures that the random processes can be replicated.\n\nData Preparation:\n\nmm1 is created by selecting specific columns from the mm dataset. These columns include the PHQ-9 sum (phqsum) and other demographic variables.\n\nSplitting the Dataset:\n\ncreateDataPartition: This function from the caret package is used to create indices for splitting the dataset. It divides the data into training (80%) and testing (20%) sets based on the phqsum column.\ntrainingIndex: Stores the indices of rows for the training set. train_data and test_data: The original dataset mm1 is split into training and testing subsets using these indices.\n\n\n\n\n19.2.2 linear regression\nModel Creation with lm():\n\nlm(data=., formula = phqsum ~ .): This line of code specifies the linear regression model. The lm() function is used for fitting linear models.\ndata=.: The dot (.) indicates that the data for the model will be the current dataset in the pipeline, which in this case is train_data.\nformula = phqsum ~ .: This formula specifies that phqsum is the dependent variable (the variable being predicted), and the tilde (~) followed by a dot means that all other columns in train_data are used as independent variables (predictors).\n\n\nmodel &lt;- train_data %&gt;%\n  lm(data=., \n     formula =  phqsum ~ . )\nsummary(model)\n\n\nCall:\nlm(formula = phqsum ~ ., data = .)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.922 -2.243 -0.246  2.024 14.358 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -2.1405458  0.2658471  -8.052 9.34e-16 ***\nGenderMale        -1.2060166  0.0703062 -17.154  &lt; 2e-16 ***\nEducation1.High    0.4247050  0.0860791   4.934 8.22e-07 ***\nEducation2.Middle  0.8875284  0.0864575  10.265  &lt; 2e-16 ***\nAge                0.1144818  0.0029737  38.498  &lt; 2e-16 ***\nIncome             0.0002444  0.0001221   2.002   0.0454 *  \nworking_h          0.0904169  0.0039128  23.108  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.143 on 7995 degrees of freedom\nMultiple R-squared:  0.2317,    Adjusted R-squared:  0.2312 \nF-statistic: 401.9 on 6 and 7995 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nstep 3\n\nAfter fitting a linear regression model to the training data, the next crucial step in supervised machine learning is to evaluate the model’s performance. This is typically done by making predictions on a separate test dataset and then calculating error metrics. Here’s how the provided R code snippet accomplishes this:\n\npredictions &lt;- predict(model, test_data)\n\n\npredict(model, test_data): This function is used to make predictions using the model (linear regression model) that you’ve trained. It applies the model to the test_data to forecast the outcomes.\npredictions: The output of the predict function, which contains the predicted values of the dependent variable (phqsum) for each observation in the test dataset.\n\nCalculating Error Meatrix\n\nMean Squared Error (MSE):\n\nmse[[“linear”]]: MSE is a measure of the average squared difference between the actual and predicted values. It’s calculated by taking the mean of the squared differences (predictions - test_data$phqsum)^2.\nA lower MSE indicates a better fit of the model to the data.\n\nRoot Mean Squared Error (RMSE):\n\nrmse[[“linear”]]: RMSE is the square root of the MSE. It’s a popular metric because it scales the error to be more interpretable, as it’s in the same units as the dependent variable.\nLike MSE, a lower RMSE signifies a better model fit.\n\n\n\nmse = list()\nrmse= list()\nmse[[\"linear\"]] &lt;- mean((predictions - test_data$phqsum)^2)\nrmse[[\"linear\"]] &lt;- sqrt(mse[[\"linear\"]])\n\n\nmse\n\n$linear\n[1] 9.967748\n\nrmse\n\n$linear\n[1] 3.157174\n\n\n\n\n19.2.3 random forest\nRandom Forest is a powerful machine learning method that utilizes multiple decision trees to make predictions. It’s particularly useful for handling large datasets with complex structures. This chapter will guide you through the process of implementing a Random Forest model in R, evaluating its performance, and comparing it with other models.\nWhat is Random Forest?\n\nEnsemble Learning Method: Random Forest is an ensemble learning technique. Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\nBased on Decision Trees: Specifically, Random Forest builds numerous decision trees and merges them together to get a more accurate and stable prediction. Each tree in a Random Forest is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.\nHandling Overfitting: One of the biggest problems in machine learning is overfitting, but most of the time, this won’t happen thanks to the way Random Forest combines the trees. By averaging or combining the results of different trees, it reduces the risk of overfitting.\n\nHow Does Random Forest Work?\n\nRandom Selection of Features: When building each tree, Random Forest randomly selects a subset of the features at each split in the decision tree. This adds an additional layer of randomness to the model, compared to a single decision tree.\nCreating Multiple Trees: It creates a forest of trees where each tree is slightly different from the others. When it’s time to make a prediction, the Random Forest takes an average of all the individual decision tree estimates.\nAdvantages: This process of averaging or combining the results helps to improve accuracy and control overfitting. Random Forest can handle both regression and classification tasks and works well with both categorical and continuous variables.\nHandling Missing Values: Random Forest can also handle missing values by imputing them.\nVariable Importance: It provides a straightforward indicator of the importance of each feature in the prediction.\n\nSetting Up for Parallel Computing\n\n# Register the parallel backend\nnumCores &lt;- parallel::detectCores()\nregisterDoParallel(cores = numCores - 1)\n\n\nParallel Computing: To speed up the computation, especially for complex models like Random Forest, we use parallel processing.\nparallel::detectCores(): Detects the number of CPU cores in your machine.\nregisterDoParallel(cores = numCores - 1): Registers the number of cores to be used for parallel processing. We leave one core free for other tasks.\n\nConfiguring the Training Process\n\ntrain_control &lt;- trainControl(method = \"repeatedcv\", \n                              number = 10, \n                              repeats = 3, \n                              allowParallel = TRUE)\n\nRepeated Cross-Validation: A Closer Look at 10-Fold Cross-Validation In machine learning, cross-validation is a technique used to assess how the results of a statistical analysis will generalize to an independent data set. It is mainly used to predict the fit of a model to a hypothetical validation set when an explicit validation set is not available.\nWhat is 10-Fold Cross-Validation?\n\nDefinition: 10-fold cross-validation is a method where the data set is randomly divided into 10 subsets (or ‘folds’). Each subset contains roughly the same proportion of the sample.\nProcess:\n\nIn each round of validation, one of the 10 subsets is used as the validation set (to test the model), and the other nine subsets are combined to form the training set (to train the model).\nThis process is repeated 10 times, each time with a different subset serving as the validation set.\nThe results from the 10 folds can then be averaged (or otherwise combined) to produce a single estimation.\n\n\nWhy Use 10-Fold Cross-Validation?\n\nBias Reduction: By using different subsets as the validation set at different times, 10-fold cross-validation reduces the bias associated with the random selection of a single train-test split.\nVariance Insight: This method provides insight into how the model’s prediction might vary with respect to the data used for training. It offers a more comprehensive view of the model’s performance.\nGeneralization: The averaged result is a better estimate of how the model will perform on an independent dataset.\n\nRepeated Cross-Validation\n\nIn your specific case, this process is repeated 3 times. This repetition means the entire process of 10-fold cross-validation is conducted three times, each time with a different random division of the original dataset into 10 folds.\nThis repetition helps to further mitigate variability in the estimation of model performance, leading to a more robust understanding of how well your model is likely to perform on unseen data.\n\n10-fold cross-validation is a reliable method for assessing model performance. By using it repeatedly, you substantially increase the robustness and reliability of your model assessment, ensuring that your model not only fits your current data well but also holds up well against new, unseen data.\nTraining the Random Forest Model\n\nModel Formula: phqsum ~ . indicates that phqsum is predicted using all other variables in train_data.\nIn the context of your R code for training a machine learning model, method = “rf” specifies the use of a Random Forest algorithm.\n\n\ntic()\nmodel_rf &lt;- train(phqsum ~ ., data = train_data, \n                  method = \"rf\",\n                  trControl = train_control)  \ntoc()\n\nEvaluating Random Forest Model Performance: Predictions and Error Metrics\nAfter training a Random Forest model (model_rf) on your training data (train_data), it’s essential to evaluate its performance. This evaluation is done by making predictions on a separate test dataset and calculating error metrics like Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).\nMaking Predictions on the Test Set\n\n# Make predictions on the test set\npredictions_rf &lt;- predict(model_rf, test_data)\n\n\npredict(model_rf, test_data): This function uses the trained Random Forest model (model_rf) to predict outcomes based on the test_data.\npredictions_rf: The result of the predict function, which contains the predicted values for the dependent variable (phqsum) for each observation in the test dataset.\n\nCalculating MSE and RMSE\n\n# Calculate MSE and RMSE\nmse[[\"rf\"]] &lt;- mean((predictions_rf - test_data$phqsum)^2)\nrmse[[\"rf\"]] &lt;- sqrt(mse[[\"rf\"]])\nmse\n\n$linear\n[1] 9.967748\n\n$rf\n[1] 9.878868\n\nrmse\n\n$linear\n[1] 3.157174\n\n$rf\n[1] 3.143067\n\n\n\n\n19.2.4 generalized additive model\nGeneralized Additive Models (GAMs) are a flexible class of models used in statistics and machine learning. They extend linear models by allowing non-linear functions of the predictor variables while maintaining interpretability\nSetting Up the Environment\n\nif(!require(\"mgcv\")) install.packages(\"mgcv\")\nlibrary(mgcv)\nnames(train_data)\n\n[1] \"phqsum\"    \"Gender\"    \"Education\" \"Age\"       \"Income\"    \"working_h\"\n\n\nTraining the GAM\n\ngam Function: This function from the mgcv package is used to fit a GAM.\nModel Formula: phqsum ~ s(Age) + Gender + Education + s(Income) + s(working_h) specifies the model’s structure.\ns(): Indicates a smooth term, allowing for a non-linear relationship between the predictor and the outcome.\nOther variables (Gender, Education) are included as linear predictors.\nData: The data=. syntax indicates that the model uses the train_data dataset.\n\n\nmodel_gam = train_data %&gt;%\n  gam(data=., \n      phqsum ~ s(Age) + Gender + Education + s(Income) + s(working_h))\n\nMaking Predictions and Evaluating Performance\n\npredictions_gam &lt;- predict(model_gam, newdata=test_data)\nmse[[\"gam\"]] &lt;- mean((predictions_gam - test_data$phqsum)^2)\nrmse[[\"gam\"]] &lt;- sqrt(mse[[\"gam\"]])\nmse\n\n$linear\n[1] 9.967748\n\n$rf\n[1] 9.878868\n\n$gam\n[1] 9.66441\n\nrmse\n\n$linear\n[1] 3.157174\n\n$rf\n[1] 3.143067\n\n$gam\n[1] 3.108763\n\n\n\n\n19.2.5 Ridge and Lasso Regression\nRidge and Lasso regression are two widely used regularization techniques in machine learning and statistics. They are particularly useful when dealing with multicollinearity or when you have more features than observations. Both methods modify the least squares objective function by adding a penalty term, which helps in controlling overfitting.\nSetting Up the Environment and Preparing Data\n\nif(!require(\"glmnet\")) install.packages(\"glmnet\")\nlibrary(glmnet)\n\n\nglmnet Package: This package is essential for fitting generalized linear models via penalized maximum likelihood. It’s installed and loaded for use.\n\n\n# Convert categorical variables to dummy variables\ntrain_data_matrix &lt;- model.matrix(phqsum ~ ., data = train_data)[, -1]  # Exclude intercept column\ntest_data_matrix &lt;- model.matrix(phqsum ~ ., data = test_data)[, -1]\n\n\nData Preparation: The model.matrix function converts categorical variables into dummy/indicator variables for regression analysis. The intercept column is excluded.\n\n\n# Define the response variable\ny_train &lt;- train_data$phqsum\ny_test &lt;- test_data$phqsum\n\n\nResponse Variable: The dependent variable (phqsum) is extracted from both training and testing datasets.\n\nFitting Ridge and Lasso Regression Models\n\n# Ridge Regression\nmodel_ridge &lt;- glmnet(train_data_matrix, y_train, alpha = 0)\n# Lasso Regression\nmodel_lasso &lt;- glmnet(train_data_matrix, y_train, alpha = 1)\n\n\nRidge Regression (alpha = 0): Adds a penalty equal to the square of the magnitude of coefficients.\nLasso Regression (alpha = 1): Adds a penalty equal to the absolute value of the magnitude of coefficients.\n\nSelecting the Best Lambda (Penalty Parameter)\n\nset.seed(123)  # For reproducibility\ncv_ridge &lt;- cv.glmnet(train_data_matrix, y_train, alpha = 0)\ncv_lasso &lt;- cv.glmnet(train_data_matrix, y_train, alpha = 1)\nbest_lambda_ridge &lt;- cv_ridge$lambda.min\nbest_lambda_lasso &lt;- cv_lasso$lambda.min\n\n\nCross-Validation: Determines the best lambda (penalty parameter) for each model. cv.glmnet: Performs cross-validation for glmnet models.\n\nMaking Predictions and Evaluating Models\n\n# Predictions\npredictions_ridge &lt;- predict(model_ridge, s = best_lambda_ridge, newx = test_data_matrix)\npredictions_lasso &lt;- predict(model_lasso, s = best_lambda_lasso, newx = test_data_matrix)\n\nChoosing Between Ridge and Lasso\n\nPredictive Performance: The choice often depends on the dataset and the problem at hand. It’s common to try both and compare their performance.\nCross-Validation: Using cross-validation to find the optimal λ is crucial in both methods. This helps in balancing the trade-off between bias and variance.\n\n\n# Evaluation\nmse[[\"ridge\"]] &lt;- mean((predictions_ridge - y_test)^2)\nrmse[[\"ridge\"]] &lt;- sqrt(mse[[\"ridge\"]])\nmse[[\"lasso\"]] &lt;- mean((predictions_lasso - y_test)^2)\nrmse[[\"lasso\"]] &lt;- sqrt(mse[[\"lasso\"]])\n\n\ndo.call(cbind, list(mse, rmse)) %&gt;% data.frame() %&gt;%\n  setNames(c(\"mse\", \"rmse\"))\n\n            mse     rmse\nlinear 9.967748 3.157174\nrf     9.878868 3.143067\ngam     9.66441 3.108763\nridge  9.974757 3.158284\nlasso  9.966269  3.15694"
  },
  {
    "objectID": "720_Supervised_01.html#classification",
    "href": "720_Supervised_01.html#classification",
    "title": "19  지도학습",
    "section": "19.3 classification",
    "text": "19.3 classification\n\n19.3.1 logistic regression\n\nStep 1: Load Libraries and Set Seed\n\nIn this step, we start by loading the caret library, which provides functions for training and evaluating machine learning models. We also set a random seed to ensure that our results are reproducible.\n\n# Import necessary libraries\nlibrary(caret)\n# Set a random seed for reproducibility\nset.seed(2023)\n\n\nStep 2: Data Preparation\n\nIn this step, we prepare our data. We select the relevant variables from the dataset, including the target variable depressive. Then, we split the data into a training set (train_data) and a testing set (test_data) using the createDataPartition function. This ensures that we have a separate dataset for model training and evaluation.\n\n# Select relevant variables, including the target variable\nmm1 &lt;- mm %&gt;%\n  select(depressive, Gender, Education, Age, Income, working_h)\n# Split the data into training and testing sets\ntrainingIndex &lt;- createDataPartition(mm1$depressive, p = 0.8, list = FALSE)\ntrain_data &lt;- mm1[+trainingIndex, ]\ntest_data  &lt;- mm1[-trainingIndex, ]\n\n\nStep 3: Model Training - Logistic Regression\n\nHere, we train a logistic regression model using the glm function. This model is used to predict whether a person is “Depressive” or “None” based on the predictor variables (Gender, Education, Age, Income, and working_h). The logistic regression model estimates the probabilities of each class.\n\n# Fit a logistic regression model\nmodel_logistic &lt;- train_data %&gt;%\n  glm(data=., family=binomial(), \n      formula = depressive == \"Depressive\" ~ .)\n\n\nStep 4: Model Evaluation\n\n\n# Make predictions\npredictions_logistic_prob = predict(model_logistic, newdata = test_data, type = \"response\")\npredictions_logistic      = ifelse(predictions_logistic_prob &gt; 0.5, \"Depressive\", \"None\")\n\n# Calculate and store balanced accuracy\nsmry_logi = confusionMatrix(predictions_logistic %&gt;% as.factor(), \n                test_data$depressive %&gt;% as.factor())\nbacu &lt;- list()\nbacu[[\"logistic\"]] &lt;- smry_logi$byClass[[\"Balanced Accuracy\"]]\n\nIn this final step, we evaluate the performance of our logistic regression model:\nWe make predictions using the trained model (model_logistic) and obtain predicted probabilities (predictions_logistic_prob) and binary class predictions (predictions_logistic) based on a threshold of 0.5. We calculate a confusion matrix (smry_logi) to assess the model’s classification performance. Finally, we store the balanced accuracy of the logistic regression model in a list called bacu under the key “logistic.” The balanced accuracy measures how well the model performs in terms of sensitivity and specificity while accounting for class imbalance in the dataset.\n\n\n19.3.2 random forest\nStep 1: Register the Parallel Backend\n\n# Register the parallel backend to speed up computations\nnumCores &lt;- parallel::detectCores()\nregisterDoParallel(cores = numCores - 1)\n\nIn this step, we configure parallel processing to utilize multiple CPU cores for faster model training. The number of available CPU cores is detected using detectCores, and one core is reserved for other system tasks. The registerDoParallel function is used to set up parallel processing.\nStep 2: Define Control Parameters for Training\n\n# Define control parameters for the training process\ntrain_control &lt;- trainControl(method = \"repeatedcv\", \n                              number = 10, \n                              repeats = 3, \n                              classProbs = TRUE,\n                              allowParallel = TRUE)\n\n\nHere, we define control parameters that guide the training process of the Random Forest model:\n\nmethod is set to “repeatedcv,” indicating repeated cross-validation as the resampling method.\nnumber is set to 10, indicating the number of cross-validation folds. repeats is set to 3, indicating the number of times the cross-validation process is repeated.\nclassProbs is set to TRUE, allowing the model to calculate class probabilities.\nallowParallel is set to TRUE, enabling parallel processing for cross-validation.\n\n\nStep 3: Train the Random Forest Model\n\n# Train the Random Forest model\ntic()  # Start measuring time\nmodel_rf_class &lt;- train(depressive ~ ., data = train_data, \n                        method = \"rf\",\n                        trControl = train_control)\ntoc()  # Stop measuring time\n\n\nIn this step, we train the Random Forest classification model:\n\ndepressive ~ . specifies that we want to predict the depressive variable based on all other variables in the train_data dataset.\nmethod is set to “rf” to specify the Random Forest algorithm.\ntrControl is set to the previously defined control parameters (train_control) to govern the training process.\n\n\nWe use tic() and toc() to measure the time taken for model training.\nStep 4: Make Predictions and Evaluate the Model\n\n# Make predictions using the trained model\nprediction_rf_class = predict(model_rf_class, newdata = test_data)\n\n\n# Calculate the confusion matrix and balanced accuracy\nsmry_rf = confusionMatrix(prediction_rf_class %&gt;% as.factor(), \n                         test_data$depressive %&gt;% as.factor())\n# Store the balanced accuracy in the 'bacu' list\nbacu[[\"rf\"]] = smry_rf$byClass[[\"Balanced Accuracy\"]]\n\nMake predictions on the test dataset using the trained Random Forest model. Calculate a confusion matrix (smry_rf) to evaluate the classification performance. Extract the balanced accuracy from the confusion matrix and store it in the bacu list under the key “rf.”\n\n\n19.3.3 Gradient Boosting Machine (GBM) classification\n\nif(!require(\"gbm\")) install.packages(\"gbm\")\n\ntic()\nmodel_gbm_class &lt;- train(depressive ~ ., data = train_data, \n                        method = \"gbm\",\n                        trControl = train_control)  \n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2461            -nan     0.1000    0.0073\n     2        1.2340            -nan     0.1000    0.0058\n     3        1.2229            -nan     0.1000    0.0055\n     4        1.2136            -nan     0.1000    0.0044\n     5        1.2066            -nan     0.1000    0.0034\n     6        1.2002            -nan     0.1000    0.0030\n     7        1.1939            -nan     0.1000    0.0029\n     8        1.1889            -nan     0.1000    0.0023\n     9        1.1834            -nan     0.1000    0.0024\n    10        1.1793            -nan     0.1000    0.0018\n    20        1.1475            -nan     0.1000    0.0013\n    40        1.1201            -nan     0.1000    0.0002\n    60        1.1098            -nan     0.1000    0.0001\n    80        1.1054            -nan     0.1000   -0.0000\n   100        1.1033            -nan     0.1000   -0.0000\n   120        1.1021            -nan     0.1000   -0.0000\n   140        1.1011            -nan     0.1000   -0.0001\n   150        1.1007            -nan     0.1000   -0.0001\n\ntoc()\n\n1.624 sec elapsed\n\nprediction_gbm_class = predict(model_gbm_class, newdata = test_data)\nsmry_gbm = confusionMatrix(prediction_gbm_class %&gt;% as.factor(), \n                          test_data$depressive %&gt;% as.factor())\nbacu[[\"gbm\"]] = smry_gbm$byClass[[\"Balanced Accuracy\"]]\n\n\n\n19.3.4 svmRadial\n\nif(!require(\"kernlab\")) install.packages(\"kernlab\")\n\ntic()\nmodel_svmRadial_class &lt;- train(depressive ~ ., data = train_data, \n                         method = \"svmRadial\",\n                         trControl = train_control)  \ntoc()\n\n24.292 sec elapsed\n\nprediction_svmRadial_class = predict(model_svmRadial_class, newdata = test_data)\nsmry_svmRadial = confusionMatrix(prediction_svmRadial_class %&gt;% as.factor(), \n                           test_data$depressive %&gt;% as.factor())\nbacu[[\"svmRadial\"]] = smry_svmRadial$byClass[[\"Balanced Accuracy\"]]\n\n\n\n19.3.5 knn\n\nif(!require(\"rpart\")) install.packages(\"rpart\")\ntic()\nmodel_tree_class &lt;- train(depressive ~ ., data = train_data, \n                               method = \"rpart\",\n                               trControl = train_control)  \ntoc()\n\n0.823 sec elapsed\n\nprediction_tree_class = predict(model_tree_class, newdata = test_data)\nsmry_tree = confusionMatrix(prediction_tree_class %&gt;% as.factor(), \n                                 test_data$depressive %&gt;% as.factor())\nbacu[[\"tree\"]] = smry_tree$byClass[[\"Balanced Accuracy\"]]\n\n\n\n19.3.6 final model comparision\n\nbacu\n\n$logistic\n[1] 0.6275994\n\n$rf\n[1] 0.6238929\n\n$gbm\n[1] 0.6374802\n\n$svmRadial\n[1] 0.597726\n\n$tree\n[1] 0.6180259"
  },
  {
    "objectID": "730_Supervised_minist_01.html",
    "href": "730_Supervised_minist_01.html",
    "title": "20  지도학습-MNIST (숫자인식)",
    "section": "",
    "text": "#\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"dslabs\")) install.packages(\"dslabs\")\nif(!require(\"caret\")) install.packages(\"caret\")\nif(!require(\"purrr\")) install.packages(\"purrr\")\nif(!require(\"purrr\")) install.packages(\"purrr\")\nif(!require(\"tensorflow\")) install.packages(\"tensorflow\")\nif(!require(\"randomForest\")) install.packages(\"randomForest\")\nif(!require(\"doParallel\")) install.packages(\"doParallel\")\nif(!require(\"foreach\")) install.packages(\"foreach\")\nif(!require(\"tictoc\")) install.packages(\"tictoc\")\nlibrary(tensorflow)\n#install_tensorflow()\nif(!require(\"keras\")) install.packages(\"keras\")\n#install_keras()\nlibrary(keras)\nlibrary(reticulate)\n\n\nuse_virtualenv(\"/home/sehnr/tensorflow/tensorvenv\", required = TRUE)\n\n\nmnist &lt;- dataset_mnist()"
  },
  {
    "objectID": "730_Supervised_minist_01.html#mnist",
    "href": "730_Supervised_minist_01.html#mnist",
    "title": "20  지도학습-MNIST (숫자인식)",
    "section": "20.1 MNIST",
    "text": "20.1 MNIST\nMNIST Dataset:\n\nWhat is MNIST? MNIST stands for “Modified National Institute of Standards and Technology.” It is a dataset of handwritten digits commonly used for training and testing machine learning and computer vision algorithms. The dataset consists of a large number of 28x28 pixel grayscale images of handwritten digits (0 to 9), along with their corresponding labels (the digit each image represents).\nPurpose: The MNIST dataset is often used as a benchmark in machine learning and deep learning tasks, particularly in the context of image classification and digit recognition. It serves as a standardized dataset for evaluating and comparing the performance of various algorithms and models.\nNumber of Samples: The MNIST dataset typically contains 60,000 training images and 10,000 testing images, making it a relatively small but well-balanced dataset.\nLabeling: Each image in the dataset is associated with a label indicating the digit it represents. For example, an image of the digit “3” would have a corresponding label of 3.\nGrayscale Images: All images in the MNIST dataset are grayscale, meaning they have only one channel (as opposed to color images with three channels: red, green, and blue). Each pixel in the image has a value representing the intensity of the grayscale.\n\n\nmnist &lt;- dataset_mnist()\n\nThis code chunk calculates and prints the dimensions of the training data (x) within the mnist object. It uses the dim() function to retrieve the number of rows and columns in the training data. This information provides an overview of the size and shape of the training dataset.\n\nnames(mnist)\ndim(mnist$train$x)\ntable(mnist$train$y)\nindices_train &lt;- sample(1:dim(mnist$train$x)[1], 20000)\nindices_test &lt;- sample(1:dim(mnist$test$x)[1], 2000)\n\nIn this step, two sets of random indices (indices_train and indices_test) are generated for the training and testing datasets. Here’s what each line does:\nindices_train &lt;- sample(1:dim(mnist\\(train\\)x)[1], 20000): This line randomly samples 20,000 indices from the range of 1 to the number of rows in the training data (dim(mnist\\(train\\)x)[1]). These indices can be used to select a subset of 20,000 training examples from the MNIST dataset for training your machine learning models.\nindices_test &lt;- sample(1:dim(mnist\\(test\\)x)[1], 2000): Similarly, this line randomly samples 2,000 indices from the range of 1 to the number of rows in the testing data (dim(mnist\\(test\\)x)[1]). These indices can be used to select a subset of 2,000 testing examples for evaluating your machine learning models.\nData Preparation for MNIST Dataset [indices_train, , ]: By using the indices from the indices_train vector, this part selects specific rows from the training data. It selects only the rows corresponding to the 20,000 training examples that were sampled in the previous step.\n\nx_train &lt;- mnist$train$x[indices_train, , ]\nx_test &lt;- mnist$test$x[indices_test, , ]\ny_train &lt;- mnist$train$y[indices_train]\ny_test &lt;- mnist$test$y[indices_test]\n\nAnalyzing and Filtering Data for ‘Zero’ Digit Images\nIn this section, we’re going to analyze the MNIST dataset to identify and filter out images of the digit ‘zero’ (0) based on a specific criterion.\nCode Step 1: Calculate Standard Deviations\n\nzero =  sapply(1:dim(x_train)[1], function(x){sd(x_train[x, , ])})\n\nWe begin by calculating the standard deviation (sd) for each image in our training dataset (x_train). Each image is represented by a row in the dataset.\nThe sapply() function helps us apply the standard deviation calculation to each row of x_train. As a result, we obtain a vector called zero, which contains the standard deviation values for all the training images.\nCode Step 2: Creating a Histogram\n\nzero %&gt;% data.frame(sd=.) %&gt;% tibble() %&gt;%\n  ggplot(aes(x=sd)) +\n  geom_histogram()\n\nCode Step 3: Filtering ‘Zero’ Digit Images\n\nindices_zero &lt;- which(zero &gt;=50)\n\nThe indices_zero variable stores these indices, allowing us to pinpoint the ‘zero’ sd images in our dataset that exhibit a higher degree of variation.\n\nx_train &lt;- x_train[indices_zero, ,]\ny_train &lt;- y_train[indices_zero ]\n\n\npar(mfrow=c(1, 1)) # Set up the plot area\napply(x_train[1, ,], 2, rev)\nimage(1:28, 1:28, x_train[1, , ])\nimage(1:28, 1:28, t(apply(x_train[1, ,], 2, rev)))\n\nWe then visualize an example image from our filtered training data (x_train[1, , ]) using the image() function. The first image() call displays the image in its original orientation.\nThe second image() call displays the same image but with its rows reversed using the rev() function. This visual representation helps us understand the pixel values and structure of the image.\nData Reshaping and Normalization\n\nx_train &lt;- array_reshape(x_train, c(nrow(x_train), 28 * 28))\nx_test  &lt;- array_reshape(x_test, c(nrow(x_test), 28 * 28))\nx_train = x_train/255\nx_test  = x_test/255\n\nIn this step, we perform data reshaping and normalization to prepare the image data for machine learning models.\nWe use the array_reshape() function to reshape the training and testing data. Each image, originally a 28x28 pixel grid, is flattened into a 1D array of length 28 * 28 = 784. This transformation makes the data suitable for many machine learning algorithms.\nTo ensure that pixel values are within the range [0, 1], we divide all pixel values in both the training and testing datasets by 255. This normalization process scales the pixel values to a range where 0 represents black (minimum intensity) and 1 represents white (maximum intensity).\nDetecting CPU Cores\nWe start by detecting the number of CPU cores available in the system using the detectCores() function from the parallel package. Knowing the number of cores is useful for parallel processing, which can speed up computations.\n\nnumCores &lt;- parallel::detectCores()\nregisterDoParallel(cores = numCores - 1)\n# Define control parameters for the training process\n\nDefine Training Control Parameters\n\ntrain_control &lt;- trainControl(method = \"repeatedcv\", \n                              number = 10, \n                              repeats = 3, \n                              allowParallel = TRUE, \n                              classProbs = TRUE\n)\n\n\nmethod = “repeatedcv”: We specify the cross-validation method as repeated cross-validation. This helps assess the model’s performance by repeatedly splitting the data into training and validation sets.\nnumber = 10: We set the number of folds for cross-validation to 10, meaning the data will be divided into 10 subsets for evaluation.\nrepeats = 3: We repeat the cross-validation process three times for robust evaluation.\nallowParallel = TRUE: We allow parallel processing during training, which can speed up model fitting.\nclassProbs = TRUE: We indicate that we want to compute class probabilities during model training, which can be useful for certain evaluation metrics.\n\nData Preparation for k-NN (Caret)\n\nset.seed(2023)  # Setting seed for reproducibility\nindex &lt;- sample(1:nrow(x_train), 10000)\ncol_index &lt;- 1:ncol(x_train)\n# make data.fram for knn (caret)\nx_train_df = x_train %&gt;% as.data.frame(.)\ny_train_fa &lt;- factor(y_train, levels = 0:9, labels = paste0(\"digit_\", 0:9))\nx_test_df = x_test %&gt;% as.data.frame(.)\ny_test_fa &lt;- factor(y_test, levels = 0:9, labels = paste0(\"digit_\", 0:9))\n\nWe set a seed value (set.seed(2023)) to ensure reproducibility of random processes in our analysis.\nWe create a random sample of 10,000 indices (index) from the training data. This allows us to work with a manageable subset of the data for training and evaluation.\nWe define col_index to represent column indices. This variable can be useful for selecting specific columns from the dataset if needed.\nWe convert the training and testing data (x_train and x_test) into data frames (x_train_df and x_test_df) to facilitate their use with the caret package.\nWe transform the training and testing labels (y_train and y_test) into factor variables (y_train_fa and y_test_fa) and assign meaningful labels (“digit_0” to “digit_9”) to represent the digits in a format suitable for modeling.\nModel Training\n\ntic()\ntrain_knn &lt;- caret::train(x_train_df[index, col_index], \n                          y_train_fa[index] , \n                   method = \"knn\", \n                   tuneGrid = data.frame(k = c(3,5,7)),\n                   trControl = train_control, \n                   metric = \"Accuracy\")\ntoc()\n\nWe begin by training a k-Nearest Neighbors (k-NN) classifier using the caret::train() function. Here’s what each part of the code does:\n\nx_train_df[index, col_index]: We provide the training features (subset of data) based on the previously sampled indices (index) and column indices (col_index) to focus on specific data points and features.\ny_train_fa[index]: We provide the corresponding training labels, ensuring that we are using the filtered labels for training.\nmethod = “knn”: We specify the method as “knn” to indicate that we want to train a k-NN classifier.\ntuneGrid = data.frame(k = c(3, 5, 7)): We define a grid of hyperparameter values for ‘k’ (number of neighbors) to tune the model. We consider values 3, 5, and 7 for ‘k’ as potential choices.\ntrControl = train_control: We use the previously defined training control parameters to guide the cross-validation process.\nmetric = “Accuracy”: We specify that we want to evaluate the model’s performance based on accuracy.\n\nMaking Predictions\n\ntic()\npredict_knn = predict(train_knn, x_test_df, type=\"raw\")\ntoc()\n\n\ntrain_knn: We use the trained k-NN model train_knn to make predictions.\nx_test_df: We provide the testing features (x_test_df) to predict the corresponding labels.\ntype = “raw”: We specify the type of predictions as “raw,” which means we get the predicted class labels.\n\nConfusion Matrix and Accuracy\n\ncm_knn = confusionMatrix(as.factor(predict_knn), y_test_fa)\nknn=cm_knn$byClass[, \"Balanced Accuracy\"] %&gt;% data.frame(\"knn\"=.)\n\nFinally, we calculate a confusion matrix (cm_knn) to evaluate the model’s performance on the testing data. Here’s how it’s done:\nas.factor(predict_knn): We convert the predicted class labels to factors for consistent comparison with the actual labels.\ny_test_fa: We use the actual testing labels.\nThe confusionMatrix() function computes various metrics, including accuracy, for evaluating the classifier’s performance. We store the balanced accuracy in the knn variable."
  },
  {
    "objectID": "730_Supervised_minist_01.html#tensorflow-keras",
    "href": "730_Supervised_minist_01.html#tensorflow-keras",
    "title": "20  지도학습-MNIST (숫자인식)",
    "section": "20.2 tensorflow-keras",
    "text": "20.2 tensorflow-keras\nIn this section, you’re working with TensorFlow and Keras in R to create, train, and evaluate a neural network model.\nPreparing the Data\n\ny_train_c &lt;- to_categorical(y_train, num_classes = 10)\ny_test_c &lt;- to_categorical(y_test, num_classes = 10)\n\n\nCategorical Encoding: Converts the response variable (y_train and y_test) into a binary matrix representation, which is necessary for multi-class classification in Keras.\n\nBuilding the Neural Network\n\ntensor_keras &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 256, activation = \"relu\", input_shape = c(784)) %&gt;%\n  layer_dropout(rate = 0.25) %&gt;% \n  layer_dense(units = 128, activation = \"relu\") %&gt;%\n  layer_dropout(rate = 0.25) %&gt;% \n  layer_dense(units = 64, activation = \"relu\") %&gt;%\n  layer_dropout(rate = 0.25) %&gt;%\n  layer_dense(units = 10, activation = \"softmax\")\nsummary(tensor_knn)\n\n\nSequential Model: A linear stack of layers is created using keras_model_sequential().\nLayers: The model consists of multiple layer_dense with relu activations and layer_dropout to prevent overfitting.\nOutput Layer: The final layer_dense has 10 units with a softmax activation function, suitable for multi-class classification.\n\nCompiling the Model\n\ntensor_keras %&gt;% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = optimizer_adam(),\n  metrics = c(\"accuracy\")\n)\n\n\nCompile: Prepares the model for training. Uses categorical crossentropy as the loss function, the Adam optimizer, and accuracy as the metric.\n\nTraining the Model\n\nhistory &lt;- tensor_keras %&gt;% \n  fit(x_train, y_train_c, \n      epochs = 50, \n      batch_size = 128, \n      validation_split = 0.15)\n\n\nFit: Trains the model on x_train and y_train_c for 50 epochs, with a batch size of 128 and holding out 15% of the data for validation\n\nMaking Predictions and Evaluation\n\n# Make predictions on the test data\npredictions_keras &lt;- tensor_keras %&gt;% predict(x_test)\n#y_test[1]\n#y_test_c[1, ]\n#predictions_keras[1, ] %&gt;% which.max() -1\npredicted_keras_class = apply(predictions_keras, 1, which.max)\n#class(predicted_keras_class)\ntrue_classes &lt;- apply(y_test_c, 1, which.max)\n#class(true_classes)\n\n# Calculate confusion matrix\ncm_keras &lt;- confusionMatrix(predicted_keras_class %&gt;% as.factor(), \n                                    true_classes %&gt;% as.factor())\n\n\nPredictions: Generates predictions for x_test.\nClass Prediction: Converts the softmax output to class predictions.\nConfusion Matrix: Evaluates the performance using a confusion matrix.\n\nVisualizing Model Performance\n\nkeras=cm_keras$byClass[, \"Balanced Accuracy\"] %&gt;% data.frame(\"keras\"=.)\n\ncbind(knn, keras) %&gt;% data.frame() %&gt;%\n  mutate(class=rownames(.)) %&gt;%\n  tibble() %&gt;%\n  pivot_longer(names_to = \"model\", values_to = \"BA\", cols = c(knn, keras)) %&gt;%\n  mutate(class= str_replace(class, \"Class: \", \"\")) %&gt;%\n  ggplot(aes(x=BA, y = class, color = model ==\"keras\")) +\n  geom_point(aes(size = model == \"keras\"))  +\n  theme_minimal()"
  },
  {
    "objectID": "740_Supervised_sentence_01.html",
    "href": "740_Supervised_sentence_01.html",
    "title": "21  지도학습-setence (문장 인식)",
    "section": "",
    "text": "library(tidyverse)\npkgs = c(\"tidyverse\",  \"tidytext\", \"tensorflow\", \"keras\", \"caret\", \"data.table\", \n         \"googleLanguageR\", \"cld2\",\"tidyverse\", \"datasets\", \"ggplot2\", \"tictoc\")\nfor (pkg in pkgs){\n  if(!require(pkg, character.only = TRUE)) install.packages(pkg)\n  library(pkg, character.only = TRUE)\n}\n\n\nmt2 = readRDS(\"db/mt2.rds\")\nmt3 = mt2 %&gt;% \n  # 몇\n  mutate(code = code2) %&gt;%\n  select(id, code, word = word2)\n\n\ncode_inc = mt3 %&gt;%\n  select(id, code) %&gt;% unique() %&gt;%\n  group_by(code) %&gt;%\n  count() %&gt;%\n  arrange(desc(n)) %&gt;% \n  filter(n&gt;1500) %&gt;%     \n  pull(code)\n\nlength(code_inc)\nwordnumber = 2000\nwordData = mt3 %&gt;%\n  filter(code %in% code_inc)  %&gt;%\n  group_by(word) %&gt;%\n  count() %&gt;%\n  arrange(desc(n)) %&gt;%\n  ungroup() %&gt;%\n  slice(1:wordnumber) %&gt;%\n  mutate(wordid = row_number()) %&gt;%\n  select(-n)\nwordData\nmt4 = mt3 %&gt;% filter(code %in% code_inc) %&gt;%\n  left_join(wordData, by = c(\"word\")) %&gt;%\n  na.omit() %&gt;%\n  ungroup() %&gt;%\n  mutate(code = as.numeric(code))\n\n## code lkup 을 통해 code를 1부터 순서대로 만든 code_nums로 트레이닝 시키기 \ncode_lkup = mt4 %&gt;%\n  select(code) %&gt;% unique() %&gt;%\n  mutate(code_nums = row_number())\n\n\nmt5 = mt4 %&gt;%\n  filter(id != \"W615000\") %&gt;%\n  left_join(code_lkup, by=c(\"code\"))\n\n\n# data 만들기\nlibrary(parallel)\ngg = mt5 %&gt;% select(id, code_nums) %&gt;% unique()\nnrow(gg)\ngg$code_nums\nset.seed(2022)\n\n\ntrainIndex = createDataPartition(gg$code_nums, p =0.7, \n                                 list = FALSE, \n                                 times = 1)\nindex = trainIndex %&gt;% as.numeric()\ntraining_pre = gg[ index, ]\ntesting_pre  = gg[-index, ]\ntrain_index = training_pre %&gt;% pull(id) %&gt;% unique()\ntest_index  = testing_pre %&gt;% pull(id) %&gt;% unique()\ntraining = mt5 %&gt;% filter(id %in% train_index)\ntesting  = mt5 %&gt;% filter(id %in% test_index)\ntraining$code %&gt;% unique()\ntraining %&gt;%  group_by(code_nums) %&gt;% count() %&gt;%\n  left_join(testing  %&gt;%  group_by(code_nums) %&gt;% count(), \n            by= c(\"code_nums\")) %&gt;%\n  setNames(c(\"code_nums\", \"train\", \"test\")) %&gt;%\n  mutate(prob = test/train)\ntrainDX = training %&gt;% select(id, wordid)\ntrainDY = training %&gt;% select(id, code_nums) %&gt;% unique()\n#trainDY %&gt;%\n#  group_by(id) %&gt;%\n#  count() %&gt;%\n#  arrange(desc(n))\ntestDX  = testing  %&gt;% select(id, wordid)\ntestDY  = testing  %&gt;% select(id, code_nums) %&gt;% unique()\ntrainY = trainDY %&gt;% pull(code_nums)\ntestY  = testDY %&gt;% pull(code_nums)\ntrainY %&gt;% unique() %&gt;% length(.)\n\n\n# list 로 변환\nlibrary(parallel)\ntrainMX = list()\ntrainF = function(i){trainDX %&gt;% filter(id == i) %&gt;% pull(wordid)}\ntrainMX =  mclapply(train_index, trainF,  mc.cores = 40)\nlength(trainMX) == nrow(trainDY)\n#saveRDS(trainMX, \"db/trainMX.rds\")\n#trainMX = readRDS(\"db/trainMX.rds\")\ntrainMX\ntestMX = list()\ntestF = function(i){testDX %&gt;% filter(id == i) %&gt;% pull(wordid)}\ntestMX =  mclapply(test_index, testF,  mc.cores = 40)\n\nsequencingF = function(mx){\n  jj = matrix(0, nrow=length(mx), ncol= 2000)\n  for (i in 1:length(mx))\n    jj[i, mx[[i]]] &lt;-1\n  return(jj)\n}\n\n\n# one-hot encode to categories\nlibrary(reticulate)\nlibrary(tensorflow)\nuse_virtualenv(\"/home/sehnr/tensorflow/tensorvenv\", required = TRUE)\nXtrain = sequencingF(trainMX)\nXtest  = sequencingF(testMX)\nYtrain = to_categorical(trainY) #\n\n\nset.seed(2022)\nnrow(Xtrain)\nval_indices &lt;- sample(c(1:c(Xtrain %&gt;% nrow())), c(Xtrain %&gt;% nrow())/10)\nx_val &lt;- Xtrain[val_indices,]\npartial_x_train &lt;- Xtrain[-val_indices,]\ny_val &lt;- Ytrain[val_indices,]\npartial_y_train = Ytrain[-val_indices,]\n\n\nlibrary(keras)\nkeras::k_clear_session()\n#ref: https://medium.com/@cmukesh8688/activation-functions-sigmoid-tanh-relu-leaky-relu-softmax-50d3778dcea5\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 18, activation = \"relu\", input_shape = c(wordnumber)) %&gt;%\n  layer_dropout(rate = 0.4) %&gt;% #  layers to avoid overfitting.\n  layer_dense(units = 72, activation = \"relu\") %&gt;%\n  layer_dropout(rate = 0.3) %&gt;%\n  layer_dense(units = 36, activation = \"sigmoid\") %&gt;%\n  layer_dense(units = 18, activation = \"softmax\")\nmodel\n\n\nmodel %&gt;% compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\"))\n\n\nhistory &lt;- model %&gt;% fit(\n  partial_x_train,\n  partial_y_train,\n  epochs = 10,\n  batch_size = 50,\n  #validation_split=0.2, \n  validation_data = list(x_val, y_val)\n)"
  }
]